[["index.html", "DMAC Training Modules Chapter 1 Introduction", " DMAC Training Modules Kyle Roell, Lauren Koval, Julia Rager 2021-06-23 Chapter 1 Introduction The UNC-Superfund Research Program (SRP) seeks to develop new solutions for reducing exposure to inorganic arsenic and prevent arsenic-induced diabetes through mechanistic and translational research. The Data Analysis and Management Core (DMAC) provide the UNC-Superfund Research Program with critical expertise in bioinformatics, statistics, data management and data integration. Our goal is to support the data management, integration, and analysis needs of the researchers to reveal multi-factorial determinants of inorganic arsenic-induced metabolic dysfunction/diabetes. All code for these modules can be found at the UNC-SRP Github Page. "],["intro.html", "Chapter 2 Setting Up Your R Environment 2.1 R and RStudio 2.2 Scripting Basics", " Chapter 2 Setting Up Your R Environment Before learning about data manipulation and statistical methods for analyzing environmental health datasets, we will provide a brief introduction to R, RStudio, and setting up an R environment and simple scripts. 2.1 R and RStudio R is a free, open source programming language for statistical computing and graphics that anyone can download and use. It doesn’t require a license and is good for reproducible analyses. There exists a large, diverse collection of packages and very comprehensive documentation. It is easy to download, install, and setup R. Additionally, RStudio is an open source integrated development environment for R. RStudio makes programming in R and using R scripts and features more user friendly. R should be downloaded prior to downloading RStudio. 2.1.1 Downloading R and RStudio The following is a walkthrough on how to download R and RStudio. 1. Navigate to R Website Figure 2.1: R Website, https://www.r-project.org 2. Select the appropriate CRAN mirror (Duke is fastest if at UNC) Figure 2.2: CRAN Mirror, https://cran.r-project.org/mirrors.html 3. Select the appropriate R distribution Figure 2.3: R Download Link, http://archive.linux.duke.edu/cran/ 4. Download R Figure 2.4: R Download, http://archive.linux.duke.edu/cran/bin/macosx/ 5. Navigate to RStudio website and download RStudio (free edition) Figure 2.5: RStudio Download, https://rstudio.com/products/rstudio/download/ 2.1.2 Installing R and RStudio Once R and RStudio have been downloaded, install R first and then RStudio, following the instructions of the installer. 2.1.3 Installing and Loading Packages Packages in R are units of shareable code that contain functions, data, and documentation on how to use all of these resources. Because R is an open source programming language, packages are constantly being developed and updated. There are many R packages that exist spanning many topics such as graphics and plotting, machine learning, and data manipulation. R packages are often written by R users and submitted to the Comprehensive R Archive Network (CRAN), or another host such as BioConductor or GitHub. Packages can be installed from the host, but need to be loaded into the workspace. Most of the time, you do not need to download anything from a website. Instead, you can install packages through running code in R or RStudio. install.packages(&quot;ggplot2&quot;, repos = &quot;https://cran.rstudio.com&quot;) Once a package is installed, it needs to be loaded using the library function or explicitly referenced to use functions or datasets from that package. library(ggplot2) 2.2 Scripting Basics Before demonstrating the basics of writing R code and scripts, it is worth noting that a function can be queried in RStudio by typing a question mark before the name of the function (e.g. ?install.packages). This will bring up documentation in the viewer window. Additionally, RStudio will autofill function names, variable names, etc. by pressing tab while typing. If multiple matches are found, RStudio will provide you with a drop down list to select from, which may be useful when searching through newly installed packages or trying to quickly type variable names in an R script. R also allows for scripts to contain non-code elements, called comments, that will not be run or interpreted. To make a comment, simply use a # followed by the comment. A # only comments out a single line of code, i.e. only that line will not be run. Comments are useful to help make code more interpretable for others or to add reminders of what and why parts of code may have been written. # This is an R comment! # Loading ggplot2 package library(ggplot2) 2.2.1 Setting Working Directory When working in R, it can be helpful to set the working directory to a local directory where data are located or output files will be saved. The current working directory can also be displayed. # Show current working directory getwd() ## [1] &quot;/Users/kroell/Documents/IEHS/UNC-SRP/test1&quot; # Set working directory setwd(&quot;~/Documents/UNCSRP/Data/&quot;) 2.2.2 Importing and Exporting Files After setting the working directory, importing and exporting files can be done using various functions based on the type of file being read or written. Often, it is easiest to import data into R that are in a comma separated values, comma delimited, (CSV) file or tab delimited file. Other datatypes such as SAS data files, large csv files, etc. may require different functions to be more efficienlty read in and some of these file formats will be discussed in future modules. # Read in CSV data csv.dataset = read.csv(file=&quot;~/Documents/UNCSRP/Data/example_data.csv&quot;) # Read in tab delimited data tab.dataset = read.table(file=&quot;~/Documents/UNCSRP/Data/example_data.txt&quot;) There are many ways to export data in R. Data can be written out into a CSV file, tab delimited file, RData file, etc. There are also many functions within packages that write out specific datasets generated by that package. # Write out to a CSV file write.csv(csv.output, file=&quot;~/Documents/UNCSRP/Output/csv_output.csv&quot;) # Write out to a tab delimited file write.table(tab.output, file=&quot;~/Documents/UNCSRP/Output/tsv_output.txt&quot;, sep=&quot;\\t&quot;) R also allows objects to be saved in RData files. These files can be read into R, as well, and will load the object into the current workspace. Entire workspaces are also able to be saved. # Read in saved single R data object r.obj = readRDS(file=&quot;~/Documents/UNCSRP/Data/data.rds&quot;) # Write single R object to file saveRDS(object, file=&quot;~/Documents/UNCSRP/Output/single_object.rds&quot;) # Read in multiple saved R objects load(file=&quot;~/Documents/UNCSRP/Data/multiple_data.RData&quot;) # Save multiple R objects save(object1, object2, file=&quot;~/Documents/UNCSRP/Output/multiple_objects.RData&quot;) # Save entire workspace save.image(file=&quot;~/Documents/UNCSRP/Output/entire_workspace.RData&quot;) # Load entire workspace load(file=&quot;~/Documents/UNCSRP/Data/entire_workspace.RData&quot;) 2.2.3 Viewing Data After data has been loaded into R, or created within R, it is good to inspect it. Datasets can be viewed in their entirety or subset to quickly look at part of the data. # View first 5 rows of the previously loaded dataset csv.dataset[1:5,] ## Sample Var1 Var2 Var3 ## 1 sample1 1 2 1 ## 2 sample2 2 4 4 ## 3 sample3 3 6 9 ## 4 sample4 4 8 16 ## 5 sample5 5 10 25 # View the entire dataset in RStudio View(csv.dataset) "],["dataorg.html", "Chapter 3 The Basics for Data Organization 3.1 Basic Data Manipulation", " Chapter 3 The Basics for Data Organization One of the first things that is useful to learn about is data organization and manipulation. While this will not be a complete guide, as there are many things you can do in R to manipulate your data, hopefully this will get you started and will provide you with the fundamentals for working with data in R. 3.1 Basic Data Manipulation Data manipulation generally refers to organizing and formatting data in a way that makes it easier to read and work with. This can be done through base R operations and functions or through a collection of packages (and philosophy) known as Tidyverse. In this brief tutorial we will first go over the basic operations and functions you can use to manipulate data, and then show the exact same steps using the Tidyverse packages, functions, and syntax. 3.1.1 Merging Here, we will be looking at merging as the joining together of two or more datasets, connected using a common identifier, generally some sort of ID. This is useful if you various datasets describing different variables across the same participants. In our example, we will be joining demographic data and environmental exposure data. #First, let&#39;s read in our datasets demo.data = read.csv(&quot;~/Downloads/demo_data.csv&quot;); chem.data = read.csv(&quot;~/Downloads/chem_data.csv&quot;); #We can merge these by their shared ID column full.data = merge(demo.data, chem.data, by=&quot;ID&quot;) #We could also merge by different ID columns using: by.x = &quot;ID.Demo&quot;, by.y=&quot;ID.Chem&quot; 3.1.2 Filtering &amp; subsetting Filtering and subsetting data is useful when you need to restrict your dataset to samples or participants with specific conditions. It is also useful for simply removing particular variables or samples. #Let&#39;s define a vector of columns we want to keep subset.columns = c(&quot;BMI&quot;, &quot;MAge&quot;, &quot;MEdu&quot;); #Now we can simply subset our data using those columns subset.data1 = full.data[,subset.columns]; #If we want to remove those columns, need to identify which columns need removing remove.columns = colnames(full.data) %in% subset.columns; cat(remove.columns); ## FALSE TRUE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE #Now we can subset our dataset by only keeping those that we didn&#39;t specify subset.data1a = full.data[,!remove.columns]; #It is easy to subset data based on rows #Keeping only first 100 rows subset.data2 = full.data[1:100,]; #Removing the first 100 rows subset.data2a = full.data[-c(1:100),]; #We can also filter data using conditional statements subset.data3 = full.data[which(full.data$BMI &gt; 25 &amp; full.data$MAge &gt; 31),]; head(subset.data3); ## ID BMI MAge MEdu BW GA DWAs DWCd DWCr ## 4 4 30.09710 34.81796 3.760030 3266.844 18.60876 5.906656 2.0752589 50.92745 ## 5 5 37.41737 42.68440 4.484686 3664.088 32.57351 7.181873 2.7626433 55.16882 ## 9 9 36.94794 33.58589 3.242318 3260.482 37.37241 9.074928 2.7277549 55.72826 ## 13 13 33.65870 33.82961 2.924433 3481.293 33.68744 7.101634 0.8443918 47.11677 ## 22 22 25.68414 37.08028 2.704409 3387.046 52.48203 7.207447 2.8088453 48.08648 ## 31 31 28.39896 47.85761 6.453362 3173.033 20.10080 6.032807 2.1929549 45.71856 ## UAs UCd UCr ## 4 8.719123 0.9364825 42.47987 ## 5 9.436559 1.4977829 47.78528 ## 9 10.818153 1.6585757 42.58577 ## 13 9.967185 -0.3466431 36.74220 ## 22 9.446643 1.9891049 34.16921 ## 31 9.917588 1.1194851 37.82297 #This can also be done using the subset function subset.data4 = subset(full.data, BMI &gt; 25 &amp; MAge &gt; 31); #Additionally, we can subset and select only specific columns to keep subset.data4 = subset(full.data, BMI &lt; 22 | BMI &gt; 27, select=c(&quot;BMI&quot;, &quot;MAge&quot;, &quot;MEdu&quot;)); 3.1.3 Melt and Cast Melting and casting refers to coverting data to “long” or “wide” form. Generally, our data is in wide format, but long format is necessary for some procedures, such as plotting with ggplot2. #Melt and cast functions from reshape2 package library(reshape2); #Melting data converts it to long format #Here, we are saying we want a row for each sample (ID) and each column full.melted = melt(full.data, id=&quot;ID&quot;) #Let&#39;s look at the new data head(full.melted); ## ID variable value ## 1 1 BMI 27.69249 ## 2 2 BMI 26.79125 ## 3 3 BMI 33.23209 ## 4 4 BMI 30.09710 ## 5 5 BMI 37.41737 ## 6 6 BMI 33.28761 #Casting puts data into wide format #We are telling the cast function to give us a sample (ID) #for every variable in the variable column full.cast = dcast(full.melted, ID ~ variable); head(full.cast); ## ID BMI MAge MEdu BW GA DWAs DWCd DWCr ## 1 1 27.69249 22.99928 3.136759 3180.058 31.79569 6.426464 1.292941 51.67987 ## 2 2 26.79125 30.05142 2.656019 3210.823 53.34991 7.832384 1.798535 50.10409 ## 3 3 33.23209 28.04660 3.259116 3311.551 53.60599 7.516569 1.288461 48.74001 ## 4 4 30.09710 34.81796 3.760030 3266.844 18.60876 5.906656 2.075259 50.92745 ## 5 5 37.41737 42.68440 4.484686 3664.088 32.57351 7.181873 2.762643 55.16882 ## 6 6 33.28761 24.94960 3.596075 3328.988 38.12531 9.723429 3.054057 51.14812 ## UAs UCd UCr ## 1 10.192695 0.7537104 42.60187 ## 2 11.815088 0.9789506 41.30757 ## 3 10.079057 0.1903262 36.47716 ## 4 8.719123 0.9364825 42.47987 ## 5 9.436559 1.4977829 47.78528 ## 6 11.589403 1.6645837 38.26386 3.1.4 Tidyverse Tidyverse is a collection of packages that are used together along with a specific type of syntax, dataset and formatting protocols, etc. Here, we will peform all the of the previous exercises using Tidyverse! library(tidyverse); #Merging full.tidy = inner_join(demo.data, chem.data, by=&quot;ID&quot;); #To merge with different IDs use: by = c(&quot;ID.Demo&quot;=&quot;ID.Chem&quot;) #Susbetting columns subset.tidy1 = full.tidy %&gt;% select(all_of(subset.columns)); subset.columns1 = c(subset.columns, &quot;NotAColName&quot;); subset.tidy1a = full.tidy %&gt;% select(any_of(subset.columns1)); #Removing columns subset.tidy1b = full.tidy %&gt;% select(-subset.columns); #Susbetting Rows subset.tidy2a = full.tidy %&gt;% slice(1:100); subset.tidy2b = full.tidy %&gt;% slice(-c(1:100)); #Filtering data subset.tidy3 = full.tidy %&gt;% filter(BMI &gt; 25 &amp; MAge &gt; 31); head(subset.tidy3); ## ID BMI MAge MEdu BW GA DWAs DWCd DWCr ## 1 4 30.09710 34.81796 3.760030 3266.844 18.60876 5.906656 2.0752589 50.92745 ## 2 5 37.41737 42.68440 4.484686 3664.088 32.57351 7.181873 2.7626433 55.16882 ## 3 9 36.94794 33.58589 3.242318 3260.482 37.37241 9.074928 2.7277549 55.72826 ## 4 13 33.65870 33.82961 2.924433 3481.293 33.68744 7.101634 0.8443918 47.11677 ## 5 22 25.68414 37.08028 2.704409 3387.046 52.48203 7.207447 2.8088453 48.08648 ## 6 31 28.39896 47.85761 6.453362 3173.033 20.10080 6.032807 2.1929549 45.71856 ## UAs UCd UCr ## 1 8.719123 0.9364825 42.47987 ## 2 9.436559 1.4977829 47.78528 ## 3 10.818153 1.6585757 42.58577 ## 4 9.967185 -0.3466431 36.74220 ## 5 9.446643 1.9891049 34.16921 ## 6 9.917588 1.1194851 37.82297 subset.tidy4 = full.tidy %&gt;% filter(BMI &gt; 25 &amp; MAge &gt; 31) %&gt;% select(BMI, MAge, MEdu); #Melting and Casting (pivoting in Tidyverse) full.pivotlong = full.tidy %&gt;% pivot_longer(-ID, names_to = &quot;var&quot;, values_to = &quot;value&quot;); head(full.pivotlong, 15); ## # A tibble: 15 x 3 ## ID var value ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 BMI 27.7 ## 2 1 MAge 23.0 ## 3 1 MEdu 3.14 ## 4 1 BW 3180. ## 5 1 GA 31.8 ## 6 1 DWAs 6.43 ## 7 1 DWCd 1.29 ## 8 1 DWCr 51.7 ## 9 1 UAs 10.2 ## 10 1 UCd 0.754 ## 11 1 UCr 42.6 ## 12 2 BMI 26.8 ## 13 2 MAge 30.1 ## 14 2 MEdu 2.66 ## 15 2 BW 3211. full.pivotwide = full.pivotlong %&gt;% pivot_wider(names_from = &quot;var&quot;, values_from=&quot;value&quot;); head(full.pivotwide); ## # A tibble: 6 x 12 ## ID BMI MAge MEdu BW GA DWAs DWCd DWCr UAs UCd UCr ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 27.7 23.0 3.14 3180. 31.8 6.43 1.29 51.7 10.2 0.754 42.6 ## 2 2 26.8 30.1 2.66 3211. 53.3 7.83 1.80 50.1 11.8 0.979 41.3 ## 3 3 33.2 28.0 3.26 3312. 53.6 7.52 1.29 48.7 10.1 0.190 36.5 ## 4 4 30.1 34.8 3.76 3267. 18.6 5.91 2.08 50.9 8.72 0.936 42.5 ## 5 5 37.4 42.7 4.48 3664. 32.6 7.18 2.76 55.2 9.44 1.50 47.8 ## 6 6 33.3 24.9 3.60 3329. 38.1 9.72 3.05 51.1 11.6 1.66 38.3 "],["finding-and-visualizing-data-trends.html", "Chapter 4 Finding and Visualizing Data Trends 4.1 Basic Statistical Tests and Visualizations of Data 4.2 Heat maps 4.3 Clustering 4.4 Data reduction (PCA)", " Chapter 4 Finding and Visualizing Data Trends 4.1 Basic Statistical Tests and Visualizations of Data Need an example dataset – maybe ELGAN shuffled/deidentified, with made-up environmental exposure column? 4.1.1 Normality Many statistical tests and methods rely on assumptions of normality. There are a few ways to look at the normality of a dataset, both formally and informally. Plotting data using historgrams, densities, or qqplots, can graphically help inform if a variable is normally distributed. There also exist statistical tests, such as the Kolmogorov-Smirnov (K-S) normality test and Shapiro-Wilk’s test, that will formally test if data come from a normal distribution. When using these tests, it is important to remember that the null hypothesis is that the sample distribution is normal, and a significant p-value means the distribution is non-normal. Even when using more formal statistical tests to test for normality, it is important to visualize the data, as well. #Histograms to visualize data hist(full.data$BMI); #Can decrease bin size (using breaks parameter) to better visualize hist(full.data$BMI, breaks=20); #Also look at normal qqplot using qqnorm function qqnorm(full.data$BMI); #Add reference line qqline(full.data$BMI); From visual inspection, BMI seems to be pretty normally distributed. But, we can also check using a more formal statistical test, the Shapiro-Wilk’s test. #Perform Shapiro-Wilk normality test (from base stats package) shapiro.test(full.data$BMI); ## ## Shapiro-Wilk normality test ## ## data: full.data$BMI ## W = 0.99617, p-value = 0.9014 We get a p-value of .9014, so cannot reject the null hypothesis. This means that we can assume normality of this data. 4.1.2 T-tests T-tests are used to test for a significant difference between the means of two groups. There are a few types of t-tests, but here, we will be comparing BMI between two groups, smokers and non-smokers, and will be using a two sample t-test (or independent samples t-test). TALK ABOUT ASSUMPTIONS… #It is nice to visualize the data across groups using things like boxplots boxplot(data=full.data, BMI ~ Smoker); #It is easy to peform a t-test on these data using t.test() from base stats package t.test(data=full.data, BMI ~ Smoker); ## ## Welch Two Sample t-test ## ## data: BMI by Smoker ## t = 1.6189, df = 77.478, p-value = 0.1095 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.4115608 3.9922067 ## sample estimates: ## mean in group 0 mean in group 1 ## 25.82896 24.03864 #We can also save the results into a variable and access various output values ttest.res = t.test(data=full.data, BMI ~ Smoker); #For example, we can access the p-value ttest.res$p.value; ## [1] 0.1095319 From the boxplots, it’s clear that there are differences in the BMIs between smokers and non-smokers. And, in fact, when running the t-test, we see that means across groups are not equal. 4.1.3 ANOVA Analysis of Variance (ANOVA) is a statistical method that is used to compare means of more than two groups. TALK ABOUT ASSUMPTIONS and other stuff. #Boxplots to look at data across groups boxplot(data=full.data, BMI ~ Smoker3); #Can also get group means full.data %&gt;% group_by(Smoker3) %&gt;% summarise(mean(BMI)); ## # A tibble: 3 x 2 ## Smoker3 `mean(BMI)` ## &lt;chr&gt; &lt;dbl&gt; ## 1 Current 19.0 ## 2 Former 27.4 ## 3 Never 26.9 #Use ANOVA to compare groups (use aov to fit ANOVA model) aov(data=full.data, BMI ~ Smoker3); ## Call: ## aov(formula = BMI ~ Smoker3, data = full.data) ## ## Terms: ## Smoker3 Residuals ## Sum of Squares 2087.198 7130.377 ## Deg. of Freedom 2 197 ## ## Residual standard error: 6.016212 ## Estimated effects may be unbalanced #We can get the typical ANOVA table using either summary or anova on fitted object anova(aov(data=full.data, BMI ~ Smoker3)); ## Analysis of Variance Table ## ## Response: BMI ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Smoker3 2 2087.2 1043.60 28.833 1.04e-11 *** ## Residuals 197 7130.4 36.19 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 From the ANOVA table, we can conclude that the group means are not all equal. 4.1.4 Regression: linear regression and logistic regression 4.1.5 Chi-squared test – box plots 4.1.6 Fisher’s exact test 4.2 Heat maps 4.2.1 pheatmap 4.2.2 heatmap2 4.2.3 superheat 4.3 Clustering Examples with genomics: Rager et al. 2014 4.3.1 Hierarchical 4.3.2 K-means 4.4 Data reduction (PCA) 4.4.1 Visualize PCA Plot 4.4.2 Identify % of variance captured "],["multi-omics-analyses-for-environmental-health.html", "Chapter 5 Multi-Omics Analyses for Environmental Health 5.1 Exposomics 5.2 Transcriptomics 5.3 Genome-wide MicroRNA 5.4 Genome-wide DNA Methylation 5.5 Proteomics", " Chapter 5 Multi-Omics Analyses for Environmental Health 5.1 Exposomics 5.1.1 Placenta Exposome about to be submitted to EI Dust NTA data 5.2 Transcriptomics 5.2.1 DESeq2 / RNAseq Wildfire dataset, available through GEO 5.3 Genome-wide MicroRNA Rager et al. 2014 miRNAs 5.4 Genome-wide DNA Methylation 5.4.1 Illumina array data https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE58499 https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE28368 5.5 Proteomics Bailey et al arsenic dataset maybe? "],["mixtures-analyses-for-environmental-health.html", "Chapter 6 Mixtures Analyses for Environmental Health 6.1 Sufficient Similarity 6.2 Mixtures Modeling through qgcomp", " Chapter 6 Mixtures Analyses for Environmental Health 6.1 Sufficient Similarity Botanicals example with chemistry and tox profiling – Julia has dataset 6.2 Mixtures Modeling through qgcomp Could use published wildfire analysis here (Rager et al. 2021, STOTEN), or online example provide through Alex Keil’s studies "],["environmental-health-databases.html", "Chapter 7 Environmental Health Databases 7.1 Comparative Toxicogenomics Database (CTD) 7.2 Gene Expression Omnibus (GEO) 7.3 NHANES", " Chapter 7 Environmental Health Databases 7.1 Comparative Toxicogenomics Database (CTD) 7.2 Gene Expression Omnibus (GEO) 7.3 NHANES "]]
