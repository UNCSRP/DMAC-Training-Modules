[["index.html", "DMAC Training Modules Chapter 1 Introduction", " DMAC Training Modules Kyle Roell, Lauren Koval, Julia Rager 2021-06-04 Chapter 1 Introduction The UNC-Superfund Research Program (SRP) seeks to develop new solutions for reducing exposure to inorganic arsenic and prevent arsenic-induced diabetes through mechanistic and translational research. The Data Analysis and Management Core (DMAC) provide the UNC-Superfund Research Program with critical expertise in bioinformatics, statistics, data management and data integration. Our goal is to support the data management, integration, and analysis needs of the researchers to reveal multi-factorial determinants of inorganic arsenic-induced metabolic dysfunction/diabetes. All code for these modules can be found at the UNC-SRP Github Page. "],["intro.html", "Chapter 2 Setting Up Your R Environment 2.1 R and RStudio 2.2 Scripting Basics", " Chapter 2 Setting Up Your R Environment Before learning about data manipulation and statistical methods for analyzing environmental health datasets, we will provide a brief introduction to R, RStudio, and setting up an R environment and simple scripts. 2.1 R and RStudio R is a free, open source programming language for statistical computing and graphics that anyone can download and use. It doesn’t require a license and is good for reproducible analyses. There exists a large, diverse collection of packages and very comprehensive documentation. It is easy to download, install, and setup R. Additionally, RStudio is an open source integrated development environment for R. RStudio makes programming in R and using R scripts and features more user friendly. R should be downloaded prior to downloading RStudio. 2.1.1 Downloading R and RStudio The following is a walkthrough on how to download R and RStudio. 1. Navigate to R Website Figure 2.1: R Website, https://www.r-project.org 2. Select the appropriate CRAN mirror (Duke is fastest if at UNC) Figure 2.2: CRAN Mirror, https://cran.r-project.org/mirrors.html 3. Select the appropriate R distribution Figure 2.3: R Download Link, http://archive.linux.duke.edu/cran/ 4. Download R Figure 2.4: R Download, http://archive.linux.duke.edu/cran/bin/macosx/ 5. Navigate to RStudio website and download RStudio (free edition) Figure 2.5: RStudio Download, https://rstudio.com/products/rstudio/download/ 2.1.2 Installing R and RStudio Once R and RStudio have been downloaded, install R first and then RStudio, following the instructions of the installer. 2.1.3 Installing and Loading Packages Packages in R are units of shareable code that contain functions, data, and documentation on how to use all of these resources. Because R is an open source programming language, packages are constantly being developed and updated. There are many R packages that exist spanning many topics such as graphics and plotting, machine learning, and data manipulation. R packages are often written by R users and submitted to the Comprehensive R Archive Network (CRAN), or another host such as BioConductor or GitHub. Packages can be installed from the host, but need to be loaded into the workspace. Most of the time, you do not need to download anything from a website. Instead, you can install packages through running code in R or RStudio. install.packages(&quot;ggplot2&quot;, repos = &quot;https://cran.rstudio.com&quot;) Once a package is installed, it needs to be loaded using the library function or explicitly referenced to use functions or datasets from that package. library(ggplot2) 2.2 Scripting Basics Before demonstrating the basics of writing R code and scripts, it is worth noting that a function can be queried in RStudio by typing a question mark before the name of the function (e.g. ?install.packages). This will bring up documentation in the viewer window. Additionally, R will autofill function names, variable names, etc. by pressing tab while typing. If multiple matches are found, R will provide you with a drop down list to select from, which may be useful when searching through newly installed packages or trying to quickly type variable names in an R script. R also allows for scripts to contain non-code elements, called comments, that will not be run or interpreted. To make a comment, simply use a # followed by the comment. A # only comments out a single line of code, i.e. only that line will not be run. Comments are useful to help make code more interpretable for others or to add reminders of what and why parts of code may have been written. # This is an R comment! # Loading ggplot2 package library(ggplot2) 2.2.1 Setting Working Directory When working in R, it can be helpful to set the working directory to a local directory where data are located or output files will be saved. The current working directory can also be displayed. # Show current working directory getwd() ## [1] &quot;/Users/kroell/Documents/IEHS/UNC-SRP/test1&quot; # Set working directory setwd(&quot;~/Documents/UNCSRP/Data/&quot;) 2.2.2 Importing and Exporting Files After setting the working directory, importing and exporting files can be done using various functions based on the type of file being read or written. Often, it is easiest to import data into R that are in a comma separated values, comma delimited, (CSV) file or tab delimited file. Other datatypes such as SAS data files, large csv files, etc. may require different functions to be more efficienlty read in and some of these file formats will be discussed in future modules. # Read in CSV data csv.dataset = read.csv(file=&quot;~/Documents/UNCSRP/Data/example_data.csv&quot;) # Read in tab delimited data tab.dataset = read.table(file=&quot;~/Documents/UNCSRP/Data/example_data.txt&quot;) There are many ways to export data in R. Data can be written out into a CSV file, tab delimited file, RData file, etc. There are also many functions within packages that write out specific datasets generated by that package. # Write out to a CSV file write.csv(csv.output, file=&quot;~/Documents/UNCSRP/Output/csv_output.csv&quot;) # Write out to a tab delimited file write.table(tab.output, file=&quot;~/Documents/UNCSRP/Output/tsv_output.txt&quot;, sep=&quot;\\t&quot;) R also allows objects to be saved in RData files. These files can be read into R, as well, and will load the object into the current workspace. Entire workspaces are also able to be saved. # Read in saved single R data object r.obj = readRDS(file=&quot;~/Documents/UNCSRP/Data/data.rds&quot;) # Write single R object to file saveRDS(object, file=&quot;~/Documents/UNCSRP/Output/single_object.rds&quot;) # Read in multiple saved R objects load(file=&quot;~/Documents/UNCSRP/Data/multiple_data.RData&quot;) # Save multiple R objects save(object1, object2, file=&quot;~/Documents/UNCSRP/Output/multiple_objects.RData&quot;) # Save entire workspace save.image(file=&quot;~/Documents/UNCSRP/Output/entire_workspace.RData&quot;) # Load entire workspace load(file=&quot;~/Documents/UNCSRP/Data/entire_workspace.RData&quot;) 2.2.3 Viewing Data After data has been loaded into R, or created within R, it is good to inspect it. Datasets can be viewed in their entirety or subset to quickly look at part of the data. # View first 5 rows of the previously loaded dataset csv.dataset[1:5,] ## Sample Var1 Var2 Var3 ## 1 sample1 1 2 1 ## 2 sample2 2 4 4 ## 3 sample3 3 6 9 ## 4 sample4 4 8 16 ## 5 sample5 5 10 25 # View the entire dataset in RStudio View(csv.dataset) "],["dataorg.html", "Chapter 3 The Basics for Data Organization 3.1 Basic Data Manipulation", " Chapter 3 The Basics for Data Organization 3.1 Basic Data Manipulation 3.1.1 Merging 3.1.2 Merging processed data with metadata file? 3.1.3 Cast 3.1.4 Melt 3.1.5 Filtering &amp; subsetting 3.1.6 Tidyverse stuff (pivots) "],["finding-and-visualizing-data-trends.html", "Chapter 4 Finding and Visualizing Data Trends 4.1 Heat maps 4.2 Clustering 4.3 Data reduction (PCA) 4.4 Basic Statistical Tests and Visualizations of Data", " Chapter 4 Finding and Visualizing Data Trends 4.1 Heat maps 4.1.1 pheatmap 4.1.2 heatmap2 4.1.3 superheat 4.2 Clustering Examples with genomics: Rager et al. 2014 4.2.1 Hierarchical 4.2.2 K-means 4.3 Data reduction (PCA) 4.3.1 Visualize PCA Plot 4.3.2 Identify % of variance captured 4.4 Basic Statistical Tests and Visualizations of Data Need an example dataset – maybe ELGAN shuffled/deidentified, with made-up environmental exposure column? 4.4.1 Normality Kruskal wallis? The other one that I always forget? Shapiro wilks? - histogram 4.4.2 T-tests – column charts 4.4.3 Regression: linear regression and logistic regression 4.4.4 ANOVA + delicate commentary 4.4.5 Chi-squared test – box plots 4.4.6 Fisher’s exact test "],["multi-omics-analyses-for-environmental-health.html", "Chapter 5 Multi-Omics Analyses for Environmental Health 5.1 Exposomics 5.2 Transcriptomics 5.3 Genome-wide MicroRNA 5.4 Genome-wide DNA Methylation 5.5 Proteomics", " Chapter 5 Multi-Omics Analyses for Environmental Health 5.1 Exposomics 5.1.1 Placenta Exposome about to be submitted to EI Dust NTA data 5.2 Transcriptomics 5.2.1 DESeq2 / RNAseq Wildfire dataset, available through GEO 5.3 Genome-wide MicroRNA Rager et al. 2014 miRNAs 5.4 Genome-wide DNA Methylation 5.4.1 Illumina array data https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE58499 https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE28368 5.5 Proteomics Bailey et al arsenic dataset maybe? "],["mixtures-analyses-for-environmental-health.html", "Chapter 6 Mixtures Analyses for Environmental Health 6.1 Sufficient Similarity 6.2 Mixtures Modeling through qgcomp", " Chapter 6 Mixtures Analyses for Environmental Health 6.1 Sufficient Similarity Botanicals example with chemistry and tox profiling – Julia has dataset 6.2 Mixtures Modeling through qgcomp Could use published wildfire analysis here (Rager et al. 2021, STOTEN), or online example provide through Alex Keil’s studies "],["environmental-health-databases.html", "Chapter 7 Environmental Health Databases 7.1 Comparative Toxicogenomics Database (CTD) 7.2 Gene Expression Omnibus (GEO) 7.3 NHANES", " Chapter 7 Environmental Health Databases 7.1 Comparative Toxicogenomics Database (CTD) 7.2 Gene Expression Omnibus (GEO) 7.3 NHANES "]]
