[["index.html", "The inTelligence And Machine lEarning (TAME) Toolkit for Introductory Data Science, Chemical-Biological Analyses, Predictive Modeling, and Database Mining for Environmental Health Research Preface", " The inTelligence And Machine lEarning (TAME) Toolkit for Introductory Data Science, Chemical-Biological Analyses, Predictive Modeling, and Database Mining for Environmental Health Research Kyle R. Roell, Lauren E. Koval, Rebecca Boyles, Grace Patlewicz, Caroline Ring, Cynthia Rider, Cavin Ward-Caviness, David M. Reif, Ilona Jaspers, Rebecca C. Fry, and Julia E. Rager Preface Background Research in exposure science, epidemiology, toxicology, and environmental health is becoming increasingly reliant upon data science and computational methods that can more efficiently extract information from complex datasets. These methods can be leveraged to better identify relationships between exposures to stressors in the environment and human disease outcomes. Still, there remains a critical gap surrounding the training of researchers on these in silico methods. Objectives We aimed to address this critical gap by developing the inTelligence And Machine lEarning (TAME) Toolkit, promoting trainee-driven data generation, management, and analysis methods to “TAME” data in environmental health studies. This toolkit encompasses training modules, organized as chapters within this Github Bookdown site. All underlying code (in RMarkdown), input files, and imported graphics for these modules can be found at the parent UNC-SRP Github Page. Module Development Overview Training modules were developed to provide applications-driven examples of data organization and analysis methods that can be used to address environmental health questions. Target audiences for these modules include students and professionals in academia, government, and industry that are interested in expanding their skillset. Modules were developed by study coauthors using annotated script formatted for R/RStudio coding language and interface and were organized into three chapters. The first group of modules focused on introductory data science, which included the following topics: setting up R/RStudio and coding in the R environment; data organization basics; finding and visualizing data trends; high-dimensional data visualizations with heat maps; and Findability, Accessibility, Interoperability, and Reusability (FAIR) data management practices. The second chapter of modules incorporated chemical-biological analyses and predictive modeling, spanning the following methods: dose-response modeling; machine learning and predictive modeling; mixtures analyses; -omics analyses; toxicokinetic modeling; and read-across toxicity predictions. The last chapter of modules was organized to provide examples on environmental health database mining and integration, including chemical exposure, health outcome, and environmental justice data. Please note that these training modules describe example techniques that can be used to carry out these types of data analyses. We encourage participants to review the additional resources listed above, as well as the resources referenced throughout this training module, when designing and completing similar research to meet the unique needs of their study. The overall organization of this TAME toolkit is summarized below. Modules are organized into three chapters, that are listed on the left side of this website. Concluding Remarks Together, this valuable resource provides unique opportunities to obtain introductory-level training on current data analysis methods applicable to 21st century exposure science, toxicology, and environmental health. These modules serve as applications-based examples on how to “TAME” data within the environmental health research field, expanding the toolbox for career development and cross-training of scientists in multiple specialties, as well as supporting the next generation of data scientists. Funding This study was supported by the National Institutes of Health (NIH) from the National Institute of Environmental Health Sciences, including the following grant funds and associated programs: P42ES031007: The University of North Carolina (UNC)-Superfund Research Program (SRP) seeks to develop new solutions for reducing exposure to inorganic arsenic and prevent arsenic-induced diabetes through mechanistic and translational research. The UNC-SRP is Directed by Dr. Rebecca C. Fry. The UNC-SRP Data Analysis and Management Core (UNC-SRP-DMAC) provides the UNC-SRP with critical expertise in bioinformatics, statistics, data management, and data integration. Dr. Julia E. Rager is a Leader of the UNC-SRP-DMAC. T32ES007126: The UNC Curriculum in Toxicology and Environmental Medicine (CiTEM) seeks to provide a cutting edge research and mentoring environment to train students and postdoctoral fellows in environmental health and toxicology. Towards this goal, the CiTEM has a T32 Training Program for Pre- and Postdoctoral Training in Toxicology to support the development of future investigators in environmental health and toxicology. This training program has received supplement funds to expand training efforts centered on data management and data science practices to address current health issues in toxicology and environmental science. The UNC CiTEM is Directed by Dr. Ilona Jaspers. Support was additionally provided through the Institute for Environmental Health Solutions (IEHS) at the University of North Carolina (UNC) Gillings School of Global Public Health. The IEHS is aimed at protecting those who are particularly vulnerable to diseases caused by environmental factors, putting solutions directly into the hands of individuals and communities of North Carolina and beyond. The IEHS is Directed by Dr. Rebecca C. Fry. Author Affiliations Kyle Roell1,†, Lauren Koval1,2,†, Rebecca Boyles3, Grace Patlewicz4, Caroline Ring4, Cynthia Rider5, Cavin Ward-Caviness6, David M. Reif7, Ilona Jaspers1,2,8,9,10, Rebecca C. Fry1,2,8, and Julia E. Rager1,2,8,9 1The Institute for Environmental Health Solutions, Gillings School of Global Public Health, The University of North Carolina at Chapel Hill, Chapel Hill, North Carolina, USA 2Department of Environmental Sciences and Engineering, Gillings School of Global Public Health, The University of North Carolina at Chapel Hill, Chapel Hill, North Carolina, USA 3Research Computing, RTI International, Durham North Carolina, USA 4Center for Computational Toxicology and Exposure, US Environmental Protection Agency, Chapel Hill, North Carolina, USA 5Division of the National Toxicology Program, National Institute of Environmental Health Sciences, Research Triangle Park, North Carolina, USA 6Center for Public Health and Environmental Assessment, US Environmental Protection Agency, Chapel Hill, North Carolina, USA 7Bioinformatics Research Center, Department of Biological Sciences, North Carolina State University, Raleigh, North Carolina, USA 8Curriculum in Toxicology and Environmental Medicine, School of Medicine, University of North Carolina, Chapel Hill, North Carolina, USA 9Center for Environmental Medicine, Asthma and Lung Biology, School of Medicine, University of North Carolina, Chapel Hill, North Carolina, USA 10Department of Pediatrics, Microbiology and Immunology, School of Medicine, University of North Carolina, Chapel Hill, North Carolina, USA †These authors have contributed equally to this work and share first authorship "],["introduction-to-coding-in-r.html", "1.1 Introduction to Coding in R Introduction to Training Module R Packages Scripting Basics Concluding Remarks", " 1.1 Introduction to Coding in R This training module was developed by Dr. Kyle R. Roell and Dr. Julia E. Rager Fall 2021 Introduction to Training Module What is R? Computer script can be used to increase data analysis reproducibility, transparency, and methods sharing, and is becoming increasingly incorporated into exposure science, toxicology, and environmental health research. One of the most utilized coding language in this research field is the R coding language. Some advantages of using R include the following: Free, open-source programming language that is licensed under the Free Software Foundation’s GNU General Public License Can be run across all major platforms and operating systems, including Unix, Windows, and MacOS Publicly available packages help you carry out analyses efficiently (without having to code for everything yourself) Large, diverse collection of packages Comprehensive documentation When code is efficiently tracked during development/execution, it promotes reproducible analyses Because of these advantages, R has emerged as an avenue for world-wide collaboration in data science. Downloading and Installing R First, users should download R by navigating to the following website: https://www.r-project.org/ And then clicking the ‘download R’ link: This link will navigate you to the CRAN mirror website. Click on the CRAN mirror location that seems closest to your usual location: This will lead you to the selected CRAN Network website (here, https://archive.linux.duke.edu/cran/), where you will again select a download option: Then, select the top (representing the most recent) available .pkg file to download, and then install according to your computer’s typical program installation steps. Downloading and Installing R Studio What is R Studio? RStudio is an Integrated Development Environment (IDE) for R, which makes it more ‘user friendly’ when developing and using R script. RStudio Desktop is a desktop application that can be downloaded for free, online. To download RStudio: Navigate to: https://rstudio.com/products/rstudio/download/ Select the free RStudio Desktop option, and click “DOWNLOAD” Then download the top (most recent) RStudio Desktop option for your operating system Here is a screenshot of what R script looks like within RStudio: R Packages One of the major benefits to coding in the R language is access to the continually expanding resource of thousands of user-developed packages that aid in improved data analyses and methods sharing. Packages have varying utilities, spanning basic organization and manipulation of data, visualizing data, and more advanced approaches to parse and analyze data, with examples included in all of the proceeding training modules. In brief, packages represent compilations of code fitted for a specialized focus or purpose. These are often written by R users and submitted to the CRAN, or another host such as BioConductor or Github. Examples of some common packages that we’ll be using throughout these training modules include the following: tidyverse: A collection of open source R packages that share an underlying design philosophy, grammar, and data structures of tidy data. For more information on the tidyverse package, see its associated CRAN webpage, primary webpage, and peer-reviewed article released in 2018. ggplot2: A system, within the tidyverse collection, for declaratively creating graphics. Users provide data they would like to plot, and then control how ggplot2 should map variables to aesthetics and define which graphical primitives to use; ggplot2 then drafts the corresponding visualizations. For more information on the ggplot2 package, see its associated CRAN webpage and R Documentation. More information on these packages, as well as many others, is included throughout the training modules. Downloading/Installing R Packages R packages often do not need to be downloaded from a website. Instead, you can just load packages through running script in R, like: install.packages(“tidyverse”) library(tidyverse) It is worth noting that a function can be queried in RStudio by typing a question mark before the name of the function. For example: ?install.packages This will bring up documentation in the viewer window. Scripting Basics Comments R allows for scripts to contain non-code elements, called comments, that will not be run or interpreted. Comments are commonly included when annotating code, or describing what your code does, where your data came from, and just general textual reminders throughout the script. To make a comment, simply use a # followed by the comment. A # only comments out a single line of code. In other words, only that line will be commented and therefore not be run, but lines directly above/below it still will be # This is an R comment! Comments are useful to help make code more interpretable for others or to add reminders of what and why parts of code may have been written. Starting Code RStudio will autofill function names, variable names, etc. by pressing tab while typing. If multiple matches are found, RStudio will provide you with a drop down list to select from, which may be useful when searching through newly installed packages or trying to quickly type variable names in an R script. One of the first lines of code in any script will likely include the loading of packages needed to run the script. Here is an example line of code to load a package: # Loading ggplot2 package (should already be installed as a base package) library(ggplot2) Many packages also exist as part of the baseline configuration of an R working environment, and do not require manual loading each time you launch R. These include the following packages: datasets graphics methods stats utils Setting Your Working Directory Another step that is commonly done at the very beginning of your code is setting your working direction. This points to where you have files that you want to upload / where the default is to deposit output files produced during your scripted activities. You must set the working directory to a local directory where data are located or output files will be saved. To view where your current working directory is (by default), run the following: Show your working directory getwd() To set the location of your working directory, run the following: Set your working directory setwd(&quot;/filepath to where your input files are&quot;) Note that in macOS, filepaths use “/” as folder separaters; whereas in PCs, filepaths use “\\”. Importing Files After setting the working directory, importing and exporting files can be done using various functions based on the type of file being read or written. Often, it is easiest to import data into R that are in a comma separated values / comma delimited file (.csv) or tab / text delimited file (.txt). Other data types such as SAS data files, large .csv files, etc. may require different functions to be more efficiently read in and some of these file formats will be discussed in future modules. # Read in the .csv data that&#39;s located in our working directory csv.dataset &lt;- read.csv(&quot;Module1_1/Module1_1_ExampleData.csv&quot;) # Read in the .txt data txt.dataset &lt;- read.table(&quot;Module1_1/Module1_1_ExampleData.txt&quot;) These datasets now appear as saved dataframes (“csv.dataset” and “txt.dataset”) in our working environment in R. Viewing Data After data have been loaded into R, or created within R, you will likely want to view what these datasets look like. Datasets can be viewed in their entirety, or datasets can be subsetted to quickly look at part of the data. Here’s some example script to view just the beginnings of a dataframe, using the “head” function in R (a part of the baseline packages) head(csv.dataset) ## Sample Var1 Var2 Var3 ## 1 sample1 1 2 1 ## 2 sample2 2 4 4 ## 3 sample3 3 6 9 ## 4 sample4 4 8 16 ## 5 sample5 5 10 25 Here, you can see that this automatically brings up a view of the first five rows of the dataframe. Another way to view the first five rows of a dataframe is to run the following: csv.dataset[1:5,] ## Sample Var1 Var2 Var3 ## 1 sample1 1 2 1 ## 2 sample2 2 4 4 ## 3 sample3 3 6 9 ## 4 sample4 4 8 16 ## 5 sample5 5 10 25 Expanding on this, to view the first 5 rows x 2 columns, run the following: csv.dataset[1:5,1:2] ## Sample Var1 ## 1 sample1 1 ## 2 sample2 2 ## 3 sample3 3 ## 4 sample4 4 ## 5 sample5 5 To view the entire dataset in RStudio, use the “View” function: View(csv.dataset) Exporting Data Now that we have these datasets saved as dataframes, we can use these as examples to export data files from the R environment back into our local directory. There are many ways to export data in R. Data can be written out into a .csv file, tab delimited .txt file, RData file, etc. There are also many functions within packages that write out specific datasets generated by that package. To write out to a .csv file: write.csv(csv.dataset, &quot;Module1_1_SameCSVFileNowOut.csv&quot;) To write out a .txt tab delimited file: write.table(txt.dataset, &quot;Module1_1_SameTXTFileNowOut.txt&quot;) R also allows objects to be saved in RData files. These files can be read into R, as well, and will load the object into the current workspace. Entire workspaces are also able to be saved. # Read in saved single R data object (note that this file is not provided, just example code is for future reference) r.obj = readRDS(&quot;data.rds&quot;) # Write single R object to file (note that this file is not provided, just example code is for future reference) saveRDS(object, &quot;single_object.rds&quot;) # Read in multiple saved R objects (note that this file is not provided, just example code is for future reference) load(&quot;multiple_data.RData&quot;) # Save multiple R objects (note that this file is not provided, just example code is for future reference) save(object1, object2, &quot;multiple_objects.RData&quot;) # Save entire workspace save.image(&quot;entire_workspace.RData&quot;) # Load entire workspace load(&quot;entire_workspace.RData&quot;) Concluding Remarks Together, this training module provides introductory level information on installing and loading packages in R. Scripting basics are also included, such as setting a working directory, importing and exporting files, and viewing data within the R console / RStudio environment. Additional resources that provide introductory-level information on coding in R include the following: Coursera provides a lot of materials on learning how to program in R: https://www.coursera.org/learn/r-programming &amp; https://www.coursera.org/courses?query=r Stack overflow is a discussion forum for an online community of coders to discuss coding problems/challenges and ways to overcome these problems/challenges: https://stackoverflow.com/questions/1744861/how-to-learn-r-as-a-programming-language Wonderful tutorials are available online, like this one on ‘R for Data Science’: https://r4ds.had.co.nz/ BioConductor provides package-specific help: https://www.bioconductor.org/ An abundance of other resources are available online just by googling! "],["data-organization-basics.html", "1.2 Data Organization Basics Introduction to Training Module Data Manipulation using Base R Introduction to Tidyverse Concluding Remarks", " 1.2 Data Organization Basics This training module was developed by Dr. Kyle R. Roell and Dr. Julia E. Rager Fall 2021 Introduction to Training Module Data within the fields of exposure science, toxicology, and public health are very rarely prepared and ready for all statistical analyses/visualization code. The beginning of almost any scripted analysis includes important formatting steps. These steps largely encompass data organization, manipulation, and other steps in preparation for actual statistical analyses/visualizations. Data organization and manipulation generally refers to organizing and formatting data in a way that makes it easier to read and work with. This can be done through several approaches, including: Base R operations and functions, or A collection of packages (and philosophy) known as Tidyverse. In this training tutorial we will go over some of the most common ways you can organize and manipulate data, including: Merging data Filtering and subsetting data Melting and casting data These approaches will first be taught using the basic operations and functions in R. Then, the exact same approaches will be taught using the Tidyverse package and associated functions and syntax. These data manipulation and organization methods are demonstrated using an example environmentally relevant human cohort dataset. This cohort was generated by creating data distributions randomly pulled from our previously published cohorts, resulting in a bespoke dataset for these training purposes with associated demographic data and variable environmental exposure metrics from metal levels obtained using sources of drinking water and human urine samples. Set your working directory In preparation, first let’s set our working directory to the folderpath that contains our input files setwd(&quot;/filepath to where your input files are&quot;) Note that in macOS, filepaths use “/” as folder separaters; whereas in PCs, filepaths use “\". Importing example datasets Then let’s read in our example datasets demo.data &lt;- read.csv(&quot;Module1_2/Module1_2_DemographicData.csv&quot;) chem.data &lt;- read.csv(&quot;Module1_2/Module1_2_ChemicalData.csv&quot;) Viewing example datasets Let’s see what these datasets look like, starting with the chemical measures: dim(chem.data) ## [1] 200 7 The chemical measurement dataset includes 200 rows x 7 columns chem.data[1:10,1:7] ## ID DWAs DWCd DWCr UAs UCd UCr ## 1 1 6.426464 1.292941 51.67987 10.192695 0.7537104 42.60187 ## 2 2 7.832384 1.798535 50.10409 11.815088 0.9789506 41.30757 ## 3 3 7.516569 1.288461 48.74001 10.079057 0.1903262 36.47716 ## 4 4 5.906656 2.075259 50.92745 8.719123 0.9364825 42.47987 ## 5 5 7.181873 2.762643 55.16882 9.436559 1.4977829 47.78528 ## 6 6 9.723429 3.054057 51.14812 11.589403 1.6645837 38.26386 ## 7 7 6.268547 1.218410 52.08578 8.887948 0.6347667 39.45535 ## 8 8 6.718448 1.414975 54.96740 9.304968 0.6658849 45.09987 ## 9 9 9.074928 2.727755 55.72826 10.818153 1.6585757 42.58577 ## 10 10 5.771691 2.410993 47.06552 8.747217 1.7354305 34.80661 These data are organized according to subject ID (first column), followed by measures of: DWAs (drinking water arsenic levels in µg/L) DWCd (drinking water cadmium levels in µg/L) DWCr (drinking water chromium levels in µg/L) UAs (urinary arsenic levels in µg/L) UCd (urinary cadmium levels in µg/L) UCr (urinary chromium levels in µg/L) Now let’s view the demographic data: dim(demo.data) ## [1] 200 6 The subject demographic dataset includes 200 rows x 6 columns demo.data[1:10,1:6] ## ID BMI MAge MEdu BW GA ## 1 1 27.7 22.99928 3 3180.058 34 ## 2 2 26.8 30.05142 3 3210.823 43 ## 3 3 33.2 28.04660 3 3311.551 40 ## 4 4 30.1 34.81796 3 3266.844 32 ## 5 5 37.4 42.68440 3 3664.088 35 ## 6 6 33.3 24.94960 3 3328.988 40 ## 7 7 24.8 29.54798 3 3061.949 30 ## 8 8 16.9 24.94954 3 3332.539 38 ## 9 9 36.9 33.58589 3 3260.482 39 ## 10 10 21.7 39.29018 3 3141.723 35 These data are organized according to subject ID (first column) followed by the following subject information: BMI (body mass index) MAge (maternal age, years) MEdu (maternal education, 1= “less than high school”; 2= “high school or some college”; 3= “college or greater”) BW (body weight, grams) GA (gestational age, week) Data Manipulation using Base R Merging Data using Base R Syntax Merging datasets represents the joining together of two or more datasets, while connecting the datasets using a common identifier (generally some sort of ID). This is useful if you have multiple datasets describing different aspects of the study, different variables, or different measures across the same samples. Samples could correspond to the same study participants, animals, cell culture samples, environmental media samples, etc, depending on the study design. In the current example, we will be joining human demographic data and environmental metals exposure data collected from drinking water and human urine samples. Let’s start by merging the example demographic data with the chemical measurement data using the base R function of “merge”. To learn more about this function, you can type the following: ?merge which brings up helpful information in the R console To merge these datasets using the merge function, use the following code: # Note that we specify to merge these datasets by their shared ID column full.data &lt;- merge(demo.data, chem.data, by=&quot;ID&quot;) dim(full.data) ## [1] 200 12 This merged dataframe contains 200 rows x 12 columns Viewing this merged dataframe full.data[1:10, 1:12] ## ID BMI MAge MEdu BW GA DWAs DWCd DWCr UAs ## 1 1 27.7 22.99928 3 3180.058 34 6.426464 1.292941 51.67987 10.192695 ## 2 2 26.8 30.05142 3 3210.823 43 7.832384 1.798535 50.10409 11.815088 ## 3 3 33.2 28.04660 3 3311.551 40 7.516569 1.288461 48.74001 10.079057 ## 4 4 30.1 34.81796 3 3266.844 32 5.906656 2.075259 50.92745 8.719123 ## 5 5 37.4 42.68440 3 3664.088 35 7.181873 2.762643 55.16882 9.436559 ## 6 6 33.3 24.94960 3 3328.988 40 9.723429 3.054057 51.14812 11.589403 ## 7 7 24.8 29.54798 3 3061.949 30 6.268547 1.218410 52.08578 8.887948 ## 8 8 16.9 24.94954 3 3332.539 38 6.718448 1.414975 54.96740 9.304968 ## 9 9 36.9 33.58589 3 3260.482 39 9.074928 2.727755 55.72826 10.818153 ## 10 10 21.7 39.29018 3 3141.723 35 5.771691 2.410993 47.06552 8.747217 ## UCd UCr ## 1 0.7537104 42.60187 ## 2 0.9789506 41.30757 ## 3 0.1903262 36.47716 ## 4 0.9364825 42.47987 ## 5 1.4977829 47.78528 ## 6 1.6645837 38.26386 ## 7 0.6347667 39.45535 ## 8 0.6658849 45.09987 ## 9 1.6585757 42.58577 ## 10 1.7354305 34.80661 We can see that the merge function retained the first column in each original dataframe (ID), though did not replicate it since it was used as the identifier to merge off of. All other columns include their original data, just merged together by the IDs in the first column. These datasets were actually quite easy to merge, since they had the same exact column identifier and number of rows. You can edit your script to include more specifics in instances when these may differ across datasets that you would like to merge. For example: full.data &lt;- merge(demo.data, chem.data, by.x=&quot;ID&quot;, by.y=&quot;ID&quot;) # This option allows you to edit the column header text that is used in each # dataframe. Here, these are still the same &quot;ID&quot;, but you can see that adding # this script allows you to specify instances when differ header text is used. Filtering and Subsetting Data using Base R Syntax Filtering and subsetting data are useful tools when you need to focus your dataset to highlight data you are interested in analyzing downstream. These could represent, for example, specific samples or participants that meet certain criteria that you are interested in evaluating. It is also useful for simply removing particular variables or samples from dataframes as you are working through your script. These methods are illustrated here. For this example, let’s first define a vector of columns that we want to keep in our analysis subset.columns &lt;- c(&quot;BMI&quot;, &quot;MAge&quot;, &quot;MEdu&quot;) subset.columns ## [1] &quot;BMI&quot; &quot;MAge&quot; &quot;MEdu&quot; Now we can simply subset our data using those columns # Subsetting the data by selecting the columns represented in the defined # &#39;subset.columns&#39; vector subset.data1 &lt;- full.data[,subset.columns] # Viewing the top of this subsetted dataframe head(subset.data1) ## BMI MAge MEdu ## 1 27.7 22.99928 3 ## 2 26.8 30.05142 3 ## 3 33.2 28.04660 3 ## 4 30.1 34.81796 3 ## 5 37.4 42.68440 3 ## 6 33.3 24.94960 3 Conversely, if we want to remove all columns except those that we are interested in within the ‘subset.columns’ vector, we can write the code as follows (to achieve the same results). Note that we have to first create a vector of TRUE/FALSE’s here to execute the removal script written below: # First specify which columns we would like to remove remove.columns &lt;- colnames(full.data) %in% subset.columns # Viewing this new vector remove.columns ## [1] FALSE TRUE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE This creates a vector of TRUE/FALSE’s denoting whether or not each column is included in the ‘subset.columns’ vector Now we can subset our dataset. Here, we decide to keep those that are labeled ‘FALSE’ in the remove.columns vector. This will remove the columns that are NOT contained in the subset.columns vector subset.data2 &lt;- full.data[,!remove.columns] # Viewing the top of this dataframe head(subset.data2) ## ID BW GA DWAs DWCd DWCr UAs UCd UCr ## 1 1 3180.058 34 6.426464 1.292941 51.67987 10.192695 0.7537104 42.60187 ## 2 2 3210.823 43 7.832384 1.798535 50.10409 11.815088 0.9789506 41.30757 ## 3 3 3311.551 40 7.516569 1.288461 48.74001 10.079057 0.1903262 36.47716 ## 4 4 3266.844 32 5.906656 2.075259 50.92745 8.719123 0.9364825 42.47987 ## 5 5 3664.088 35 7.181873 2.762643 55.16882 9.436559 1.4977829 47.78528 ## 6 6 3328.988 40 9.723429 3.054057 51.14812 11.589403 1.6645837 38.26386 We can also easily subset data based on row numbers. For example, to keep only the first 100 rows: subset.data3 &lt;- full.data[1:100,] # Viewing the dimensions of this new dataframe dim(subset.data3) ## [1] 100 12 To remove the first 100 rows: subset.data4 &lt;- full.data[-c(1:100),] # Viewing the dimensions of this new dataframe dim(subset.data4) ## [1] 100 12 To filter data using conditional statements: subset.data5 &lt;- full.data[which(full.data$BMI &gt; 25 &amp; full.data$MAge &gt; 31),] # Viewing the top of this new dataframe head(subset.data5) ## ID BMI MAge MEdu BW GA DWAs DWCd DWCr UAs ## 4 4 30.1 34.81796 3 3266.844 32 5.906656 2.0752589 50.92745 8.719123 ## 5 5 37.4 42.68440 3 3664.088 35 7.181873 2.7626433 55.16882 9.436559 ## 9 9 36.9 33.58589 3 3260.482 39 9.074928 2.7277549 55.72826 10.818153 ## 13 13 33.7 33.82961 3 3481.293 36 7.101634 0.8443918 47.11677 9.967185 ## 22 22 25.7 37.08028 3 3387.046 43 7.207447 2.8088453 48.08648 9.446643 ## 31 31 28.4 47.85761 3 3173.033 30 6.032807 2.1929549 45.71856 9.917588 ## UCd UCr ## 4 0.9364825 42.47987 ## 5 1.4977829 47.78528 ## 9 1.6585757 42.58577 ## 13 -0.3466431 36.74220 ## 22 1.9891049 34.16921 ## 31 1.1194851 37.82297 Filtering data based on conditions can also be done using the subset function: subset.data6 &lt;- subset(full.data, BMI &gt; 25 &amp; MAge &gt; 31) Additionally, we can subset and select specific columns we would like to keep, using ‘select’ within the subset function: subset.data7 &lt;- subset(full.data, BMI &lt; 22 | BMI &gt; 27, select=c(&quot;BMI&quot;, &quot;MAge&quot;, &quot;MEdu&quot;)) For more information on the subset function, see its associated RDocumentation website. Melting and Casting Data using Base R Syntax Melting and casting refers to the conversion of data to “long” or “wide” form. You will often see data within the environmental health field in wide format; though long format is necessary for some procedures, such as plotting with ggplot2. Here, we’ll illustrate some example script to melt and cast data using the reshape2 package. Let’s first load the reshape2 library: library(reshape2) Using the fully merged dataframe, let’s remind ourselves what these data look like in the current dataframe format: head(full.data) ## ID BMI MAge MEdu BW GA DWAs DWCd DWCr UAs ## 1 1 27.7 22.99928 3 3180.058 34 6.426464 1.292941 51.67987 10.192695 ## 2 2 26.8 30.05142 3 3210.823 43 7.832384 1.798535 50.10409 11.815088 ## 3 3 33.2 28.04660 3 3311.551 40 7.516569 1.288461 48.74001 10.079057 ## 4 4 30.1 34.81796 3 3266.844 32 5.906656 2.075259 50.92745 8.719123 ## 5 5 37.4 42.68440 3 3664.088 35 7.181873 2.762643 55.16882 9.436559 ## 6 6 33.3 24.94960 3 3328.988 40 9.723429 3.054057 51.14812 11.589403 ## UCd UCr ## 1 0.7537104 42.60187 ## 2 0.9789506 41.30757 ## 3 0.1903262 36.47716 ## 4 0.9364825 42.47987 ## 5 1.4977829 47.78528 ## 6 1.6645837 38.26386 These data are represented by single subject identifiers listed as unique IDs per row, with associated environmental measures and demographic data organized across the columns. Thus, this dataframe is currently in wide (also known as casted) format. Let’s convert this dataframe to long (also known as melted) format: # Here, we are saying that we want a row for each unique # sample ID - variable measure pair full.melted &lt;- melt(full.data, id=&quot;ID&quot;) # Viewing this new dataframe head(full.melted) ## ID variable value ## 1 1 BMI 27.7 ## 2 2 BMI 26.8 ## 3 3 BMI 33.2 ## 4 4 BMI 30.1 ## 5 5 BMI 37.4 ## 6 6 BMI 33.3 You can see here that each measure that was originally contained as a unique column has been reoriented, such that the original column header is now listed throughout the second column labeled “variable”. Then, the third column contains the value of this variable. Let’s see an example view of the middle of this new dataframe full.melted[1100:1110,1:3] ## ID variable value ## 1100 100 DWAs 7.928885 ## 1101 101 DWAs 8.677403 ## 1102 102 DWAs 8.115183 ## 1103 103 DWAs 7.134189 ## 1104 104 DWAs 8.816142 ## 1105 105 DWAs 7.487227 ## 1106 106 DWAs 7.541973 ## 1107 107 DWAs 6.313516 ## 1108 108 DWAs 6.654474 ## 1109 109 DWAs 7.564429 ## 1110 110 DWAs 7.357122 Here, we can see a different variable (DWAs) now being listed. This continues throughout the entire dataframe, which has the following dimensions: dim(full.melted) ## [1] 2200 3 Thus, this dataframe is clearly melted, in long format. Let’s now re-cast this dataframe back into wide format using the ‘dcast’ function # Here, we are telling the dcast # function to give us a sample (ID) for every variable in the column labeled &#39;variable&#39;. # Then it automatically fills the dataframe with values from the &#39;value&#39; column full.cast &lt;- dcast(full.melted, ID ~ variable) head(full.cast) ## ID BMI MAge MEdu BW GA DWAs DWCd DWCr UAs ## 1 1 27.7 22.99928 3 3180.058 34 6.426464 1.292941 51.67987 10.192695 ## 2 2 26.8 30.05142 3 3210.823 43 7.832384 1.798535 50.10409 11.815088 ## 3 3 33.2 28.04660 3 3311.551 40 7.516569 1.288461 48.74001 10.079057 ## 4 4 30.1 34.81796 3 3266.844 32 5.906656 2.075259 50.92745 8.719123 ## 5 5 37.4 42.68440 3 3664.088 35 7.181873 2.762643 55.16882 9.436559 ## 6 6 33.3 24.94960 3 3328.988 40 9.723429 3.054057 51.14812 11.589403 ## UCd UCr ## 1 0.7537104 42.60187 ## 2 0.9789506 41.30757 ## 3 0.1903262 36.47716 ## 4 0.9364825 42.47987 ## 5 1.4977829 47.78528 ## 6 1.6645837 38.26386 Here, we can see that this dataframe is back in its original casted (or wide) format. Introduction to Tidyverse Tidyverse is a collection of packages that are commonly used to more efficiently organize and manipulate datasets in R. This collection of packages has its own specific type of syntax, dataset and formatting protocols that slightly differ from the Base R functions. Here, we will carry out all the of the same data organization exercises described above using Tidyverse. Downloading and Loading the Tidyverse Package If you don’t have tidyverse already installed, you will need to install it using: if(!require(tidyverse)) install.packages(&quot;tidyverse&quot;) And then load the tidyverse package using: library(tidyverse) Merging Data using Tidyverse Syntax To merge the same example dataframes using tidyverse, you can run the following script: full.data.tidy &lt;- inner_join(demo.data, chem.data, by=&quot;ID&quot;) # Note, for future scripting purposes, we can still merge with different IDs # using: by = c(&quot;ID.Demo&quot;=&quot;ID.Chem&quot;) head(full.data.tidy) ## ID BMI MAge MEdu BW GA DWAs DWCd DWCr UAs ## 1 1 27.7 22.99928 3 3180.058 34 6.426464 1.292941 51.67987 10.192695 ## 2 2 26.8 30.05142 3 3210.823 43 7.832384 1.798535 50.10409 11.815088 ## 3 3 33.2 28.04660 3 3311.551 40 7.516569 1.288461 48.74001 10.079057 ## 4 4 30.1 34.81796 3 3266.844 32 5.906656 2.075259 50.92745 8.719123 ## 5 5 37.4 42.68440 3 3664.088 35 7.181873 2.762643 55.16882 9.436559 ## 6 6 33.3 24.94960 3 3328.988 40 9.723429 3.054057 51.14812 11.589403 ## UCd UCr ## 1 0.7537104 42.60187 ## 2 0.9789506 41.30757 ## 3 0.1903262 36.47716 ## 4 0.9364825 42.47987 ## 5 1.4977829 47.78528 ## 6 1.6645837 38.26386 Filtering and Subsetting Data using Tidyverse Syntax To subset columns in tidyverse, run the following: subset.tidy1 = full.data.tidy %&gt;% select(all_of(subset.columns)) head(subset.tidy1) ## BMI MAge MEdu ## 1 27.7 22.99928 3 ## 2 26.8 30.05142 3 ## 3 33.2 28.04660 3 ## 4 30.1 34.81796 3 ## 5 37.4 42.68440 3 ## 6 33.3 24.94960 3 Note that you can also include column identifiers that may get dropped in the subsetting vector here: # Note that we&#39;re including a &#39;fake&#39; column here &#39;NotAColName&#39; to illustrate # how to incorporate additional columns; though this column gets dropped in # the next line of code subset.columns2 &lt;- c(subset.columns, &quot;NotAColName&quot;) # Viewing this new vector subset.columns2 ## [1] &quot;BMI&quot; &quot;MAge&quot; &quot;MEdu&quot; &quot;NotAColName&quot; subset.tidy2 &lt;- full.data.tidy %&gt;% select(any_of(subset.columns2)) # Viewing the top of this new dataframe head(subset.tidy2) ## BMI MAge MEdu ## 1 27.7 22.99928 3 ## 2 26.8 30.05142 3 ## 3 33.2 28.04660 3 ## 4 30.1 34.81796 3 ## 5 37.4 42.68440 3 ## 6 33.3 24.94960 3 Note that the ‘fake’ column ‘NotAColName’ gets automatically dropped here To remove columns using tidyverse, you can run the following: # Removing columns subset.tidy3 &lt;- full.data.tidy %&gt;% select(-subset.columns) # Viewing this new dataframe head(subset.tidy3) ## ID BW GA DWAs DWCd DWCr UAs UCd UCr ## 1 1 3180.058 34 6.426464 1.292941 51.67987 10.192695 0.7537104 42.60187 ## 2 2 3210.823 43 7.832384 1.798535 50.10409 11.815088 0.9789506 41.30757 ## 3 3 3311.551 40 7.516569 1.288461 48.74001 10.079057 0.1903262 36.47716 ## 4 4 3266.844 32 5.906656 2.075259 50.92745 8.719123 0.9364825 42.47987 ## 5 5 3664.088 35 7.181873 2.762643 55.16882 9.436559 1.4977829 47.78528 ## 6 6 3328.988 40 9.723429 3.054057 51.14812 11.589403 1.6645837 38.26386 Subsetting rows using tidyverse: # Selecting to retain the first 100 rows subset.tidy4 &lt;- full.data.tidy %&gt;% slice(1:100) dim(subset.tidy4) ## [1] 100 12 # Selecting to remove the first 100 rows subset.tidy5 &lt;- full.data.tidy %&gt;% slice(-c(1:100)) dim(subset.tidy5) ## [1] 100 12 Filtering data based on conditional statements using tidyverse: subset.tidy6 &lt;- full.data.tidy %&gt;% filter(BMI &gt; 25 &amp; MAge &gt; 31) dim(subset.tidy6) ## [1] 49 12 Another example of a conditional statement that can be used to filter data: subset.tidy7 &lt;- full.data.tidy %&gt;% filter(BMI &gt; 25 &amp; MAge &gt; 31) %&gt;% select(BMI, MAge, MEdu) Melting and Casting Data using Tidyverse Syntax To melt and cast data in tidyverse, you can use the ‘pivot’ functions (i.e., ‘pivot_longer’ or ‘pivot_wider’). These are exemplified below. Melting to long format using tidyverse: full.pivotlong &lt;- full.data.tidy %&gt;% pivot_longer(-ID, names_to = &quot;var&quot;, values_to = &quot;value&quot;) head(full.pivotlong, 15) ## # A tibble: 15 × 3 ## ID var value ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 BMI 27.7 ## 2 1 MAge 23.0 ## 3 1 MEdu 3 ## 4 1 BW 3180. ## 5 1 GA 34 ## 6 1 DWAs 6.43 ## 7 1 DWCd 1.29 ## 8 1 DWCr 51.7 ## 9 1 UAs 10.2 ## 10 1 UCd 0.754 ## 11 1 UCr 42.6 ## 12 2 BMI 26.8 ## 13 2 MAge 30.1 ## 14 2 MEdu 3 ## 15 2 BW 3211. Casting to wide format using tidyverse: full.pivotwide &lt;- full.pivotlong %&gt;% pivot_wider(names_from = &quot;var&quot;, values_from=&quot;value&quot;) head(full.pivotwide) ## # A tibble: 6 × 12 ## ID BMI MAge MEdu BW GA DWAs DWCd DWCr UAs UCd UCr ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 27.7 23.0 3 3180. 34 6.43 1.29 51.7 10.2 0.754 42.6 ## 2 2 26.8 30.1 3 3211. 43 7.83 1.80 50.1 11.8 0.979 41.3 ## 3 3 33.2 28.0 3 3312. 40 7.52 1.29 48.7 10.1 0.190 36.5 ## 4 4 30.1 34.8 3 3267. 32 5.91 2.08 50.9 8.72 0.936 42.5 ## 5 5 37.4 42.7 3 3664. 35 7.18 2.76 55.2 9.44 1.50 47.8 ## 6 6 33.3 24.9 3 3329. 40 9.72 3.05 51.1 11.6 1.66 38.3 Concluding Remarks Together, this training module provides introductory level information on the basics of data organization in R. The important data organization / manipulation methods of merging, filtering, subsetting, melting, and casted are presented on an environmentally relevant dataset. "],["finding-and-visualizing-data-trends.html", "1.3 Finding and Visualizing Data Trends Introduction to Training Module Basic Data Analysis Regression Modeling Categorical Data Analysis Concluding Remarks", " 1.3 Finding and Visualizing Data Trends This training module was developed by Dr. Kyle R. Roell and Dr. Julia E. Rager Fall 2021 Introduction to Training Module This training module provides a brief introduction to some of the most commonly implemented statistics and associated visualizations used in exposure science, epidemiological, toxicology, and environmental health studies. This module first uploads an example dataset that is similar to the data used in the previous data organization training module, though includes some expanded subject information data to allow for more example statistical tests. Then, methods to evaluate data normality are presented, including visualization-based approaches using histograms and Q-Q plots as well as statistical-based approaches. Basic statistical tests are then presented, including the t-test, analysis of variance, regression modeling, chi-squared test, and Fischer’s exact test. These statistical tests are very simple, with more extensive examples and associated descriptions of statistical models in the proceeding applications-based training modules. Script Preparations Cleaning the global environment rm(list=ls()) Installing required R packages If you already have these packages installed, you can skip this step, or you can run the below code which checks installation status for you if (!requireNamespace(&quot;tidyverse&quot;)) install.packages(&quot;tidyverse&quot;); Loading R packages required for this session # All tidyverse packages, including dplyr and ggplot2 library(tidyverse) Set your working directory setwd(&quot;/filepath to where your input files are&quot;) Importing example datasets Then let’s read in our example dataset. Note that these data are similar to those used in the previous training module on data organization, except that demographic and chemical measurement data were previously merged, and a few additional columns of subject information/demographics were added to serve as more thorough examples of data for use in this training module. full.data &lt;- read.csv(&quot;Module1_3/Module1_3_FullDemoChemData.csv&quot;) Viewing example datasets Let’s see what this dataset looks like: dim(full.data) ## [1] 200 15 This dataset includes 200 rows x 15 columns Let’s view the top of the first 9 columns of data in this dataframe: full.data[1:10,1:9] ## ID BMI BMIcat MAge MEdu BW GA Smoker Smoker3 ## 1 1 27.7 Overweight 22.99928 3 3180.058 34 0 Never ## 2 2 26.8 Overweight 30.05142 3 3210.823 43 1 Never ## 3 3 33.2 Overweight 28.04660 3 3311.551 40 0 Never ## 4 4 30.1 Overweight 34.81796 3 3266.844 32 1 Never ## 5 5 37.4 Overweight 42.68440 3 3664.088 35 0 Never ## 6 6 33.3 Overweight 24.94960 3 3328.988 40 0 Never ## 7 7 24.8 Overweight 29.54798 3 3061.949 30 0 Never ## 8 8 16.9 Underweight 24.94954 3 3332.539 38 0 Current ## 9 9 36.9 Overweight 33.58589 3 3260.482 39 0 Never ## 10 10 21.7 Normal 39.29018 3 3141.723 35 0 Current These represent the subject information/demographic data, which include the following columns: ID (subject number) BMI (body mass index) BMIcat (BMI &lt;= 18.5 binned as “Underweight”; 18.5 &lt; BMI &lt;= 24.5 binned as “Normal”; BMI &gt; 24.5 binned as “Overweight”) MAge (maternal age, years) MEdu (maternal education, 1= “less than high school”; 2= “high school or some college”; 3= “college or greater”) BW (body weight, grams) GA (gestational age, week) Smoker (0= non-smoker; 1=smoker) Smoker3 (“Never”, “Former”, or “Current” smoking status) Let’s now view the remaining columns (columns 10-15) in this dataframe: full.data[1:10,10:15] ## DWAs DWCd DWCr UAs UCd UCr ## 1 6.426464 1.292941 51.67987 10.192695 0.7537104 42.60187 ## 2 7.832384 1.798535 50.10409 11.815088 0.9789506 41.30757 ## 3 7.516569 1.288461 48.74001 10.079057 0.1903262 36.47716 ## 4 5.906656 2.075259 50.92745 8.719123 0.9364825 42.47987 ## 5 7.181873 2.762643 55.16882 9.436559 1.4977829 47.78528 ## 6 9.723429 3.054057 51.14812 11.589403 1.6645837 38.26386 ## 7 6.268547 1.218410 52.08578 8.887948 0.6347667 39.45535 ## 8 6.718448 1.414975 54.96740 9.304968 0.6658849 45.09987 ## 9 9.074928 2.727755 55.72826 10.818153 1.6585757 42.58577 ## 10 5.771691 2.410993 47.06552 8.747217 1.7354305 34.80661 These columns represent the environmental exposure measures, including: DWAs (drinking water arsenic levels in µg/L) DWCd (drinking water cadmium levels in µg/L) DWCr (drinking water chromium levels in µg/L) UAs (urinary arsenic levels in µg/L) UCd (urinary cadmium levels in µg/L) UCr (urinary chromium levels in µg/L) Now that the script is prepared and the data are uploaded, we can start running some basic statistical tests and visualizations of data trends. Basic Data Analysis Visualize and Test Data for Normality When selecting the appropriate statistical tests to evaluate potential trends in your data, statistical test selection often relies upon whether or not the underlying data are normally distributed. Many statistical tests and methods that are commonly implemented in exposure science, toxicology, and environmental health research rely on assumptions of normality. Thus, one of the most common statistic tests to perform at the beginning of an analysis is a test for normality. There are a few ways to evaluate the normality of a dataset: First, you can visually gage whether a dataset appears to be normally distributed through plots. For example, plotting data using histograms, densities, or Q-Q plots can graphically help inform if a variable’s values appear to be normally distributed or not. Second, you can evaluate normality using statistical tests, such as the Kolmogorov-Smirnov (K-S) test and Shapiro-Wilk test. When using these tests and interpreting their results, it is important to remember that the null hypothesis is that the sample distribution is normal, and a significant p-value means the distribution is non-normal. Let’s start with the first approach, based on data visualizations. Here, let’s begin with a histogram to view the distribution of BMI data, as an example. hist(full.data$BMI) We can edit some of the parameters to improve this basic histogram visualization. For example, we can decrease the size of each bin using breaks parameter: hist(full.data$BMI, breaks=20) Let’s also view the Q–Q (quantile-quantile) plot using the qqnorm function qqnorm(full.data$BMI) # Adding a reference line for theoretically normally distributed data qqline(full.data$BMI) From these visualizations, the BMI variable appears to be normally distributed, with data centered in the middle and spreading with a distribution on both the lower and upper sides that follow typical normal data distributions. Let’s now implement the second approach, based on statistical tests for normality. Here, let’s use the Shapiro-Wilk test as an example, again looking at the BMI data. This test can be carried out simply using the shapiro.test function from the base R stats package. shapiro.test(full.data$BMI) ## ## Shapiro-Wilk normality test ## ## data: full.data$BMI ## W = 0.99232, p-value = 0.3773 This test resulted in a p-value of 0.9014, so we cannot reject the null hypothesis (that data are normally distributed). This means that we can assume that these data are normally distributed. Two-Group Visualizations and Statistical Comparisons using the T-Test T-tests are commonly used to test for a significant difference between the means of two groups. In this example, we will be comparing BMI measures between two groups: smokers vs. non-smokers. We will specifically be implementing a two sample t-test (or independent samples t-test). Let’s first visualize the BMI data across these two groups using boxplots, for this example: boxplot(data=full.data, BMI ~ Smoker) From this plot, it looks like non-smokers (labeled 0) may have significantly higher BMI than smokers (labeled 1), though we need statistical evaluation of these data to more thoroughly evaluate this potential data trend. It is easy to peform a t-test on these data using the t.test function from the base R stats package: t.test(data=full.data, BMI ~ Smoker) ## ## Welch Two Sample t-test ## ## data: BMI by Smoker ## t = 2.5372, df = 80.362, p-value = 0.01311 ## alternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0 ## 95 percent confidence interval: ## 0.583061 4.823447 ## sample estimates: ## mean in group 0 mean in group 1 ## 26.11176 23.40851 From this statistical output, we can see that the overall mean BMI in group 0 (non-smokers) is 26, and the overall mean BMI in group 1 (smokers) is 23. We can also see that the resulting p-value comparison between the means of these two groups is, indeed, significant (p=0.0125), meaning that the means across these groups are significantly different (i.e., are not equal). It’s also helpful to save these results into a variable within the R global environment, which then allows us to access specific output values and extract them more easily for our records. For example, we can run the following to specifically extract the resulting p-value from this test: # Making a list in the R global environment with the statistical results ttest.res &lt;- t.test(data=full.data, BMI ~ Smoker) # Pulling the p-value ttest.res$p.value ## [1] 0.01310998 Two-Group Visualizations and Statistical Comparisons using an ANOVA Analysis of Variance (ANOVA) is a statistical method that can be used to compare means across more than two groups. To demonstrate an ANOVA test on this dataset, let’s evaluate BMI distributions across current vs. former vs. never smokers (using the ‘Smoker3’ variable from our dataset). Let’s again, start by viewing these data distributions using a boxplot: boxplot(data=full.data, BMI ~ Smoker3) Let’s also calculate the group means using tidyverse syntax and the summarise function, as helpful example script: # Can also get group means full.data %&gt;% group_by(Smoker3) %&gt;% summarise(mean(BMI)) ## # A tibble: 3 × 2 ## Smoker3 `mean(BMI)` ## &lt;chr&gt; &lt;dbl&gt; ## 1 Current 19.1 ## 2 Former 26.5 ## 3 Never 27.2 From this cursory review of the data, it looks like the current smokers likely demonstrate significantly different BMI measures than the former and never smokers, though we need statistical tests to verify this potential trend. We also require statistical tests to evaluate potential differences (or lack of differences) between former and never smokers. Let’s now run the ANOVA to compare BMI between smoking groups, using the aov function to fit an ANOVA model: aov(data=full.data, BMI ~ Smoker3) ## Call: ## aov(formula = BMI ~ Smoker3, data = full.data) ## ## Terms: ## Smoker3 Residuals ## Sum of Squares 2046.713 6817.786 ## Deg. of Freedom 2 197 ## ## Residual standard error: 5.882861 ## Estimated effects may be unbalanced We can extract the typical ANOVA results table using either summary or anova on the resulting fitted object anova(aov(data=full.data, BMI ~ Smoker3)) ## Analysis of Variance Table ## ## Response: BMI ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Smoker3 2 2046.7 1023.36 29.57 5.888e-12 *** ## Residuals 197 6817.8 34.61 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 From this ANOVA output table, we can conclude that the group means across all three groups are not equal. Regression Modeling Regression Modeling and Visualization: Linear and Logistic Regressions Regression modeling aims to find a relationship between a dependent variable (or outcome, response, y) and an independent variable (or predictor, explanatory variable, x). There are many forms of regression analysis, but here we will focus on two: linear regression and logistic regression. In brief, linear regression is generally used when you have a continuous dependent variable and there is assumed to be some sort of linear relationship between the dependent and independent variables. Conversely, logistic regression is often used when the dependent variable is dichotomous (0 or 1). Let’s first run through an example linear regression model. Linear Regression For this example, let’s evaluate the potential relationship between the subjects’ birthweight (BW) and BMI. Here, we will first visualize the data and a run simple correlation analysis to evaluate whether these data are generally correlated. Then, we will run a linear regression to evaluate the relationship between these variables in more detail. Plotting the variables against one another using the basic ‘plot’ function to produce a scatterplot: plot(data=full.data, BW ~ BMI) Running a basic correlation analyses between these two variables using the ‘cor’ function: cor(full.data$BW, full.data$BMI) ## [1] 0.2485164 The provides a correlation coefficient (R) value of 0.25. Let’s now use the ‘cor.test’ function to extract the correlation p-value: cor.res &lt;- cor.test(full.data$BW, full.data$BMI) cor.res$p.value ## [1] 0.0003876464 Checking to see that we get the same correlation coefficient (R) using this function: cor.res$estimate ## cor ## 0.2485164 Together, it looks like there may be a relationship between BW and BMI, based on these correlation results, demonstrating a significant p-value of 0.0004. To test this further, let’s run a linear regression analysis using the ‘lm’ function, using BMI as the independent variable (X) and BW as the dependent variable (Y): lm.res &lt;- lm(data=full.data, BW ~ BMI) # Viewing the results summary summary(lm.res) ## ## Call: ## lm(formula = BW ~ BMI, data = full.data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -598.39 -116.72 8.11 136.54 490.11 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3069.201 52.576 58.38 &lt; 2e-16 *** ## BMI 7.208 1.997 3.61 0.000388 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 188 on 198 degrees of freedom ## Multiple R-squared: 0.06176, Adjusted R-squared: 0.05702 ## F-statistic: 13.03 on 1 and 198 DF, p-value: 0.0003876 We can see here that the relationship between BMI and BW is shown to be significant, with a p-value of 0.000411 We can also derive confidence intervals for the BMI estimate using: confint(lm.res)[&quot;BMI&quot;,] ## 2.5 % 97.5 % ## 3.270873 11.145740 Notice that the r-squared (R^2) value in regression output is the squared value of the previously calculated correlation coefficient (R) sqrt(summary(lm.res)$r.squared) ## [1] 0.2485164 In epidemiological studies, the potential influence of confounders is considered by including important covariates within the final regression model. Here, let’s include the covariates of maternal age (MAge) and gestational age (GA) as an example for running a linear regression model with covariates: summary(lm(data=full.data, BW ~ BMI + MAge + GA)) ## ## Call: ## lm(formula = BW ~ BMI + MAge + GA, data = full.data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -454.04 -111.24 5.79 116.46 488.41 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2247.995 117.946 19.060 &lt; 2e-16 *** ## BMI 6.237 1.774 3.515 0.000547 *** ## MAge 4.269 1.887 2.263 0.024752 * ## GA 19.612 2.656 7.385 4.28e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 165.5 on 196 degrees of freedom ## Multiple R-squared: 0.2805, Adjusted R-squared: 0.2695 ## F-statistic: 25.47 on 3 and 196 DF, p-value: 5.884e-14 Here, we can see that BMI is still significantly associated with BW, and the included covariates are also shown to be related to BW in this model. Let’s further visualize these regression modeling results by adding a regression line to the original scatterplot: plot(data=full.data, BW ~ BMI) # Add a regression line to plot abline(lm(data=full.data, BW ~ BMI)) Collectively, these results demonstrate a significant relationship between BMI and BW, both when modeling with and without covariates. Logistic Regression To carry out a logistic regression, we need to evaluate one continuous variable (here, we select maternal education, using MEdu variable) and one dichotomous variable (here, we select smoking status, using the Smoker variable). When considering these data, we may hypothesize that higher levels of education are negatively associated with smoking status. In other words, those with higher education are less likely to smoke. Because smoking status is a dichotomous variable, we will use logistic regression to look at this relationship. Let’s first visualize these data using a boxplot for the dichotomous smoker dataset: boxplot(MEdu ~ Smoker, data=full.data) With this visualization, it’s difficult to tell whether or not there are significant differences in maternal education based on smoking status. Let’s now run the statistical analysis, using logistic regression modeling: # Use GLM (generalized linear model) and specify the family as binomial # this tells GLM to run a logistic regression log.res = glm(Smoker ~ MEdu, family = &quot;binomial&quot;, data=full.data) # Viewing the results summary(log.res) ## ## Call: ## glm(formula = Smoker ~ MEdu, family = &quot;binomial&quot;, data = full.data) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.8261 -0.7052 -0.7052 -0.7052 1.7398 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.7172 0.6031 -1.189 0.234 ## MEdu -0.1826 0.2305 -0.792 0.428 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 218.10 on 199 degrees of freedom ## Residual deviance: 217.49 on 198 degrees of freedom ## AIC: 221.49 ## ## Number of Fisher Scoring iterations: 4 Similar to the regression modeling analysis, we can also derive confidence intervals: confint(log.res)[&quot;MEdu&quot;,] ## Waiting for profiling to be done... ## 2.5 % 97.5 % ## -0.6262616 0.2839524 Collectively, these results show a non-significant p-value relating maternal education to smoking status. The confidence intervals also overlap across zero. Therefore, these data do not demonstrate a significant association between maternal education and smoking status. Categorical Data Analysis Chi-Squared Test and Fisher’s Exact Test Chi-squared test and Fisher’s exact tests are used primarily when evaluating data distributions between two categorical variables. The difference between a Chi-squared test and the Fisher’s exact test surrounds the specific procedure being run. Basically, the Chi-squared test is an approximation and is run with larger sample sizes to determine whether there is a statistically significant difference between the expected vs. observed frequencies in one or more categories of a contingency table. The Fisher’s exact test is similar, though is an exact measure that can be run on any sample size, including smaller sample sizes. For this example, we are interested in evaluating the potential relationship between two categorical variables: smoking status (using the ‘Smoker’ variable) and categorical BMI group (using the ‘BMIcat’ variable). To run these categorical statistical tests, let’s first create and view a 2-way contingency table, describing the frequencies of observations across the categorical BMI and smoking groups: ContingencyTable &lt;- with(full.data, table(BMIcat, Smoker)) ContingencyTable ## Smoker ## BMIcat 0 1 ## Normal 43 14 ## Overweight 87 22 ## Underweight 23 11 Now let’s run the Chi-squared test on this table: chisq.test(ContingencyTable) ## ## Pearson&#39;s Chi-squared test ## ## data: ContingencyTable ## X-squared = 2.1849, df = 2, p-value = 0.3354 This results in a p-value = 0.34, demonstrating that there is no significant relationship between BMI categories and smoking status based off this test. Note that we can also run the Chi-squared test using the following code, without having to generate the contingency table: chisq.test(full.data$BMI, full.data$Smoker) ## ## Pearson&#39;s Chi-squared test ## ## data: full.data$BMI and full.data$Smoker ## X-squared = 143.17, df = 144, p-value = 0.5039 Or: with(full.data, chisq.test(BMI, Smoker)) ## ## Pearson&#39;s Chi-squared test ## ## data: BMI and Smoker ## X-squared = 143.17, df = 144, p-value = 0.5039 Note that these all produce the same results. We can also run a Fisher’s Exact Test when considering smaller cell sizes. We won’t run this here due to computing time, but here is some example code for your records: # With small cell sizes, can use Fisher&#39;s Exact Test # fisher.test(full.data$BMI, full.data$Smoker) Concluding Remarks In conclusion, this training module serves as a high-level introduction to basic statistics and visualization methods. Statistical approaches described in this traiing module include tests for normality, t-test, analysis of variance, regression modeling, chi-squared test, and Fischer’s exact test. Visualization approaches include boxplots, histograms, scatterplots, and regression lines. These methods serve as an important foundation for nearly all studies carried out in environmental health research. "],["high-dimensional-data-visualizations.html", "1.4 High-Dimensional Data Visualizations Introduction to Training Module High-Dimensional Data Visualizations Density Plot Visualizations GGally Visualizations Boxplot Visualizations Correlation Plot Visualizations Hierarchical Clustering Visualizations Heat Map Visualizations Concluding Remarks", " 1.4 High-Dimensional Data Visualizations This training module was developed by Dr. Kyle R. Roell, Lauren E. Koval, and Dr. Julia E. Rager Fall 2021 Introduction to Training Module Visualizing data is an important step in any data analysis within environmental health research. Often, visualizations allow scientists to better understand trends and patterns within a particular dataset under evaluation. Even after statistical analysis of a dataset, it is important to then communicate these findings to a wide variety of target audiences. Visualizations are a vital part of communicating complex data and results to target audiences. There are many ways to visualize data, from simple scatter plots to more complicated heat maps. The previous training module included some example visualization methods while evaluating basic trends in environmental health datasets. Here, we expand upon these methods by highlighting some that can be used to visualize larger, more high-dimensional datasets. This training module specifically reviews the formatting of data in preparation of generating visualizations, scaling datasets, and then guides users through the generation of the following example data visualizations: Density plots GGally plots Boxplots Correlation plots Hierarchical clustering Heat maps These visualization approaches are demonstrated using a large environmental chemistry dataset. This example dataset was generated through chemical speciation analysis of smoke samples collected during lab-based simulations of wildfire events. Specifically, different biomass materials (eucalyptus, peat, pine, pine needles, and red oak) were burned under two combustion conditions of flaming and smoldering, resulting in the generation of 12 different smoke samples. These data have been previously published in the following example environmental health research studies, with data made publicly available: Rager JE, Clark J, Eaves LA, Avula V, Niehoff NM, Kim YH, Jaspers I, Gilmour MI. Mixtures modeling identifies chemical inducers versus repressors of toxicity associated with wildfire smoke. Sci Total Environ. 2021 Jun 25;775:145759. doi: 10.1016/j.scitotenv.2021.145759. Epub 2021 Feb 10. PMID: 33611182. Kim YH, Warren SH, Krantz QT, King C, Jaskot R, Preston WT, George BJ, Hays MD, Landis MS, Higuchi M, DeMarini DM, Gilmour MI. Mutagenicity and Lung Toxicity of Smoldering vs. Flaming Emissions from Various Biomass Fuels: Implications for Health Effects from Wildland Fires. Environ Health Perspect. 2018 Jan 24;126(1):017011. doi: 10.1289/EHP2200. PMID: 29373863. Script Preparations Cleaning the global environment rm(list=ls()) Installing required R packages If you already have these packages installed, you can skip this step, or you can run the below code which checks installation status for you if (!requireNamespace(&quot;GGally&quot;)) install.packages(&quot;GGally&quot;); if (!requireNamespace(&quot;superheat&quot;)) install.packages(&quot;superheat&quot;); if (!requireNamespace(&quot;pheatmap&quot;)) install.packages(&quot;pheatmap&quot;); if (!requireNamespace(&quot;viridis&quot;)) install.packages(&quot;viridis&quot;); Loading R packages required for this session library(ggplot2); library(GGally); library(superheat); library(pheatmap); library(corrplot); library(reshape2); library(viridis); Set your working directory setwd(&quot;/filepath to where your input files are&quot;) Importing example dataset Then let’s read in our example dataset. As mentioned in the introduction, this example dataset represents chemical measurements across 12 different biomass burn scenarios, representing potential wildfire events. Let’s upload and view these data: # Load the data SmokeData1 &lt;- read.csv(&quot;Module1_4/Module1_4_DataforVisualizations.csv&quot;) # View the top of the dataset head(SmokeData1) ## Chemical.Category Chemical CASRN Eucalyptus_Smoldering ## 1 n-Alkanes 2-Methylnonadecane 1560-86-7 0.06 ## 2 n-Alkanes 3-Methylnonadecane 6418-45-7 0.04 ## 3 n-Alkanes Docosane 629-97-0 0.21 ## 4 n-Alkanes Dodecylcyclohexane 1795-17-1 0.04 ## 5 n-Alkanes Eicosane 112-95-8 0.11 ## 6 n-Alkanes Heneicosane 629-94-7 0.13 ## Eucalyptus_Flaming Peat_Smoldering Peat_Flaming Pine_Smoldering Pine_Flaming ## 1 0.06 1.36 0.06 0.06 0.06 ## 2 0.04 1.13 0.90 0.47 0.04 ## 3 0.25 9.46 0.57 0.16 0.48 ## 4 0.04 0.25 0.04 0.04 0.04 ## 5 0.25 7.55 0.54 0.17 0.29 ## 6 0.28 6.77 0.34 0.13 0.42 ## Pine_Needles_Smoldering Pine_Needles_Flaming Red_Oak_Smoldering ## 1 0.06 0.06 0.06 ## 2 0.04 0.72 0.04 ## 3 0.32 0.18 0.16 ## 4 0.12 0.04 0.04 ## 5 0.28 0.16 0.15 ## 6 0.30 0.13 0.13 ## Red_Oak_Flaming Units ## 1 0.13 ng_per_uL ## 2 0.77 ng_per_uL ## 3 0.36 ng_per_uL ## 4 0.04 ng_per_uL ## 5 0.38 ng_per_uL ## 6 0.69 ng_per_uL Formatting dataframes for downstream visualization code For some of the visualizations below (e.g., heat maps) we’ll use data from the original full dataframe. The only formatting that needs to be done to this dataframe is to move the chemical names to the row names position of the dataframe: rownames(SmokeData1) &lt;- SmokeData1$Chemical # View the top of the reorganized dataset head(SmokeData1) ## Chemical.Category Chemical CASRN ## 2-Methylnonadecane n-Alkanes 2-Methylnonadecane 1560-86-7 ## 3-Methylnonadecane n-Alkanes 3-Methylnonadecane 6418-45-7 ## Docosane n-Alkanes Docosane 629-97-0 ## Dodecylcyclohexane n-Alkanes Dodecylcyclohexane 1795-17-1 ## Eicosane n-Alkanes Eicosane 112-95-8 ## Heneicosane n-Alkanes Heneicosane 629-94-7 ## Eucalyptus_Smoldering Eucalyptus_Flaming Peat_Smoldering ## 2-Methylnonadecane 0.06 0.06 1.36 ## 3-Methylnonadecane 0.04 0.04 1.13 ## Docosane 0.21 0.25 9.46 ## Dodecylcyclohexane 0.04 0.04 0.25 ## Eicosane 0.11 0.25 7.55 ## Heneicosane 0.13 0.28 6.77 ## Peat_Flaming Pine_Smoldering Pine_Flaming ## 2-Methylnonadecane 0.06 0.06 0.06 ## 3-Methylnonadecane 0.90 0.47 0.04 ## Docosane 0.57 0.16 0.48 ## Dodecylcyclohexane 0.04 0.04 0.04 ## Eicosane 0.54 0.17 0.29 ## Heneicosane 0.34 0.13 0.42 ## Pine_Needles_Smoldering Pine_Needles_Flaming ## 2-Methylnonadecane 0.06 0.06 ## 3-Methylnonadecane 0.04 0.72 ## Docosane 0.32 0.18 ## Dodecylcyclohexane 0.12 0.04 ## Eicosane 0.28 0.16 ## Heneicosane 0.30 0.13 ## Red_Oak_Smoldering Red_Oak_Flaming Units ## 2-Methylnonadecane 0.06 0.13 ng_per_uL ## 3-Methylnonadecane 0.04 0.77 ng_per_uL ## Docosane 0.16 0.36 ng_per_uL ## Dodecylcyclohexane 0.04 0.04 ng_per_uL ## Eicosane 0.15 0.38 ng_per_uL ## Heneicosane 0.13 0.69 ng_per_uL For most other visualizations below, we’ll need a separate dataframe that just contains the chemical concentration columns (specifically columns 4 through 13 from the above view). Let’s create this separate dataframe and call it SmokeData2. First, we’ll pull the chemical concentration values from columns 4 through 13. While we pull these values, let’s also check that they are recognized as numeric values using the ‘apply’ function. Here, we run the apply function across columns 4 through 13 (SmokeData[,4:13]) by columns (indicated by the number 2; if we wanted this by rows we would use the number 1), and coerce all values to numeric using the ‘as.numeric’ function. ## Eucalyptus_Smoldering Eucalyptus_Flaming Peat_Smoldering Peat_Flaming ## [1,] 0.06 0.06 1.36 0.06 ## [2,] 0.04 0.04 1.13 0.90 ## [3,] 0.21 0.25 9.46 0.57 ## [4,] 0.04 0.04 0.25 0.04 ## [5,] 0.11 0.25 7.55 0.54 ## [6,] 0.13 0.28 6.77 0.34 ## Pine_Smoldering Pine_Flaming Pine_Needles_Smoldering Pine_Needles_Flaming ## [1,] 0.06 0.06 0.06 0.06 ## [2,] 0.47 0.04 0.04 0.72 ## [3,] 0.16 0.48 0.32 0.18 ## [4,] 0.04 0.04 0.12 0.04 ## [5,] 0.17 0.29 0.28 0.16 ## [6,] 0.13 0.42 0.30 0.13 ## Red_Oak_Smoldering Red_Oak_Flaming ## [1,] 0.06 0.13 ## [2,] 0.04 0.77 ## [3,] 0.16 0.36 ## [4,] 0.04 0.04 ## [5,] 0.15 0.38 ## [6,] 0.13 0.69 For more information on the apply function, see its RDocumentation and a helpful tutorial on the R apply family of functions. Let’s now add back in the chemical identifiers, by grabbing the row names from the original dataframe, SmokeData1. ## Eucalyptus_Smoldering Eucalyptus_Flaming Peat_Smoldering ## 2-Methylnonadecane 0.06 0.06 1.36 ## 3-Methylnonadecane 0.04 0.04 1.13 ## Docosane 0.21 0.25 9.46 ## Dodecylcyclohexane 0.04 0.04 0.25 ## Eicosane 0.11 0.25 7.55 ## Heneicosane 0.13 0.28 6.77 ## Peat_Flaming Pine_Smoldering Pine_Flaming ## 2-Methylnonadecane 0.06 0.06 0.06 ## 3-Methylnonadecane 0.90 0.47 0.04 ## Docosane 0.57 0.16 0.48 ## Dodecylcyclohexane 0.04 0.04 0.04 ## Eicosane 0.54 0.17 0.29 ## Heneicosane 0.34 0.13 0.42 ## Pine_Needles_Smoldering Pine_Needles_Flaming ## 2-Methylnonadecane 0.06 0.06 ## 3-Methylnonadecane 0.04 0.72 ## Docosane 0.32 0.18 ## Dodecylcyclohexane 0.12 0.04 ## Eicosane 0.28 0.16 ## Heneicosane 0.30 0.13 ## Red_Oak_Smoldering Red_Oak_Flaming ## 2-Methylnonadecane 0.06 0.13 ## 3-Methylnonadecane 0.04 0.77 ## Docosane 0.16 0.36 ## Dodecylcyclohexane 0.04 0.04 ## Eicosane 0.15 0.38 ## Heneicosane 0.13 0.69 Scaling dataframes for downstream data visualizations A data preparation method that is commonly used to convert values into those that can be used to better illustrate overall data trends is data scaling. Scaling can be achieved through data transformations or normalization procedures, depending on the specific dataset and goal of analysis/visualization. Scaling is often carried out using data vectors or columns of a dataframe. For this example, we will normalize the chemical concentration dataset using a basic scaling and centering procedure using the base R function ‘scale’. This algorithm results in the normalization of a dataset using the mean value and standard deviation. This scaling step will convert chemical concentration values in our dataset into normalized values across samples, such that each chemical’s concentration distributions are more easily comparable between the different biomass burn conditions. For more information on the ‘scale’ function, see its associated RDocumentation and helpful tutorial on Implementing the scale() function in R. Let’s make a new dataframe (ScaledData) based off scaled values of the data within the SmokeData2 dataframe: ## Eucalyptus_Smoldering Eucalyptus_Flaming Peat_Smoldering ## 2-Methylnonadecane -0.3347765 -0.3347765 2.841935 ## 3-Methylnonadecane -0.8794448 -0.8794448 1.649829 ## Peat_Flaming Pine_Smoldering Pine_Flaming ## 2-Methylnonadecane -0.3347765 -0.3347765 -0.3347765 ## 3-Methylnonadecane 1.1161291 0.1183422 -0.8794448 ## Pine_Needles_Smoldering Pine_Needles_Flaming ## 2-Methylnonadecane -0.3347765 -0.3347765 ## 3-Methylnonadecane -0.8794448 0.6984509 ## Red_Oak_Smoldering Red_Oak_Flaming ## 2-Methylnonadecane -0.3347765 -0.1637228 ## 3-Methylnonadecane -0.8794448 0.8144726 We can see that each chemical is now scaled, centered around 0, with values distributed that are less than zero, and others that are greater than zero, based off a normal distribution. Let’s also create a melted (or long) dataframe and save it as ScaledData.melt: ScaledData.melt &lt;- melt(ScaledData) colnames(ScaledData.melt) &lt;- c(&quot;Chemical&quot;, &quot;Biomass_Burn_Condition&quot;, &quot;Scaled_Chemical_Concentration&quot;) #updating the column names ScaledData.melt[1:10,] ## Chemical Biomass_Burn_Condition Scaled_Chemical_Concentration ## 1 2-Methylnonadecane Eucalyptus_Smoldering -0.3347765 ## 2 3-Methylnonadecane Eucalyptus_Smoldering -0.8794448 ## 3 Docosane Eucalyptus_Smoldering -0.3465132 ## 4 Dodecylcyclohexane Eucalyptus_Smoldering -0.4240624 ## 5 Eicosane Eucalyptus_Smoldering -0.3802202 ## 6 Heneicosane Eucalyptus_Smoldering -0.3895328 ## 7 Hentriacontane Eucalyptus_Smoldering 0.1304235 ## 8 Heptacosane Eucalyptus_Smoldering 0.1291155 ## 9 Heptadecane Eucalyptus_Smoldering -0.3759424 ## 10 Hexacosane Eucalyptus_Smoldering 0.5060578 Now we have all the dataframes we need, formatted and ready to go for visualizations! High-Dimensional Data Visualizations Here, we provide some example data visualization approaches that can be used to visualize high-dimensional datasets of relevance to environmental health. Density Plot Visualizations Density plots are an effective way to show overall distributions of data and can be useful to compare across various test conditions or other stratifications of the data under evaluation. Here, we use ggplot to create density plots, displaying trends in chemical concentrations across the tested biomass burn conditions. We run the ggplot function over the scaled chemistry data to yield the following plot: ggplot(ScaledData.melt, aes(x=Scaled_Chemical_Concentration, color=Biomass_Burn_Condition)) + geom_density() Here are some interesting take-aways from viewing this density plot: + In general, there are a high number of chemicals that were measured at relatively lower abundances (hence, the peak in occurrence density occurring towards the left, before 0) + The three conditions of smoldering peat, flaming peat, and flaming pine contained the most chemicals at the highest relative concentrations (hence, these lines are the top three lines towards the right) GGally Visualizations GGally is a package that serves as an extension of ggplot2, the baseline R plotting system based on the grammer of graphics. GGally is very useful for creating plots that compare groups or features within a dataset, among many other utilities. Here we will demonstrate the ‘ggpairs’ function within GGally using the scaled chemistry datasets. This function will produce an image that shows correlation values between biomass burn sample pairs, and also illustrates the overall distributions of values in samples. # Using ggpairs to visualize data; note that we need to supply the data # specifically as a dataframe (hence the &#39;data.frame&#39; function) ggpairs(data.frame(ScaledData)) For more information on GGally see its associated RDocumentation and example helpful tutorial. Boxplot Visualizations As demonstrated in the previous module on identifying and visualizing data trends, boxplots have utility towards visualizing potential differences between data categories or groupings. Boxplots are very easy to make and still provide informative visualizations for between group comparisons. Here, we will generate an example boxplot visualization using ggplot ggplot(ScaledData.melt, aes(x=Scaled_Chemical_Concentration, color=Biomass_Burn_Condition)) + geom_boxplot() Correlation Plot Visualizations Correlation plots are used to display correlations among variables in a dataset. There are many approaches that can be used to generate correlation plot visualizations. Here, we demonstrate two different approaches: First, we demonstrate further utility of the GGally package towards the generation of correlation plots through the ‘ggcorr’ function: # Note that we need to supply the data specifically as a dataframe (hence the &#39;data.frame&#39; function) ggcorr(data.frame(ScaledData), size = 2) Second, we demonstrate a different function to produce correlation plot visualizations; namely, the ‘corrplot’ function. For the function ‘corrplot’ to work, it needs to be supplied a correlation matrix as the input data, which is demonstrated below. Example using the ‘corrplot’ function to visualize statistical correlations between biomass burn conditions: # Need to supply corrplot with a correlation matrix, here, using the &#39;cor&#39; function corrplot(cor(SmokeData2)) Example using the ‘corrplot’ function to visualize statistical correlations between measured chemicals: corrplot(cor(t(SmokeData2)), tl.cex = .4, # Change size of text tl.col = &#39;black&#39;); # Change font color to black Hierarchical Clustering Visualizations Hierarchical clustering is a common method used to cluster high dimensional data. In this clustering approach, data are typically grouped using a dendrogram which shows how similar groups of variables are to one another. There are various methods for hierarchical clustering of data. Here, we use the ‘hclust’ function from the base R programming. For this function to work, it requires a distance matrix as input, which summarizes how similar variables are in a dataset based on distance calculation methods. Here, we demonstrate these steps by first calculating a distance matrix from the scaled chemistry dataset using the ‘dist’ function, and then using this as input towards the hierarchical clustering function, ‘hclust’. Finally, the resulting clustering dendograms are visualized using a basic ‘plot’ function. # First calculate our distance matrix using a simple euclidance distance measure d &lt;- dist(ScaledData, method = &quot;euclidean&quot;) # Hierarchical clustering using average linkage method hc1 &lt;- hclust(d, method = &quot;average&quot; ) # Plot the obtained dendrogram plot(hc1, cex = 0.5); # cex sets text size Here, this dendogram shows how chemicals cluster together based on relative concentrations. We can see, for example, that inorganic and ionic constituents tend to group together based on relative concentrations on the left of this dendogram. Other interesting patterns are notable, including the group of polycyclic aromatic hydrocarbons (PAHs) in the middle (e.g., benzo(a)pyrene, benzo(e)pyrene, etc). Heat Map Visualizations Heat maps are a highly effective method of viewing an entire dataset (or large subset) at once. Heat maps can appear similar to correlation plots, but typically illustrate other values (e.g., concentrations, expression levels, presence/absence, etc) besides correlation R values. When generating heat maps, scaled values can be used to better distinguish patterns between groups/samples. You can additionally add dendrograms to the outsides of the heat map plot, similar to what we produced in the previous example. Here, we will be demonstrating the use of multiple heat map functions: First, the classic ‘heatmap’ function in base R can be used to create a simple heat map with dendrogram. This is useful for just a quick look at the data. Second, we will use ‘pheatmap’ from the pheatmap package. One advantage to using this package is that you can easily add colored labels to both the rows and columns of the heat map if you have numerous groups. This can help to additionally visualize differences across various groups. Third, we will use the ‘superheat’ package. This package is useful if you want to generate additional plots and figures on the side or top of the heat map, such as boxplots of groups, distributions, etc. Let’s start with the first method, using the class ‘heatmap’ function: heatmap(ScaledData) For more information on the ‘heatmap’ package, see its associated RDocumentation and helpful example tutorial website. Here is an example using the ‘pheatmap’ function from the pheatmap package: # Using pheatmap # First let&#39;s create the row side and top side color matrices # These are used to label variable categories on the heat map # Colors to be used for the row side (chemical categories) side.colors &lt;- data.frame(SmokeData1$Chemical.Category); rownames(side.colors) &lt;- SmokeData1$Chemical; colnames(side.colors) &lt;- &quot;Chemical Category&quot;; # Categories to be used for the top side (bio-conditions) top.colors &lt;- data.frame(rep(c(&quot;Smoldering&quot;, &quot;Flaming&quot;),5)); rownames(top.colors) &lt;- colnames(SmokeData2); colnames(top.colors) &lt;- &quot;Smoke Type&quot;; # Finally plot the dataset pheatmap(ScaledData, scale = &quot;none&quot;, # We already scaled our data, can use &quot;row&quot; or &quot;column&quot; also show_rownames = T, show_colnames = T, # To display rownames and column names annotation_row = side.colors, annotation_col = top.colors, # Data that contains the groupings we created color = viridis(10), # Set the color, using viridis package for color pallete cluster_rows = T, cluster_cols = T, # Cluster both rows and columns fontsize_row = 7); # Set fontsize for the rows For more information on the ‘pheatmap’ package, see its associated RDocumentation and helpful example tutorial. Lastly, here is an example using the ‘superheat’ package: # Using superheat superheat(data.frame(ScaledData), scale = F, # Set to false, if set to true, will center and scale columns pretty.order.rows = T, # Use hierarchical clustering on rows pretty.order.cols = T, # Use hierarchical clustering on columns yr = rowMeans(ScaledData), # Create row side plot of means yr.axis.name = &quot;Average Concentration\\nby Chemical&quot;, # Name of plot yr.plot.type = &quot;bar&quot;, # Type of plot yt = colMeans(ScaledData),# Create top side plot of means yt.axis.name = &quot;Average Concentration\\nby Sample&quot;, # Name of plot yt.plot.type = &quot;scatter&quot;, # Type of plot left.label.text.size = 3, # Set label size on rows left.label.col = &quot;white&quot;, # Set color of row label bottom.label.text.size = 3, # Set label size on columns bottom.label.text.angle = 90, # Set angle of bottom labels to be vertical bottom.label.size = 0, # Get rid of extra space between legend and bottom labels bottom.label.col = &quot;white&quot; # Set color of bottom label ) For more information on the ‘superheat’ package, see its associated RDocumentation and helpful example tutorial. Additional examples, packages, and approaches that can be used to generate heat maps are available in abundance through online resources, including this helpful example heat map tutorial website. Concluding Remarks In conclusion, this training module serves as a high-level introduction to high-dimensional data visualizations. Approaches described in this training module include data formatting, data scaling, and the visualization of prepared datasets through the following methods: density plots, GGally plots, boxplots, correlation plots, hierarchical clustering, and heat maps. Visualizations are generated using a dataset containing chemical concentrations measured in different biomass burn scenarios, representing different wildfire events, serving as an example dataset relevant to environmental health research. These visualization methods serve as an important foundation for many environmental health research studies. "],["fair-data-management-practices.html", "1.5 FAIR Data Management Practices Introduction to Training Module Introduction to FAIR Breaking Down FAIR, Letter-by-Letter Data Repositories for Sharing of Data Helpful Resources on FAIR Concluding Remarks", " 1.5 FAIR Data Management Practices This training module was developed by Ms. Rebecca Boyles, MSPH, with contributions from Dr. Julia E. Rager Fall 2021 Introduction to Training Module This training module provides a description of FAIR data management practices, and points participants to important resources to help ensure generated data meet current FAIR guidelines. This training module is descriptive-based (as opposed to coding-based), in order to present information clearly and serve as an important resource alongside the other scripted training activities. Training Module’s Questions This training module was specifically developed to answer the following questions: What is FAIR? When was FAIR first developed? When making data ‘Findable’, who and what should be able to find your data? When saving/formatting your data, which of the following formats is preferred to meet FAIR principles: .pdf, .csv, or a proprietary output file from your lab instrument? How can I find a suitable data repository for my data? Introduction to FAIR Proper data management is of utmost importance while leading data analyses within the field of environmental health science. A method to ensure proper data management is the implementation of Findability, Accessibility, Interoperability, and Reusability (FAIR) practices. A landmark paper that describes FAIR practices in environmental health research is the following: Wilkinson MD, Dumontier M, Aalbersberg IJ, et al. The FAIR Guiding Principles for scientific data management and stewardship. Sci Data. 2016 Mar 15. PMID: 26978244. The FAIR principles describe a framework for data management and stewardship aimed at increasing the value of data by enabling sharing and reuse. These principles were originally developed from discussions during the Jointly Designing a Data FAIRport meeting at the Lorentz Center in Leiden, The Netherlands in 2014, which brought together stakeholders to discuss the creation of an environment for virtual computational science. The resulting principles are technology agnostic, discipline independent, community driven, and internationally adopted. Below is a schematic providing an overview of this guiding principle: With this background, we can now answer Question 1: What is FAIR? Answer: A guiding framework that was recently established to promote best data management practices, to ensure that data are Findable, Accessibility, Interoperable, and Reusable. We can also answer Question 2: When was FAIR first developed? Answer: 2014- which means that these principles are very new and continuing to evolve! Breaking Down FAIR, Letter-by-Letter The aspects of the FAIR principles apply to data and metadata with the aim of making the information available to people and computers as described in the seminal paper by Wilkinson et al., 2016. F (Findable) in FAIR The F in FAIR identifies components of the principles needed to make the meta(data) findable through the application of unique persistent identifiers, thoroughly described, reference the unique identifiers, and that the descriptive information (i.e., metadata) could be searched by both humans and computer systems. F1. (Meta)data are assigned a globally unique and persistent identifier Each data set is assigned a globally unique and persistent identifier (PID), for example a DOI. These identifiers allow to find, cite and track (meta)data. A DOI looks like: https://doi.org/10.1109/5.771073 Action: Ensure that each data set is assigned a globally unique and persistent identifier. Certain repositories automatically assign identifiers to data sets as a service. If not, obtain a PID via a PID registration service. F2. Data are described with rich metadata Each data set is thoroughly (see R1) described: these metadata document how the data was generated, under what term (license) and how it can be (re)used and provide the necessary context for proper interpretation. This information needs to be machine-readable. Action: Fully document each data set in the metadata, which may include descriptive information about the context, quality and condition, or characteristics of the data. Another researcher in any field, or their computer, should be able to properly understand the nature of your dataset. Be as generous as possible with your metadata (see R1). F3. Metadata clearly and explicitly include the identifier of the data it describes Explanation: The metadata and the data set they describe are separate files. The association between a metadata file and the data set is obvious thanks to the mention of the data set’s PID in the metadata. Action: Make sure that the metadata contains the data set’s PID. F4. (Meta)data are registered or indexed in a searchable resource Explanation: Metadata are used to build easily searchable indexes of data sets. These resources will allow to search for existing data sets similarly to searching for a book in a library. Action: Provide detailed and complete metadata for each data set (see F2). With this information, can can now answer Question 3: When making data ‘Findable’, who and what should be able to find your data? Answer: Both humans and computer systems should be able to find your data. A (Accessible) in FAIR The A components are designed to enable meta(data) be available long-term, accessed by humans and machines using standard communication protocols with clearly described limitations on reuse. A1. (Meta)data are retrievable by their identifier using a standardized communications protocol Explanation: If one knows a data set’s identifier and the location where it is archived, one can access at least the metadata. Furthermore, the user knows how to proceed to get access to the data. Action: Clearly define who can access the actual data and specify how. It is possible that data will not be downloaded, but rather reused in situ. If so, the metadata must specify the conditions under which this is allowed (sometimes versus the conditions needed to fulfill for external usage/“download”). A1.1 The protocol is open, free, and universally implementable Explanation: Anyone with a computer and an internet connection can access at least the metadata. A1.2 The protocol allows for an authentication and authorization procedure, where necessary Explanation: It often makes sense to request users to create a user account on a repository. This allows to authenticate the owner (or contributor) of each data set, and to potentially set user specific rights. A2. Metadata are accessible, even when the data are no longer available Explanation: Maintaining all data sets in a readily usable state eternally would require an enormous amount of curation work (adapting to new standards for formats, converting to different format if specifically needed software is discontinued, etc). Keeping the metadata describing each data set accessible, however, can be done with much less resources. This allows to build comprehensive data indexes including all current, past, and potentially arising data sets. Action: Provide detailed and complete metadata for each data set (see R1). I (Interoperable) in FAIR The I components of the principles address needs for data exchange and interpretation by humans and machines which includes the use of controlled vocabularies or ontologies to describe meta(data) and to describe provenance relationships through appropriate data citation. I1. (Meta)data use a formal, accessible, shared, and broadly applicable language Explanation: Interoperability typically means that each computer system has at least knowledge of the other system’s formats in which data is exchanged. If (meta)data are to be searchable and if compatible data sources should be combinable in a (semi)automatic way, computer systems need to be able to decide if the content of data sets are comparable. Action: Provide machine readable data and metadata in an accessible language, using a well-established formalism. Data and metadata are annotated with resolvable vocabularies/ontologies/thesauri that are commonly used in the field (see I2). I2. (Meta)data use vocabularies that follow FAIR principles Explanation: The controlled vocabulary (e.g., MESH) used to describe data sets needs to be documented. This documentation needs to be easily findable and accessible by anyone who uses the data set. Action: The vocabularies/ontologies/thesauri are themselves findable, accessible, interoperable and thoroughly documented, hence FAIR. Lists of these standards can be found at: NCBO BioPortal, FAIRSharing, OBO Foundry. I3. (Meta)data include qualified references to other (meta)data Explanation: If the data set builds on another data set, if additional data sets are needed to complete the data, or if complementary information is stored in a different data set, this needs to be specified. In particular, the scientific link between the data sets needs to be described. Furthermore, all data sets need to be properly cited (i.e. including their persistent identifiers). Action: Properly cite relevant/associated data sets, by providing their persistent identifiers, in the metadata, and describe the scientific link/relation to your data set. R (Reusable) in FAIR The R components highlight needs for the meta(data) to be reused and support integration such as sufficient description of the data and data use limitations. R1. Meta(data) are richly described with a plurality of accurate and relevant attributes Explanation: Description of a data set is required at two different levels: Metadata describing the data set: what does the data set contain, how was the data generated, how has it been processed, how can it be reused. Metadata describing the data: any needed information to properly use the data, such as definitions of the variable names Action: Provide complete metadata for each data file. Scope of your data: for what purpose was it generated/collected? Particularities or limitations about the data that other users should be aware of. Date of the data set generation, lab conditions, who prepared the data, parameter settings, name and version of the software used. Variable names are explained or self-explanatory. Version of the archived and/or reused data is clearly specified and documented. What does this mean to you? We advise the following as ‘starting-points’ for participants to start meeting FAIR guidances: Learn how to create a Data Management Plan Keep good documentation (project &amp; data-level) while working Do not use proprietary file formats (.csv is a great go-to formats for your data!) When able, use a domain appropriate metadata standard or ontology Ruthlessly document any steps in a project Most of FAIR can be handled by selecting a good data or software repository Don’t forget to include a license! With this information, can can now answer Question 4: When saving/formatting your data, which of the following formats is preferred to meet FAIR principles: .pdf, .csv, or a proprietary output file from your lab instrument? Answer: Answer: A .csv file is preferred to enhance data sharing. Data Repositories for Sharing of Data When you are organizing your data to deposit online, it is important to identify an appropriate repository to publish your dataset it. A good starting place is a repository registry such as re3data.org or the NIH list of repositories. Many funding agencies have a list of recommended data repositories. Below are some examples of two main categories of data repositories: 1. Domain Agnostic Data Repositories Domain agnostic repositories allow the deposition of any data type. Some examples include the following: Data in Brief Articles (e.g., Elsevier’s Data in Brief Journal) Dryad Figshare The Dataverse Project Zenodo 2. Domain Specific Data Repositories Domain specific repositories allow the deposition of specific types of data, produced from specific types of technologies or within specific domains. Some examples include the following: Gene Expression Omnibus The Immunology Database and Analysis Portal Microphysiology Systems Database Mouse Genome Informatics Mouse Phenome Database OpenNeuro Protein Data Bank ProteomeXchange Rat Genome Database The Database of Genotypes and Phenotypes Zebrafish Model Organism Database and many, many others… With this information, can can now answer Question 5: How can I find a suitable data repository for my data? Answer: I can search through a data repository registry service or look for recommendations from NIH or other funding agencies. Helpful Resources on FAIR Additional Training Resources on FAIR Many organizations, from specific programs to broad organizations, provide training and resources for scientists in FAIR principles. Some of the notable global organizations organizing and providing training that offer opportunities for community involvement are: Committee on Data for Science and Technology (CODATA) Global Alliance for Genomics &amp; Health GoFAIR Force11 Research Data Alliance Additional Examples and Documents on FAIR This topic is receiving much attention in recent years, including the following workshops, government reports, and publications. Example Workshops discussing FAIR: NAS Implementing FAIR Data for People and Machines: Impacts and Implications (2019). Available at: https://www.nationalacademies.org/our-work/implementing-fair-data-for-people-and-machines-impacts-and-implications NIH Catalyzing Knowledge-driven Discovery in Environmental Health Sciences Through a Harmonized Language, Virtual Workshop (2021). Available at: https://www.niehs.nih.gov/news/events/pastmtg/2021/ehslanguage/index.cfm NIH Trustworthy Data Repositories Workshop (2019). Available at: https://datascience.nih.gov/data-ecosystem/trustworthy-data-repositories-workshop NIH Virtual Workshop on Data Metrics (2020). Available at: https://datascience.nih.gov/data-ecosystem/nih-virtual-workshop-on-data-metrics NIH Workshop on the Role of Generalist Repositories to Enhance Data Discoverability and Reuse: Workshop Summary (2020). Available at: https://datascience.nih.gov/data-ecosystem/nih-data-repository-workshop-summary Example Government Report Documents on FAIR: Collins S, Genova F, Harrower N, Hodson S, Jones S, Laaksonen L, Mietchen D, Petrauskaite R, Wittenburg P. Turning FAIR into reality: Final report and action plan from the European Commission expert group on FAIR data: European Union; 2018. Available at: https://www.vdu.lt/cris/handle/20.500.12259/103794. EU. FAIR Data Advanced Use Cases: From Principles to Practice in the Netherlands. 2018. European Union. Available at: doi:10.5281/zenodo.1250535. NIH. Final NIH Policy for Data Management and Sharing and Supplemental Information. National Institutes of Health. Federal Register, vol. 85, 2020-23674, 30 Oct. 2020, pp. 68890–900. Available at: https://www.federalregister.gov/d/2020-23674. NIH. NIH Strategic Plan for Data Science 2018. National Institutes of Health. Available at: https://datascience.nih.gov/strategicplan. NLM. NLM Strategic Plan 2017 to 2027. U.S. National Library of Medicine, Feb. 2018. Available at: https://www.nlm.nih.gov/about/strategic-plan.html. Example Related Publications on FAIR: Comess S, Akbay A, Vasiliou M, Hines RN, Joppa L, Vasiliou V, Kleinstreuer N. Bringing Big Data to Bear in Environmental Public Health: Challenges and Recommendations. Front Artif Intell. 2020 May;3:31. doi: 10.3389/frai.2020.00031. Epub 2020 May 15. PMID: 33184612; PMCID: PMC7654840. Koers H, Bangert D, Hermans E, van Horik R, de Jong M, Mokrane M. Recommendations for Services in a FAIR Data Ecosystem. Patterns (N Y). 2020 Jul 7;1(5):100058. doi: 10.1016/j.patter.2020.100058. Erratum in: Patterns (N Y). 2020 Sep 11;1(6):100104. PMID: 33205119. Kush RD, Warzel D, Kush MA, Sherman A, Navarro EA, Fitzmartin R, Pétavy F, Galvez J, Becnel LB, Zhou FL, Harmon N, Jauregui B, Jackson T, Hudson L. FAIR data sharing: The roles of common data elements and harmonization. J Biomed Inform. 2020 Jul;107:103421. doi: 10.1016/j.jbi.2020.103421. Epub 2020 May 12. PMID: 32407878. Lin D, Crabtree J, Dillo I, Downs RR, Edmunds R, Giaretta D, De Giusti M, L’Hours H, Hugo W, Jenkyns R, Khodiyar V, Martone ME, Mokrane M, Navale V, Petters J, Sierman B, Sokolova DV, Stockhause M, Westbrook J. The TRUST Principles for digital repositories. Sci Data. 2020 May 14;7(1):144. PMID: 32409645. Thessen AE, Grondin CJ, Kulkarni RD, Brander S, Truong L, Vasilevsky NA, Callahan TJ, Chan LE, Westra B, Willis M, Rothenberg SE, Jarabek AM, Burgoon L, Korrick SA, Haendel MA. Community Approaches for Integrating Environmental Exposures into Human Models of Disease. Environ Health Perspect. 2020 Dec;128(12):125002. PMID: 33369481. Roundtable on Environmental Health Sciences, Research, and Medicine; Board on Population Health and Public Health Practice; Health and Medicine Division; National Academies of Sciences, Engineering, and Medicine. Principles and Obstacles for Sharing Data from Environmental Health Research: Workshop Summary. Washington (DC): National Academies Press (US); 2016 Apr 29. PMID: 27227195. Concluding Remarks In conclusion, this training module introduces participants to best data management practices in the field of exposure science, toxicology, and environmental health research through the implementation of FAIR practices. This module first provides an introduction to FAIR, including a history of how this term was first developed and implemented. Participants are then guided through each component of FAIR, organized by letter (i.e., Findable, Accessible, Interoperable, and Reusable). The training module then reviews different types of data repositories that can be used to publish datasets in exposure science, toxicology, and environmental health research. Lastly, this module summarizes additional training resources, workshops, government reports, and example publications surrounding the use of FAIR data management practices. "],["dose-response-modeling.html", "2.1 Dose-Response Modeling Introduction to Training Module Plotting Data in Dose-Response Dose-Response Curve Fitting Comparing Curve Fits Curve Fitting Example Deriving Benchmark Dose Concluding Remarks", " 2.1 Dose-Response Modeling This training module was developed by Dr. Kyle R. Roell and Dr. Julia E. Rager Fall 2021 Introduction to Dose-Response Modeling “The Dose Makes the Poison” One of the most well-established principles in the field of toxicology is the concept, The Dose Makes the Poison. Record of this founding principle dates back to the 16th century, when the Swiss physician, Paracelsus, concluded that in sufficient quantities, everything had the potential to cause harm, and the only thing that differentiated something from being harmful or not was the dose. Findings have continued to support the principle that all substances have the potential to cause harm should the dose be sufficient, regardless of the source of an exposure/insult. This concept supports the critical need to evaluate and quantify dose-response relationships. Dose-response Modeling Dose-response modeling is a method used to quantitatively assess the relationship between an exposure to a chemical (or other stressor) and its related effects. The overall goal of these modeling efforts is to identify which exposure doses are harmful, and which aren’t, to human health. The following summarize the main steps in dose-response modeling: 1. Plot data. Data from relevant epidemiological, clinical, and/or toxicological evaluations are first plotted. Data are plotted in an X-Y plane, where the doses (or concentration) of a chemical (or stressor) are along the x-axis and the response of interests are along the y-axis. 2. Test various curve fits. There are many different models that can be used to mathematically describe the relationships between doses and associated responses. Typically, several curve fit models are tested in a dose-response analysis to see which fit the data the best. 3. Identify the best fitting model curve. Results from the tested model curve fits are evaluated, and the model(s) which fit the data the best are identified. 4. Derive values to carry forward in your analyses, based on the best fitting curve model. Using the best fitting curve model(s), final benchmark doses (BMDs), benchmark dose lower bounds (BMDLs), and other types of points of departure (PODs) are derived, which then get carried forward in the analysis (e.g., chemical risk assessment). Introduction to Training Module This training module provides an overview on analyzing exposure-associated response/outcome data in relation to exposure concentration (or dose), resulting in the derivation of benchmark doses (BMDs). This topic is of high relevance to the field of environmental health, as BMDS represent values foundational to assessing risk in chemical safety evaluations, and therefore, ultimately dictating the levels at which chemicals are regulated. This module specifically analyzes animal tumor incidence rates in response to exposure to a fictitious chemical (referred to a Chemical Z) tested across 12 different concentrations in drinking water. This dataset was generated for the specific purposes of this exercise, to allow for some interesting curve fits and a comparison between tissue site sensitivity to an example chemical insult. Training Module’s Environmental Health Questions This training module was specifically developed to answer the following environmental health questions: Which target tissue demonstrated the overall highest incidence of tumor formation from any single dose of Chemical Z? Which target tissue’s tumor incidence seems to not be related to dose? When we generate scatter plots illustrating exposure concentration vs disease outcome, without curves fitted to the data, are we able to derive benchmark doses? Upon visual inspection of example log-logistic vs. Weibull model curve fits on the intestinal tumor response data, can we confidently determine which of these two models best fits these data? For the liver tumor response data, which model fits the resulting dose-response curve the best? For the liver tumor response data, what are the final resulting BMD and BMDL estimates from the best fitting curve model? In comparing between the intestinal vs liver datasets, which tissue is estimated to show tumor responses at a lower exposure dose? Script Preparations Cleaning the global environment rm(list=ls()) Installing required R packages If you already have these packages installed, you can skip this step, or you can run the below code which checks installation status for you. To install “bmd” package, you need to first install package “remotes”. This package allows users to install packages from remote repositories, including GitHub, to then be able to install “bmd” package from Github. if (!require(&quot;Hmisc&quot;)) install.packages(&quot;Hmisc&quot;); if (!require(&quot;drc&quot;)) install.packages(&quot;drc&quot;); if (!require(&quot;remotes&quot;)) install.packages(&quot;remotes&quot;); if (!require(&quot;drc&quot;)) remotes::install_github(&quot;DoseResponse/bmd&quot;) Loading R packages required for this session # The describe function in the Hmisc package will be used to summarize a # description of the dataset library(Hmisc) # drc package will be used create and plot dose response models library(drc) #bmd pacakge will be used to caculate the benchmark dose library(bmd) For more information on the drc package, see its associated CRAN webpage and primary publication. For more information on the bmd package, see its associated R Documentation file and primary publication Set your working directory setwd(&quot;/filepath to where your input files are&quot;) Loading the Example Dataset Let’s start by loading the dataset needed for this training module. This dataset is a mock example that was generated for the purposes of this training module, in order to best capture variable types of dose-response relationships and resulting curve fits. This specific dataset analyzes the relationship between exposure to a fictional chemical, chemical Z, in drinking water and tumor incidence in the stomach, intestine (small and/or large), kidney, and liver in mice. These mice were evaluated in a traditional two-year animal bioassay setting. Note that animals are assumed to drink equivalent amounts of water each day for simplicity. Loading the data dose_response.data &lt;- read.csv(&quot;Module2_1/Module2_1_DoseResponseData.csv&quot;) Data Viewing Start with viewing the overall dimensions dim(dose_response.data) ## [1] 13 10 Then viewing a list of the column headers colnames(dose_response.data) ## [1] &quot;ChemicalZ_ugperL&quot; &quot;TotalNoAnimals_Tested&quot; ## [3] &quot;NoAnimals_StomachTumor&quot; &quot;Incidence_StomachTumor&quot; ## [5] &quot;NoAnimals_IntestinalTumor&quot; &quot;Incidence_IntestinalTumor&quot; ## [7] &quot;NoAnimals_KidneyTumor&quot; &quot;Incidence_KidneyTumor&quot; ## [9] &quot;NoAnimals_LiverTumor&quot; &quot;Incidence_LiverTumor&quot; With this, we can see that data are included for various chemical Z doses (noted in the first column), followed by a column noting the total number of animals tested per dose (in the second column). Then, columns are included describing the number of animals, followed by incidence, of tumor formation across each tissue target of interest (i.e., stomach, intestine, kidney, and liver). Viewing a quick summary of the values contained within this dataset summary(dose_response.data) ## ChemicalZ_ugperL TotalNoAnimals_Tested NoAnimals_StomachTumor ## Min. : 0 Min. :10 Min. :2.000 ## 1st Qu.: 100 1st Qu.:10 1st Qu.:2.000 ## Median : 300 Median :10 Median :2.000 ## Mean : 370 Mean :10 Mean :2.462 ## 3rd Qu.: 600 3rd Qu.:10 3rd Qu.:3.000 ## Max. :1000 Max. :10 Max. :3.000 ## Incidence_StomachTumor NoAnimals_IntestinalTumor Incidence_IntestinalTumor ## Min. :0.2000 Min. :0.000 Min. :0.0000 ## 1st Qu.:0.2000 1st Qu.:0.000 1st Qu.:0.0000 ## Median :0.2000 Median :0.000 Median :0.0000 ## Mean :0.2462 Mean :2.308 Mean :0.2308 ## 3rd Qu.:0.3000 3rd Qu.:5.000 3rd Qu.:0.5000 ## Max. :0.3000 Max. :7.000 Max. :0.7000 ## NoAnimals_KidneyTumor Incidence_KidneyTumor NoAnimals_LiverTumor ## Min. :0.000 Min. :0.0000 Min. :0.000 ## 1st Qu.:1.000 1st Qu.:0.1000 1st Qu.:0.000 ## Median :3.000 Median :0.3000 Median :2.000 ## Mean :3.692 Mean :0.3692 Mean :1.385 ## 3rd Qu.:6.000 3rd Qu.:0.6000 3rd Qu.:3.000 ## Max. :9.000 Max. :0.9000 Max. :3.000 ## Incidence_LiverTumor ## Min. :0.0000 ## 1st Qu.:0.0000 ## Median :0.2000 ## Mean :0.1385 ## 3rd Qu.:0.3000 ## Max. :0.3000 With this data summary, we can answer Environmental Health Question 1: Which target tissue demonstrated the overall highest incidence of tumor formation from any single dose of Chemical Z? Answer: The kidney indicates a maximum of 9 animals with tumors developing from a single dose, representing an alarming incidence rate of 90%. Alternatively, you can obtain a larger view of dataset using the descibe from Hmisc package Hmisc::describe(dose_response.data) ## dose_response.data ## ## 10 Variables 13 Observations ## -------------------------------------------------------------------------------- ## ChemicalZ_ugperL ## n missing distinct Info Mean Gmd .05 .10 ## 13 0 13 1 370 383.3 6 18 ## .25 .50 .75 .90 .95 ## 100 300 600 780 880 ## ## lowest : 0 10 50 100 150, highest: 500 600 700 800 1000 ## ## Value 0 10 50 100 150 200 300 400 500 600 700 ## Frequency 1 1 1 1 1 1 1 1 1 1 1 ## Proportion 0.077 0.077 0.077 0.077 0.077 0.077 0.077 0.077 0.077 0.077 0.077 ## ## Value 800 1000 ## Frequency 1 1 ## Proportion 0.077 0.077 ## -------------------------------------------------------------------------------- ## TotalNoAnimals_Tested ## n missing distinct Info Mean Gmd ## 13 0 1 0 10 0 ## ## Value 10 ## Frequency 13 ## Proportion 1 ## -------------------------------------------------------------------------------- ## NoAnimals_StomachTumor ## n missing distinct Info Mean Gmd ## 13 0 2 0.75 2.462 0.5385 ## ## Value 2 3 ## Frequency 7 6 ## Proportion 0.538 0.462 ## -------------------------------------------------------------------------------- ## Incidence_StomachTumor ## n missing distinct Info Mean Gmd ## 13 0 2 0.75 0.2462 0.05385 ## ## Value 0.2 0.3 ## Frequency 7 6 ## Proportion 0.538 0.462 ## -------------------------------------------------------------------------------- ## NoAnimals_IntestinalTumor ## n missing distinct Info Mean Gmd ## 13 0 6 0.843 2.308 3.205 ## ## lowest : 0 1 4 5 6, highest: 1 4 5 6 7 ## ## Value 0 1 4 5 6 7 ## Frequency 7 1 1 1 1 2 ## Proportion 0.538 0.077 0.077 0.077 0.077 0.154 ## -------------------------------------------------------------------------------- ## Incidence_IntestinalTumor ## n missing distinct Info Mean Gmd ## 13 0 6 0.843 0.2308 0.3205 ## ## lowest : 0.0 0.1 0.4 0.5 0.6, highest: 0.1 0.4 0.5 0.6 0.7 ## ## Value 0.0 0.1 0.4 0.5 0.6 0.7 ## Frequency 7 1 1 1 1 2 ## Proportion 0.538 0.077 0.077 0.077 0.077 0.154 ## -------------------------------------------------------------------------------- ## NoAnimals_KidneyTumor ## n missing distinct Info Mean Gmd .05 .10 ## 13 0 10 0.992 3.692 3.615 0.0 0.2 ## .25 .50 .75 .90 .95 ## 1.0 3.0 6.0 7.8 8.4 ## ## lowest : 0 1 2 3 4, highest: 5 6 7 8 9 ## ## Value 0 1 2 3 4 5 6 7 8 9 ## Frequency 2 2 2 1 1 1 1 1 1 1 ## Proportion 0.154 0.154 0.154 0.077 0.077 0.077 0.077 0.077 0.077 0.077 ## -------------------------------------------------------------------------------- ## Incidence_KidneyTumor ## n missing distinct Info Mean Gmd .05 .10 ## 13 0 10 0.992 0.3692 0.3615 0.00 0.02 ## .25 .50 .75 .90 .95 ## 0.10 0.30 0.60 0.78 0.84 ## ## lowest : 0.0 0.1 0.2 0.3 0.4, highest: 0.5 0.6 0.7 0.8 0.9 ## ## Value 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 ## Frequency 2 2 2 1 1 1 1 1 1 1 ## Proportion 0.154 0.154 0.154 0.077 0.077 0.077 0.077 0.077 0.077 0.077 ## -------------------------------------------------------------------------------- ## NoAnimals_LiverTumor ## n missing distinct Info Mean Gmd ## 13 0 3 0.865 1.385 1.538 ## ## Value 0 2 3 ## Frequency 6 3 4 ## Proportion 0.462 0.231 0.308 ## -------------------------------------------------------------------------------- ## Incidence_LiverTumor ## n missing distinct Info Mean Gmd ## 13 0 3 0.865 0.1385 0.1538 ## ## Value 0.0 0.2 0.3 ## Frequency 6 3 4 ## Proportion 0.462 0.231 0.308 ## -------------------------------------------------------------------------------- Overall, we can see that there are 4 disease outcomes included in this dataset: stomach tumors intestinal tumors kidney tumors liver tumors All with observed incidences that depend upon the exposure concentration of Chemical Z Plotting Data in Dose-Response Basic Plotting of Data in Dose-Response Let’s plot each tumor incidence against exposure concentration together in a 2x2 plot. Here, the y-axis will range from 0 to 1, with 0 indicating no incidence of tumors and 1 indicating all animals that were tested acquired tumors. Here, we will use the ‘with()’ function, which allows us to create a mini-environment using the specified data. We also use the ‘par()’ function to set graphical parameters, allowing us to create a 2x2 set of plots. par(mfrow=c(2,2)); with(dose_response.data, plot(Incidence_StomachTumor~ChemicalZ_ugperL, ylim=c(0,1))); with(dose_response.data, plot(Incidence_IntestinalTumor~ChemicalZ_ugperL, ylim=c(0,1))); with(dose_response.data, plot(Incidence_KidneyTumor~ChemicalZ_ugperL, ylim=c(0,1))); with(dose_response.data, plot(Incidence_LiverTumor~ChemicalZ_ugperL, ylim=c(0,1))); par(mfrow=c(1,1)) With these plots, we can answer Environmental Health Question 2: Which target tissue’s tumor incidence seems to not be related to dose? Answer: Stomach. We can also answer Environmental Health Question 3: When we generate scatter plots illustrating exposure concentration vs disease outcome, without curves fitted to the data, are we able to derive benchmark doses? Answer: No, a curve fit is still needed to describe the overall trend in the dataset, which can then be used in the final calculation of a benchmark dose. Dose-Response Curve Fitting It is notable that there are many different packages that can be used to fit curves to data. Here, we incorporate the drc package to fit several types of potential curve fit models to this example dataset. The drm function is specifically used from the drc package. Common parameters to consider when constructing the curve fit models in drm include the following: 1. Formula This parameter describes the formula used to fit the data, formatted similar to a standard regression formula line of code. For the purposes of the current training module, this formula will be to fit to describe tumor incidence on chemical exposure concentration, which looks like this in the final code: Incidence_StomachTumor ~ ChemicalZ_ugperL 2. Data This parameter specifies the dataset you are evaluating. For the current training module, we will be referring to the full dataframe, dose_response.data 3. Weights This parameter contributes to determining how many observations are used at each dose/concentration, which can inform the model type. For the current training module, the weights in the dataset reflect the total number of animals tested at each exposure concentration. 4. Function (fct) This parameter specifies which type of curve fit function you want to implement. Example functions include various types of log-logistic, genaralized log-logistic, weibull, asymptotic regression, and Michaelis-Menten models. Note that getMeanFunctions() can be called for the full list of available functions: getMeanFunctions() ## Log-logistic (ED50 as parameter) with lower limit at 0 and upper limit at 1 ## (2 parameters) ## In &#39;drc&#39;: LL.2 ## ## Log-logistic (ED50 as parameter) with lower limit at 0 ## (3 parameters) ## In &#39;drc&#39;: LL.3 ## ## Log-logistic (ED50 as parameter) with upper limit at 1 ## (3 parameters) ## In &#39;drc&#39;: LL.3u ## ## Log-logistic (ED50 as parameter) ## (4 parameters) ## In &#39;drc&#39;: LL.4 ## ## Generalized log-logistic (ED50 as parameter) ## (5 parameters) ## In &#39;drc&#39;: LL.5 ## ## Weibull (type 1) with lower limit at 0 and upper limit at 1 ## (2 parameters) ## In &#39;drc&#39;: W1.2 ## ## Weibull (type 1) with lower limit at 0 ## (3 parameters) ## In &#39;drc&#39;: W1.3 ## ## Weibull (type 1) ## (4 parameters) ## In &#39;drc&#39;: W1.4 ## ## Weibull (type 2) with lower limit at 0 and upper limit at 1 ## (2 parameters) ## In &#39;drc&#39;: W2.2 ## ## Weibull (type 2) with lower limit at 0 ## (3 parameters) ## In &#39;drc&#39;: W2.3 ## ## Weibull (type 2) ## (4 parameters) ## In &#39;drc&#39;: W2.4 ## ## Brain-Cousens (hormesis) with lower limit fixed at 0 ## (4 parameters) ## In &#39;drc&#39;: BC.4 ## ## Brain-Cousens (hormesis) ## (5 parameters) ## In &#39;drc&#39;: BC.5 ## ## Log-logistic (log(ED50) as parameter) with lower limit at 0 and upper limit at 1 ## (2 parameters) ## In &#39;drc&#39;: LL2.2 ## ## Log-logistic (log(ED50) as parameter) with lower limit at 0 ## (3 parameters) ## In &#39;drc&#39;: LL2.3 ## ## Log-logistic (log(ED50) as parameter) with upper limit at 1 ## (3 parameters) ## In &#39;drc&#39;: LL2.3u ## ## Log-logistic (log(ED50) as parameter) ## (4 parameters) ## In &#39;drc&#39;: LL2.4 ## ## Generalised log-logistic (log(ED50) as parameter) ## (5 parameters) ## In &#39;drc&#39;: LL2.5 ## ## Asymptotic regression with lower limit at 0 ## (2 parameters) ## In &#39;drc&#39;: AR.2 ## ## Shifted asymptotic regression ## (3 parameters) ## In &#39;drc&#39;: AR.3 ## ## Michaelis-Menten ## (2 parameters) ## In &#39;drc&#39;: MM.2 ## ## Shifted Michaelis-Menten ## (3 parameters) ## In &#39;drc&#39;: MM.3 5. Type This parameter specifies the data type of the response (e.g., binomial, continuous, etc). For the current training module, we will select the binomial type of response, which in this package refers to the modeling of data types that are not fully continuous, including this quantile-based incidence rate outcome. First try fitting a log-logistic (LL) model Because log-logistic (LL) models are commonly used to evaluate dose-response relationships, let’s first start by trying to fit a 2 parameter LL function. Running the model, on the intestinal tumor incidence outcome as an example LL2.model.int &lt;- with(dose_response.data, drm(Incidence_IntestinalTumor~ChemicalZ_ugperL, weights=TotalNoAnimals_Tested, fct=LL.2(), type=&quot;binomial&quot;)) It’s easy to plot these results using the plot function plot(LL2.model.int, type=&quot;all&quot;, ylim=c(0,1)); Let’s next try fitting a Weibull model Running the Weibull curve model, on the intestinal tumor incidence outcome as an example W23.model.int &lt;- with(dose_response.data, drm(Incidence_IntestinalTumor~ChemicalZ_ugperL, weights=TotalNoAnimals_Tested, fct=W2.3(), type=&quot;binomial&quot;)) Let’s plot the results of this function plot(W23.model.int, type=&quot;all&quot;, ylim=c(0,1)); Let’s try fitting another model fit based on asymptotic regression modeling Running the asymptotic regression model, on the intestinal tumor incidence outcome as an example AR2.model.int &lt;- with(dose_response.data, drm(Incidence_IntestinalTumor~ChemicalZ_ugperL, weights=TotalNoAnimals_Tested, fct=AR.2(), type=&quot;binomial&quot;)) Let’s plot the results of this function plot(AR2.model.int, type=&quot;all&quot;, ylim=c(0,1)); Important note on the variety of curve fit models to consider There are many different types of curve fit models to consider when running your analyses. For example, additional functions are available from other packages, such as the aomisc package, which has an associated Github page and R-bloggers article. This package contains a collection of functions that are not included in the current drc pacakage. There are many other options available as well, if you search CRAN, Bioconductor, Github, and general search engines. With this, we can now answer Environmental Health Question 4: Upon visual inspection of example log-logistic vs. Weibull model curve fits on the intestinal tumor response data, can we confidently determine which of these two models best fits these data? Answer: No, both of these models appear to fit this dataset to a large extent. A more quantitative approach based on AIC is required to identify the best fitting model (see below). Comparing Curve Fits Given the variety of models that can be used to fit dose-response data, it is important to consider the results of each model curve fit and identify which model best fits the data. There are many ways to identify best fitting curves. The most commonly implemented strategies include the following: 1. Visual Inspection. Model curve fits can be evaluated visually, to gauge whether or not resulting curves fit the data. 2. Akaike Information Criterion (AIC). AIC values are commonly used for model selection, and represent an estimator of prediction error and relative quality of statistical models for a given set of data. AIC incorporates the trade-off between a model’s goodness of fit and the simplicity, such that it weighs the risk of overfitting vs underfitting. In applications, it is common to choose models with the lowest AIC, pending they describe the data sufficiently. The AIC function can simply be used here to calculate each resulting model’s AIC. Remember, the lower AIC represents the better model curve fit. # Results from the log-logistic model AIC(LL2.model.int) ## [1] 30.85574 # Results from the Weibull model AIC(W23.model.int) ## [1] 22.87452 # Results from the asymptotic regression model AIC(AR2.model.int) ## [1] 40.37098 These results demonstrate, quantitatively, that the Weibull model likely describes this dataset the best (out of the evaluated models), since it has the lowest AIC value. Let’s finally produce a summary visualization that display the results of these three model curve fits across this intestinal dataset, with all the curve fits in one plot. # First defining a vector of text to use in the legend, summary of the three curve fits and their AICs IntestinalCurveFitAICs &lt;- c(&quot;Log-Logistic, AIC=30.9&quot;, &quot;Weibull, AIC=22.9&quot;, &quot;Asymptotic Regression, AIC=40.4&quot;) # Generating the plot plot(LL2.model.int, type=&quot;all&quot;, ylim=c(0,1)) # Can add the next models on top of current plot with different line types and weights plot(W23.model.int, add=TRUE,col=&quot;red&quot;,lty=4, lwd=1.5) plot(AR2.model.int, add=TRUE,col=&quot;blue&quot;,lty=2, lwd=1.5) # A way to coerce the dots back to black for final view: plot(LL2.model.int, add=TRUE,col=&quot;black&quot;) # Can add a legend as well, specifying the same paramters for linetype (lty) and color (col) legend(x=1, y=.8, legend=IntestinalCurveFitAICs, col=c(&quot;black&quot;, &quot;red&quot;, &quot;blue&quot;), lty=c(1,4,2)) Curve Fitting Example Curve Fitting for Liver Outcomes For this exercise now focusing on liver tumors, let’s quickly fit the same three models and plot the results. Example log-logistic (LL) model function: LL2.model.liver &lt;- with(dose_response.data, drm(Incidence_LiverTumor~ChemicalZ_ugperL, weights=TotalNoAnimals_Tested, fct=LL.2(), type=&quot;binomial&quot;)) Example Weibull model function: W23.model.liver &lt;- with(dose_response.data, drm(Incidence_LiverTumor~ChemicalZ_ugperL, weights=TotalNoAnimals_Tested, fct=W2.3(), type=&quot;binomial&quot;)) Example asymptotic regression model function: AR2.model.liver &lt;- with(dose_response.data, drm(Incidence_LiverTumor~ChemicalZ_ugperL, weights=TotalNoAnimals_Tested, fct=AR.2(), type=&quot;binomial&quot;)) Calculating AICs: AIC(LL2.model.liver) ## [1] 28.1143 AIC(W23.model.liver) ## [1] 24.46089 AIC(AR2.model.liver) ## [1] 28.80896 Plotting the resulting curve fits: # First defining a vector of text to use in the legend, summary the two curve fits and their AICs LiverCurveFitAICs &lt;- c(&quot;Log-Logistic, AIC=28.1&quot;, &quot;Weibull, AIC=24.4&quot;, &quot;Asymptotic Regression, AIC=28.8&quot;) # Generating the plot plot(LL2.model.liver, type=&quot;all&quot;, ylim=c(0,1)) plot(W23.model.liver, add=TRUE,col=&quot;red&quot;,lty=3, lwd=1.5) plot(AR2.model.liver, add=TRUE,col=&quot;blue&quot;,lty=2, lwd=1.5) # A way to coerce the dots back to black for final view: plot(LL2.model.liver, add=TRUE,col=&quot;black&quot;) # Can add a legend as well, specifying the same paramters for linetype (lty) and color (col) legend(x=1, y=.8, legend=LiverCurveFitAICs, col=c(&quot;black&quot;, &quot;red&quot;, &quot;blue&quot;), lty=c(1,4,2)) With this, we can now answer Environmental Health Question 5: For the liver tumor response data, which model fits the resulting dose-response curve the best? Answer: It is clear from visual inspection of the resulting curve fits and the calculated AIC values that the Weibull model fits the liver tumor response data the best. Deriving Benchmark Dose Deriving the Final Benchmark Dose (BMD) Estimates Using the results from our best fitting models, we can now estimate the resulting benchmark doses (BMDs) to serve as the ‘tipping points’ of where exposure concentrations are demonstrated to elicit outcomes. A BMD is technically defined as a dose or concentration that produces a predetermined change in the response rate of an adverse effect. This predetermined change in response rate is termed the benchmark response (BMR). In this training module, we implement the bmd package to calculate a BMD from the resulting models derived through the drc package (as detailed in the code above). A typical BMR used in chemical risk assessments for incidence data is 10%, so let’s use that here as our example BMR to base the derivation of BMDs off of. Running the BMD estimate off the Weibull model fit to the liver tumor response data: bmd::bmd(W23.model.liver, bmr = .1, backg = 0) ## BMD BMDL ## e:1:0.1 283.7063 191.9821 These results indicate that, in order to achieve a change in response rate of 10% (from a background of 0 ug/L), and exposure concentration of 283.7 ug/L is required. Note that a benchmark dose lower bound (BMDL) is also provided, indicating the lower bound of the confidence interval surrounding the BMD. BMDL values are also commonly carried forward in risk assessment, since these values are more conservative and thus protective of public health. With this, we can now answer Environmental Health Question 6: For the liver tumor response data, what are the final resulting BMD and BMDL estimates from the best fitting curve model? Answer: BMD=283.7 and BMDL=192.0 ug/L. Let’s compare these BMD/BMDL estimates to those generated from the best fitting curve fit on the intestinal tumor response data: bmd::bmd(W23.model.int, bmr = .1, backg = 0) ## BMD BMDL ## e:1:0.1 409.2645 345.9804 With this, we can now answer Environmental Health Question 7: In comparing between the intestinal vs liver datasets, which tissue is estimated to show tumor responses at a lower exposure dose? Answer: The liver demonstrates tumor responses at a lower exposure dose, since the intestinal BMD is 409.3 which is much higher than the liver BMD of 283.7 ug/L. Concluding Remarks In conclusion, this training module provides several examples of methods to fit model curves to dose-response data that are typically evaluated in environmental health assessments. These examples highlight the importance of evaluating model fit to ultimately determine which model should be used to derive final BMD and BMDL estimates. Through this training module, example methods and associated script are designed with flexibility to aid in future analyses in which researchers may aim to analyze in vitro bioactivity, in vivo apical outcomes, and human health outcomes in the context of dose-response. For additional case studies that leverage dose-response modeling techniques, see the following publications that also address environmental health questions: Auerbach SS, Paules RS. Genomic dose response: Successes, challenges, and next steps. Current Opinion in Toxicology. 2018 Oct; 11-12:84-92. Johnson KJ, Auerbach SS, Costa E. A Rat Liver Transcriptomic Point of Departure Predicts a Prospective Liver or Non-liver Apical Point of Departure. Toxicol Sci. 2020 Jul 1;176(1):86-102. PMID: 32384157. Rager JE, Auerbach SS, Chappell GA, Martin E, Thompson CM, Fry RC. Benchmark Dose Modeling Estimates of the Concentrations of Inorganic Arsenic That Induce Changes to the Neonatal Transcriptome, Proteome, and Epigenome in a Pregnancy Cohort. Chem Res Toxicol. 2017 Oct 16;30(10):1911-1920. PMID: 28927277. Ramaiahgari SC, Auerbach SS, Saddler TO, Rice JR, Dunlap PE, Sipes NS, DeVito MJ, Shah RR, Bushel PR, Merrick BA, Paules RS, Ferguson SS. The Power of Resolution: Contextualized Understanding of Biological Responses to Liver Injury Chemicals Using High-throughput Transcriptomics and Benchmark Concentration Modeling. Toxicol Sci. 2019 Jun 1;169(2):553-566. PMID: 30850835. Thompson CM, Kirman CR, Hays SM, Suh M, Harvey SE, Proctor DM, Rager JE, Haws LC, Harris MA. Integration of mechanistic and pharmacokinetic information to derive oral reference dose and margin-of-exposure values for hexavalent chromium. J Appl Toxicol. 2018 Mar;38(3):351-365. PMID: 29064106. For additional modeling tools and guidance documents, see the below: U.S. EPA’s BMD software (BMDS). U.S. EPA’s guidance document: U.S. EPA (2012). Benchmark Dose Technical Guidance. Risk Assessment Forum. Document ID EPA/100/R-12/001. Washington, DC. Netherlands’ National Institute for Public Health and the Environment (RIVM) software, PROAST. U.S. NTP’s Bayesian BMD Estimation tool that’s available as an online tool with the accompanying publication. U.S. NTP’s BMDExpress tool for performing high-throughput dose-response assessments on gene expression datasets, which is available as a tool with the accompanying publication. National Institutes of Environmental Health Sciences (NIEHS)’s new dose-response modeling tool, ToxicR, which was just released the Spring of 2022. This tool is a R package developed by Dr. Matt Wheeler that was built around the core functions also available in the US EPA BMDS software listed above. This tool incorporates newer advances in BMD modeling including Bayesian estimates and model averaging. "],["machine-learning-and-predictive-modeling.html", "2.2 Machine Learning and Predictive Modeling Introduction to Training Module K-means Analysis Principal Component Analysis Combining K-Means with PCA Concluding Remarks", " 2.2 Machine Learning and Predictive Modeling The development of this training module was led by Dr. David M. Reif Fall 2021 Introduction to Machine Learning (ML) and Predictive Modeling The need for predictive modeling We can screen for biological responses to a variety of chemical exposures/treatment conditions very efficiently, leveraging technologies like cell-based high-throughput screening These screening efforts result in increasing amounts of data, which can be gathered to start building big databases Alongside these big databases, the associated dimensionality of these data gets “Big” And diversity across types of screening platforms, technologies, cell types, species, etc, leading to compounding dimensionality How do we even begin to analyze such data? For diverse, high-dimensional data, new approaches are needed. Traditional statistics may be able to handle 1:1 or 1:many comparisons of singular quantities (e.g. activity concentrations (e.g., AC50s for two chemicals). However, once the modeling needs become overly complex (or exploratory), assumptions of most traditional methods will be violated. Defining predictive modeling in the context of toxicology and environmental health We often think of predictions as having a forward-time component (i.e. What will happen next?). What about “prediction” in a different sense as applied to toxicology? Working definition: Predictive toxicology describes a multidisciplinary approach to chemical toxicity evaluation that more efficiently uses animal test results, when needed, and leverages expanding non-animal test methods to forecast the effects of a chemical on biological systems eg 1. Can I more efficiently design animal studies and analyze data from shorter assays using less animals to predict long-term health outcomes? eg 2. Can this suite of in vitro assays predict what would happen in an organism? eg 3. Can I use diverse, high-dimensional data to cluster chemicals into predicted activity classes? Similar logic applies to the field of exposure science. What about “prediction” applied to exposure science? Working definition: Predictive exposure science describes a multidisciplinary approach to chemical exposure evaluations that more efficiently uses biomonitoring, chemical inventory, and other exposure science-relevant databases to forecast exposure rates in target populations. eg 1. Can I use existing biomonitoring data from NHANES to predict exposure rates for chemicals that have yet to be measured in target populations? (see ExpoCast program, eg. Wambaugh et al.) eg 2. Can I use chemical product use inventory data to predict the likelihood of a chemical being present in a certain consumer product (eg. Phillips et al.) Distinguish between machine learning (ML) and traditional statistical methods There is plenty of debate as to where the line(s) between ML and traditional statistics should be drawn. A perfect delineation is not necessary for our purposes. Rather, we will focus on the usual goals/intent of each to help us understand the distinction for Environmental Health Research. Working distinction: Statistics draws population inferences from a sample, and machine learning finds generalizable predictive patterns. [https://www.nature.com/articles/nmeth.4642] Thus, by our working definition of predictive toxicology, we are interested in predictive aspects of ML that can give us generalizable forecasts as to effects of chemicals on biological systems. The image below shows graphical abstractions of how a “problem” is solved using either traditional statistics in the top row of (A) logistic and (B) linear regression or ML in the bottom row of (C) support vector machines, (D) artificial neural networks, and (E) decision trees. [https://www.sciencedirect.com/science/article/pii/S2590139719300432?via%3Dihub] The list of ML methods is continually expanding and subject to new taxonomic description. There are many different types of ML methods that we can employ, depending on the data type / purpose of data analysis. Generally speaking, ML is considered to encompass the study of computer algorithms that can improve automatically through experience and by the use of data. It is seen as a part of artificial intelligence (AI). A recent review written together with Erin Baker’s lab provides a high-level overview on some of the types of ML methods and challenges to address when analyzing multi-omic data (including chemical signature data). K-Means Clustering A common type of ML method that will be included in the scripted activity is called k-means clustering. K-means is a common clustering algorithm used to partition quantitative data. This algorithm works by first, randomly selecting a pre-specified number of clusters, k, across the data space, with each cluster having a data centroid. When using a standard Euclidean distance metric, the distance is calculated from an observation to each centroid, then the observation is assigned to the cluster of the closest centroid. After all observations have been assigned to one of the k clusters, the average of all observations in a cluster is calculated, and the centroid for the cluster is moved to the location of the mean. The process then repeats, with the distance computed between the observations and the updated centroids. Observations may be reassigned to the same cluster, or moved to a different cluster if it is closer to another centroid. These iterations continue until there are no longer changes between cluster assignments for observations, resulting in the final cluster assignments that are then carried forward for analysis/interpretation. Helpful resources on k-means clustering include the following: The Elements of Statistical Learning &amp; Towards Data Science Principal Component Analysis (PCA) Another very common ML method you can use to look at big data is a method to reduce high-dimensional data called Principal Component Analysis (PCA). This can be defined in many ways, though here are some of the important elements that underly a PCA: PCA partitions variance in a dataset into linearly uncorrelated principal components (PCs), which are weighted combinations of the original features. Each PC (starting from PC1) summarizes a decreasing % of variance. Every instance (e.g. chemical) in the original dataset has a “score” on each PC. Any combination of PCs can be compared to summarize relationships amongst the instances (e.g. chemicals). Introduction to Training Module In this activity we are going to analyze an example dataset of physicochemical property information for chemicals spanning per- and polyfluoroalkyl substances (PFAS) and statins. PFAS represent a ubiquitous and pervasive class of man-made industrial chemicals that are commonly used in food packaging, commercial household products such as Teflon, cleaning products, and flame retardants. PFAS are recognized as highly stable compounds that, upon entering the environment, can persist for many years and act as harmful sources of exposure. Statins represent a class of lipid-lowering compounds that are commonly used as pharmaceutical treatments for patients at risk of cardiovascular disease. Because of their common use amongst patients, statins can also end up in water and wastewater effluent, making them of environmental relevance as well. This training module was designed to evaluate the chemical space of these diverse compounds, and to illustrate the utility of machine learning methods to differentiate chemical class and predict chemical groupings that can inform a variety of environmental and toxicological applications. The two types of machine learning methods that will be employed are k-means and PCA (as described in the introduction). Training Module’s Environmental Health Questions This training module was specifically developed to answer the following environmental health questions: Can we differentiate between PFAS and statin chemical classes, when considering just the raw physicochemical property variables without applying machine learning techniques? What are some of the physicochemical properties that seem to be driving chemical clustering patterns derived through k-means? Upon reducing the data through PCA, which physicochemical property contributes the most towards informing data variance captured in the primary principal component (Comp.1)? How do the data compare when physicochemical properties are reduced using PCA? If we did not have information telling us which chemical belonged to which class, could we use PCA and k-means to accurately predict whether a chemical is a PFAS vs statin? What kinds of applications/endpoints can be better understood and/or predicted, because of these derived chemical groupings? Script Preparations Cleaning the global environment rm(list=ls()) Installing required R packages If you already have these packages installed, you can skip this step, or you can run the below code which checks installation status for you if (!requireNamespace(&quot;ggplot2&quot;)) install.packages(&quot;ggplot2&quot;, repos = &quot;https://cloud.r-project.org&quot;); if (!requireNamespace(&quot;pheatmap&quot;)) install.packages(&quot;pheatmap&quot;, repos = &quot;https://cloud.r-project.org&quot;); Loading R packages required for this session library(ggplot2) # Used to make heat maps. This can be done in ggplot2 but pheatmap is easier and nicer library(pheatmap) Getting help with packages and functions # Package documentation for ggplot2 ?ggplot2 # Package documentation for kmeans (a part of the standard stats R package, # automatically uploaded) ?kmeans # Package documentation for deriving principal components within a PCA # (a part of the standard stats R package, automatically uploaded) ?princomp # Package documentation for pheatmap ?pheatmap Set your working directory setwd(&quot;/filepath to where your input files are&quot;) # e.g. setwd(&quot;/Downloads&quot;) Loading the Example Dataset Let’s start by loading the datasets needed for this training module. We are going to use a dataset of substances that have a diverse chemical space of PFAS and statin compounds. This list of chemicals will be uploaded alongside physicochemical property data. The chemical lists for ‘PFAS’ and ‘Statins’ were obtained from the EPA’s Computational Toxicology Dashboard Chemical Lists. The physicochemical properties were obtained by uploading these lists into the National Toxoicology Program’s Integrated Chemical Environment (ICE). dat &lt;- read.csv(&quot;Module2_2/Module2_2_Chemical_Lists_PFAS-Statins.csv&quot;, fileEncoding=&quot;UTF-8-BOM&quot;) Data Viewing Let’s first view the substances dataset, starting with the overall dimensions: dim(dat) ## [1] 144 14 Then looking at the first four rows and five columns of data: dat[1:4,1:5] ## List Substance.Name CASRN ## 1 PFAS Perfluoro-2-(trifluoromethyl)propanesulphonic acid 93762-09-5 ## 2 PFAS Potassium perfluoroheptanesulfonate 60270-55-5 ## 3 PFAS Bis(2-hydroxyethyl)ammonium perfluoroheptanesulfonate 70225-15-9 ## 4 PFAS Potassium perfluoro-p-ethylcyclohexanesulfonate 335-24-0 ## DTXSID Molecular.Weight ## 1 DTXSID90239569 300.100 ## 2 DTXSID9069392 488.212 ## 3 DTXSID60880946 555.258 ## 4 DTXSID50880117 500.223 Note that the first column, ‘List’, designates the following two larger chemical classes: unique(dat$List) ## [1] &quot;PFAS&quot; &quot;Statins&quot; Let’s lastly view all of the column headers: colnames(dat) ## [1] &quot;List&quot; ## [2] &quot;Substance.Name&quot; ## [3] &quot;CASRN&quot; ## [4] &quot;DTXSID&quot; ## [5] &quot;Molecular.Weight&quot; ## [6] &quot;OPERA..Boiling.Point&quot; ## [7] &quot;OPERA..Henry.s.Law.Constant&quot; ## [8] &quot;OPERA..Melting.Point&quot; ## [9] &quot;OPERA..Negative.Log.of.Acid.Dissociation.Constant&quot; ## [10] &quot;OPERA..Octanol.Air.Partition.Coefficient&quot; ## [11] &quot;OPERA..Octanol.Water.Distribution.Coefficient&quot; ## [12] &quot;OPERA..Octanol.Water.Partition.Coefficient&quot; ## [13] &quot;OPERA..Vapor.Pressure&quot; ## [14] &quot;OPERA..Water.Solubility&quot; In the data file, the first four columns represent chemical identifier information. All remaining columns represent different physicochemical properties derived from OPERA via Integrated Chemical Environment (ICE). Because the original titles of these physicochemical properties contained commas and spaces, R automatically coverted these into periods. Hence, titles like “OPERA..Boiling.Point” For ease of downstream data analyses, let’s create a more focused dataframe option containing only one chemical identifier (CASRN) as row names, and then just the physicochemical property columns. dat.x &lt;- dat[,5:ncol(dat)] rownames(dat.x) &lt;- dat$CASRN Now we can explore this data subset. # Overall dimensions dim(dat.x) ## [1] 144 10 # Viewing the first four rows and five columns dat.x[1:4,1:5] ## Molecular.Weight OPERA..Boiling.Point OPERA..Henry.s.Law.Constant ## 93762-09-5 300.100 213.095 -3.60 ## 60270-55-5 488.212 223.097 -9.75 ## 70225-15-9 555.258 223.097 -9.75 ## 335-24-0 500.223 220.578 -7.56 ## OPERA..Melting.Point ## 93762-09-5 96.455 ## 60270-55-5 273.228 ## 70225-15-9 182.152 ## 335-24-0 231.827 ## OPERA..Negative.Log.of.Acid.Dissociation.Constant ## 93762-09-5 0.175 ## 60270-55-5 -1.810 ## 70225-15-9 1.000 ## 335-24-0 1.000 colnames(dat.x) ## [1] &quot;Molecular.Weight&quot; ## [2] &quot;OPERA..Boiling.Point&quot; ## [3] &quot;OPERA..Henry.s.Law.Constant&quot; ## [4] &quot;OPERA..Melting.Point&quot; ## [5] &quot;OPERA..Negative.Log.of.Acid.Dissociation.Constant&quot; ## [6] &quot;OPERA..Octanol.Air.Partition.Coefficient&quot; ## [7] &quot;OPERA..Octanol.Water.Distribution.Coefficient&quot; ## [8] &quot;OPERA..Octanol.Water.Partition.Coefficient&quot; ## [9] &quot;OPERA..Vapor.Pressure&quot; ## [10] &quot;OPERA..Water.Solubility&quot; Let’s first see how these chemicals group when using the ‘real’ physicochemical property data, without any fancy data reduction or other machine learning techniques. We can plot chemicals along the first two ‘real’ properties, with molecular weight as one axis and boiling point as the other. Here we can create a plot using basic ggplot functions, coloring by the chemical classes from the ‘List’ column of the original dataframe. ggplot(as.data.frame(dat.x[,1:2]), aes(x=Molecular.Weight, y=OPERA..Boiling.Point, color=as.factor(dat$List))) + geom_point(size=4) + theme_bw() + ggtitle(&#39;Version A: Bivariate Plot of Two Original Physchem Variables&#39;) + xlab(&quot;Molecular Weight&quot;) + ylab(&quot;Boiling Point&quot;) Now we can plot chemicals along the next two sets of ‘real’ property data, with Henry’s Law constant as one axis and melting point as the other. Here we can create a plot using basic ggplot functions, coloring by the chemical classes from the ‘List’ column of the original dataframe. ggplot(as.data.frame(dat.x[,3:4]), aes(x=OPERA..Henry.s.Law.Constant, y=OPERA..Melting.Point, color=as.factor(dat$List))) + geom_point(size=4) + theme_bw() + ggtitle(&#39;Version B: Bivariate Plot of Two Other Original Physchem Variables&#39;) + xlab(&quot;OPERA..Henry.s.Law.Constant&quot;) + ylab(&quot;OPERA..Melting.Point&quot;) These plots provide two examples illustrating part of the distribution of physicochemical property data across the two classes of chemicals, spanning PFAS and statins. With these, we can answer Environmental Health Question 1: Can we differentiate between PFAS and statin chemical classes, when considering just the raw physicochemical property variables without applying machine learning techniques? Answer: Only in part. From the first plot, we can see that PFAS tend to have lower molecular weight ranges in comparison to the statins, though other property variables clearly overlap in ranges of values, making the groupings not entirely clear. With this data summary, we can answer Environmental Health Question 1: Which target tissue demonstrated the overall highest incidence of tumor formation from any single dose of Chemical Z? Answer: The kidney indicates a maximum of 9 animals with tumors developing from a single dose, representing an alarming incidence rate of 90%. K-means Analysis Identifying Clusters of Chemicals through k-means Let’s derive clusters of chemicals, based on ALL underlying physicochemical property data, using k-means clustering. For this example, let’s coerce the k-means algorithms to calculate n=2 distinct clusters (based on their corresponding mean centered values). Here we choose to derive two distinct clusters, because we are ultimately going to see if we can use this information to predict each chemical’s classification into two distinct chemical classes (i.e., PFAS vs statins). Note that we can derive more clusters using similar code, depending on the question being addressed. We can give a name to this variable, to easily provide the number of clusters in the next lines of code, ‘num.centers’: num.centers &lt;- 2 Estimate k-means Clusters Here we derive chemical clusters using k-means: clusters &lt;- kmeans(dat.x, # input dataframe centers = num.centers, # number of cluster centers to calculate iter.max = 1000, # the maximum number of iterations allowed nstart = 50) # the number of rows used as the random # set for the initial centers # (during the first iteration) The resulting property values that were derived as the final cluster centers can be pulled using: clusters$centers ## Molecular.Weight OPERA..Boiling.Point OPERA..Henry.s.Law.Constant ## 1 395.0716 281.4445 -8.655185 ## 2 690.1443 233.0402 -9.589444 ## OPERA..Melting.Point OPERA..Negative.Log.of.Acid.Dissociation.Constant ## 1 157.5036 1.33226852 ## 2 183.7980 0.01658333 ## OPERA..Octanol.Air.Partition.Coefficient ## 1 6.629556 ## 2 5.940861 ## OPERA..Octanol.Water.Distribution.Coefficient ## 1 -1.271315 ## 2 -2.541750 ## OPERA..Octanol.Water.Partition.Coefficient OPERA..Vapor.Pressure ## 1 3.010302 -6.762009 ## 2 4.000639 -5.538889 ## OPERA..Water.Solubility ## 1 -3.450750 ## 2 -3.760222 Visualize k-means Clusters Let’s add the cluster assignments to the physicochemical data and create a new dataframe, which can then be used in a heat map visualization to see how these physicochemical data distributions clustered according to k-means. These cluster assignments can be pulled from the ‘cluster’ list output, selecting the ‘cluster’ list, where chemicals are designated to each cluster with either a 1 or 2. You can view these using: clusters$cluster ## 93762-09-5 60270-55-5 70225-15-9 335-24-0 647-29-0 68259-12-1 ## 1 1 2 1 1 2 ## 68259-09-6 68259-07-4 60453-92-1 357-31-3 441296-91-9 749786-16-1 ## 1 1 1 1 2 2 ## 93762-10-8 135524-36-6 93894-55-4 34642-43-8 2706-91-4 791563-89-8 ## 1 1 2 1 1 2 ## 742-73-4 29420-49-3 3871-99-6 29359-39-5 3872-25-1 126105-34-8 ## 1 1 1 2 1 2 ## 630402-22-1 2274731-07-4 98789-57-2 85963-79-7 375-73-5 108427-53-8 ## 1 2 2 1 1 1 ## 4021-47-0 117806-54-9 67906-42-7 68555-66-8 92982-03-1 375-92-8 ## 1 1 2 1 2 1 ## 175905-36-9 102061-82-5 134615-58-0 174675-49-1 79780-39-5 91036-71-4 ## 1 1 2 2 2 2 ## 70225-17-1 6401-03-2 374-58-3 646-83-3 86525-30-6 3916-24-3 ## 1 1 1 1 2 1 ## 42409-05-2 474511-07-4 2795-39-3 45187-15-3 82382-12-5 79963-95-4 ## 1 2 2 1 1 1 ## 45298-90-6 134615-57-9 927670-12-0 2806-15-7 70225-14-8 131651-65-5 ## 1 1 1 2 2 1 ## 343629-46-9 144797-51-3 29081-56-9 80988-54-1 1379460-39-5 343629-43-6 ## 2 1 1 1 2 2 ## 146689-46-5 29457-72-5 355-46-4 3107-18-4 70259-86-8 1036375-28-6 ## 1 1 1 1 1 1 ## 70225-18-2 70225-16-0 84224-48-6 507453-86-3 40365-28-4 110676-15-8 ## 1 1 1 2 2 1 ## 70259-85-7 2106-55-0 1997344-07-6 423-41-6 115416-68-7 17202-41-4 ## 1 1 1 1 1 2 ## 93894-73-6 134615-56-8 134615-59-1 68259-08-5 68259-10-9 374-62-9 ## 2 1 2 1 1 1 ## 68555-67-9 2806-16-8 36913-91-4 85187-17-3 803688-15-5 55120-77-9 ## 1 2 2 2 1 1 ## 335-77-3 141263-54-9 95465-60-4 130200-44-1 144535-22-8 130468-11-0 ## 2 1 1 1 1 1 ## 93957-54-1 126059-69-6 153463-20-8 154417-69-3 147511-69-1 141263-69-6 ## 1 1 1 1 1 1 ## 77517-29-4 80799-31-1 73390-02-0 503-49-1 117678-63-4 145599-86-6 ## 1 1 1 1 1 1 ## 147098-20-2 85798-96-5 120551-59-9 13552-81-3 90761-31-2 79691-18-2 ## 2 1 1 2 1 1 ## 73573-88-3 114801-27-3 151106-12-6 129443-92-1 134523-03-8 122254-45-9 ## 1 1 1 1 2 1 ## 75330-75-5 137023-81-5 136320-61-1 87770-13-6 85551-06-0 144501-27-9 ## 1 1 1 1 1 1 ## 159014-70-7 153321-50-7 133983-25-2 78366-44-6 148750-02-1 79902-63-9 ## 1 1 1 1 1 1 ## 120185-34-4 120171-12-2 141267-47-2 94061-80-0 141240-46-2 81093-37-0 ## 1 1 1 1 1 1 Because these results are listed in the exact same order as the inputted dataframe, we can simply bind these assignments to the dat.x dataframe using cbind(). dat_wclusters &lt;- as.data.frame(cbind(dat.x,clusters$cluster)) # Renaming this new column &quot;kmeans_cluster&quot; colnames(dat_wclusters)[11] &lt;- &quot;kmeans_cluster&quot; # Sorting data by cluster assignments dat_wclusters &lt;- dat_wclusters[order(dat_wclusters$kmeans_cluster),] Heat Map Visualizations To generate a heat map, we need to first create a separate dataframe for the cluster assignments, ordered in the same way as the physicochemical data: # Creating the dataframe hm_cluster &lt;- data.frame(dat_wclusters$kmeans_cluster, row.names=row.names(dat_wclusters)) # Reassigning the column name colnames(hm_cluster) &lt;- &quot;kmeans_cluster&quot; # Coercing the cluster numbers into factor variables, to make the heat map prettier hm_cluster$kmeans_cluster &lt;- as.factor(hm_cluster$kmeans_cluster) # Viewing this new cluster assignment dataframe head(hm_cluster) ## kmeans_cluster ## 93762-09-5 1 ## 60270-55-5 1 ## 335-24-0 1 ## 647-29-0 1 ## 68259-09-6 1 ## 68259-07-4 1 Then we can call this dataframe, as well as the main physicochemical property dataframe (both sorted by clusters) into the following heat map visualization code, leveraging the pheatmap function. chem_hm &lt;- pheatmap(dat_wclusters[,1:10], main=&quot;Heat Map of Physicochemical Properties with k-means Cluster Assignments&quot;, cluster_rows=FALSE, cluster_cols = FALSE, # no further clustering, for simplicity scale=&quot;column&quot;, # scaling the data to make differences across chemicals more apparent annotation_row = hm_cluster, # calling the cluster assignment dataframe as a separate color bar angle_col = 45, fontsize_col = 7, fontsize_row = 3, # adjusting size and orientation of labels on axes cellheight = 3, cellwidth = 25, # setting height and width for cells border_color = FALSE # specify no border surrounding the cells ) Shown here is a heat map displaying the relative values for each physicochemical property, with all 10 properties listed along the bottom. Individual chemicals are listed along the right hand side. The k-means cluster assignment is provided as a separate color bar on the left. With this, we can answer Environmental Health Question 2: What are some of the physicochemical properties that seem to be driving chemical clustering patterns derived through k-means? Answer: Properties with values that show obvious differences between resulting clusters including molecular weight, boiling point, negative log of acid dissociation constant, octanol air partition coefficient, and octanol water distribution coefficient. Principal Component Analysis Next, we will run through some example analyses applying the common data reduction technique of PCA. We can calculate the principal components across ALL physicochemical data across all chemicals using the princomp function. my.pca &lt;- princomp(dat.x, # input dataframe of physchem data cor = T) # calculations will be based on the correlation matrix # (as opposed to covariance) since we have all numeric # values here (default PCA option) Here are the resulting scores for each chemical’s contribution towards each principal component (shown here as components 1-10). head(my.pca$scores) ## Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 ## 93762-09-5 -2.0425355 -1.4875982 -1.29779776 -0.04882877 0.25393797 -0.6799177 ## 60270-55-5 -1.2291769 2.2936873 0.24345932 0.40280922 0.63000240 -1.0186985 ## 70225-15-9 -1.0982561 1.3963638 0.03352018 0.90707254 0.05756006 0.1438501 ## 335-24-0 -1.1374460 1.0712815 -0.14349891 1.09092722 0.21246867 -0.9427527 ## 647-29-0 -0.4847481 0.1264224 1.16553341 -1.11771990 -0.29674860 0.1924128 ## 68259-12-1 -0.3276157 0.2377300 1.32445577 -0.47677888 -1.17966092 0.0593078 ## Comp.7 Comp.8 Comp.9 Comp.10 ## 93762-09-5 0.14597268 1.25959099 0.231742917 -0.14124625 ## 60270-55-5 0.11356003 -0.34454904 -0.385021331 -0.09883538 ## 70225-15-9 -0.38489641 0.01723486 -0.006725509 0.02725202 ## 335-24-0 0.22957369 0.11497271 -0.108096107 -0.17762819 ## 647-29-0 0.18292023 -0.48181130 0.075229509 -0.22829905 ## 68259-12-1 -0.01404007 0.03803686 0.043460416 0.18095023 And the resulting loading factors of each property’s contribution towards each principal component. head(my.pca$loadings) ## Comp.1 Comp.2 ## Molecular.Weight 0.09825313 0.108454961 ## OPERA..Boiling.Point 0.46350428 0.029650863 ## OPERA..Henry.s.Law.Constant -0.17856542 -0.502116638 ## OPERA..Melting.Point 0.20645719 0.474473487 ## OPERA..Negative.Log.of.Acid.Dissociation.Constant 0.32172963 -0.119465105 ## OPERA..Octanol.Air.Partition.Coefficient 0.45329804 -0.008918089 ## Comp.3 Comp.4 ## Molecular.Weight 0.6797404 0.48432419 ## OPERA..Boiling.Point -0.1993659 -0.03108544 ## OPERA..Henry.s.Law.Constant -0.1798767 0.27695374 ## OPERA..Melting.Point 0.2148579 0.09449999 ## OPERA..Negative.Log.of.Acid.Dissociation.Constant -0.2862395 0.58268278 ## OPERA..Octanol.Air.Partition.Coefficient -0.1321577 -0.04820475 ## Comp.5 Comp.6 ## Molecular.Weight 0.17351578 0.35736795 ## OPERA..Boiling.Point 0.22224554 -0.01850753 ## OPERA..Henry.s.Law.Constant 0.30566003 -0.47066669 ## OPERA..Melting.Point -0.08063905 -0.68672356 ## OPERA..Negative.Log.of.Acid.Dissociation.Constant -0.66998767 0.02924804 ## OPERA..Octanol.Air.Partition.Coefficient 0.20778895 0.20575789 ## Comp.7 Comp.8 ## Molecular.Weight 0.11763362 0.32938640 ## OPERA..Boiling.Point 0.12503355 0.09718690 ## OPERA..Henry.s.Law.Constant 0.21138163 0.44526650 ## OPERA..Melting.Point 0.34342931 -0.10233816 ## OPERA..Negative.Log.of.Acid.Dissociation.Constant -0.09083446 0.03113686 ## OPERA..Octanol.Air.Partition.Coefficient 0.44434707 -0.29734602 ## Comp.9 Comp.10 ## Molecular.Weight 0.03295675 0.02698233 ## OPERA..Boiling.Point 0.03336277 0.81709497 ## OPERA..Henry.s.Law.Constant 0.19706729 -0.10099077 ## OPERA..Melting.Point -0.24532148 -0.10229774 ## OPERA..Negative.Log.of.Acid.Dissociation.Constant 0.02576652 -0.03380215 ## OPERA..Octanol.Air.Partition.Coefficient 0.49672303 -0.39565984 my.pca$loadings ## ## Loadings: ## Comp.1 Comp.2 Comp.3 Comp.4 ## Molecular.Weight 0.108 0.680 0.484 ## OPERA..Boiling.Point 0.464 -0.199 ## OPERA..Henry.s.Law.Constant -0.179 -0.502 -0.180 0.277 ## OPERA..Melting.Point 0.206 0.474 0.215 ## OPERA..Negative.Log.of.Acid.Dissociation.Constant 0.322 -0.119 -0.286 0.583 ## OPERA..Octanol.Air.Partition.Coefficient 0.453 -0.132 ## OPERA..Octanol.Water.Distribution.Coefficient 0.330 -0.437 -0.151 ## OPERA..Octanol.Water.Partition.Coefficient 0.162 -0.343 0.467 -0.485 ## OPERA..Vapor.Pressure -0.352 -0.350 0.195 0.250 ## OPERA..Water.Solubility -0.365 0.255 -0.254 -0.130 ## Comp.5 Comp.6 Comp.7 Comp.8 ## Molecular.Weight 0.174 0.357 0.118 0.329 ## OPERA..Boiling.Point 0.222 0.125 ## OPERA..Henry.s.Law.Constant 0.306 -0.471 0.211 0.445 ## OPERA..Melting.Point -0.687 0.343 -0.102 ## OPERA..Negative.Log.of.Acid.Dissociation.Constant -0.670 ## OPERA..Octanol.Air.Partition.Coefficient 0.208 0.206 0.444 -0.297 ## OPERA..Octanol.Water.Distribution.Coefficient 0.169 0.220 ## OPERA..Octanol.Water.Partition.Coefficient -0.485 -0.162 0.178 ## OPERA..Vapor.Pressure 0.350 -0.654 ## OPERA..Water.Solubility -0.291 0.284 0.652 0.350 ## Comp.9 Comp.10 ## Molecular.Weight ## OPERA..Boiling.Point 0.817 ## OPERA..Henry.s.Law.Constant 0.197 -0.101 ## OPERA..Melting.Point -0.245 -0.102 ## OPERA..Negative.Log.of.Acid.Dissociation.Constant ## OPERA..Octanol.Air.Partition.Coefficient 0.497 -0.396 ## OPERA..Octanol.Water.Distribution.Coefficient -0.744 -0.199 ## OPERA..Octanol.Water.Partition.Coefficient 0.306 ## OPERA..Vapor.Pressure 0.313 ## OPERA..Water.Solubility ## ## Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7 Comp.8 Comp.9 ## SS loadings 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 ## Proportion Var 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 ## Cumulative Var 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 ## Comp.10 ## SS loadings 1.0 ## Proportion Var 0.1 ## Cumulative Var 1.0 With these results, we can answer Environmental Health Question 3: Upon reducing the data through PCA, which physicochemical property contributes the most towards informing data variance captured in the primary principal component (Comp.1)? Answer: Boiling point contributes the most towards principal component #1. Variance Captured by each Principal Component We can view summary statistics describing how much of the variance from the original dataset was captured by each component, using the summary function. summary(my.pca) ## Importance of components: ## Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 ## Standard deviation 2.0230157 1.5440214 1.2415840 0.76635873 0.6905932 ## Proportion of Variance 0.4092593 0.2384002 0.1541531 0.05873057 0.0476919 ## Cumulative Proportion 0.4092593 0.6476595 0.8018125 0.86054312 0.9082350 ## Comp.6 Comp.7 Comp.8 Comp.9 Comp.10 ## Standard deviation 0.60491164 0.48939394 0.40589919 0.32548349 0.203793303 ## Proportion of Variance 0.03659181 0.02395064 0.01647542 0.01059395 0.004153171 ## Cumulative Proportion 0.94482682 0.96877746 0.98525288 0.99584683 1.000000000 We can also calculate these values, and pull them into a dataframe for future use. For example, to pull the percentage of variance explained by each principal component, we can run the following calculations, where first eigenvalues (eigs) are calculated and then used to calculate percent of variance, per principal component: eigs &lt;- my.pca$sdev^2 Comp.stats &lt;- data.frame(eigs, eigs/sum(eigs), row.names=names(eigs)) colnames(Comp.stats) &lt;- c(&quot;Eigen_Values&quot;, &quot;Percent_of_Variance&quot;) head(Comp.stats) ## Eigen_Values Percent_of_Variance ## Comp.1 4.0925925 0.40925925 ## Comp.2 2.3840022 0.23840022 ## Comp.3 1.5415308 0.15415308 ## Comp.4 0.5873057 0.05873057 ## Comp.5 0.4769190 0.04769190 ## Comp.6 0.3659181 0.03659181 Here, we can see that Principal Component #1 (Comp.1) captures ~41% of the variance across all physicochemical property values, across all chemicals. Principal Component #2 captures ~24%, etc. Visualizing PCA Results Let’s now view the results of this PCA, focusing on the first two principal components, and coloring each chemical according to class (i.e. PFAS vs statins). ggplot(as.data.frame(my.pca$scores), aes(x=Comp.1, y=Comp.2, color=as.factor(dat$List))) + geom_point(size=4) + theme_bw() + ggtitle(&#39;Version C: PCA Plot of the First 2 PCs, colored by Chemical Class&#39;) + xlab(&quot;Principal Component 1&quot;) + ylab(&quot;Principal Component 2&quot;) With this, we can answer Environmental Health Question 4: How do the data compare when physicochemical properties are reduced using PCA? Answer: Data become more compressed, and variables reduce across principal components capturing the majority of variance. This results in improved data visualizations, where all dimensions of the physiochemical dataset are compressed and captured across the displayed components. Combining K-Means with PCA Incorporating K-Means into PCA for Predictive Modeling We can also identify cluster-based trends within data that are reduced, after running PCA. This example analysis does so, expanding upon the previously generated PCA results. Estimate k-means clusters from PCA results Let’s first run script, similar to the previous k-means analysis and associated parameters, though instead here we will use data reduced values from the PCA analysis. Specifically, clusters across PCA “scores” values will be derived, where scores represent the relative amount each chemical contributed to each principal component. clusters_PCA &lt;- kmeans(my.pca$scores, centers = num.centers, iter.max = 1000, nstart = 50) The resulting PCA score values that were derived as the final cluster centers can be pulled using: clusters_PCA$centers ## Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 ## 1 -1.191669 0.1393319 0.2836947 -0.004022509 -0.08434048 -0.02299446 ## 2 2.621672 -0.3065303 -0.6241284 0.008849520 0.18554906 0.05058781 ## Comp.7 Comp.8 Comp.9 Comp.10 ## 1 -0.01558687 0.02403981 0.008361355 -0.005429933 ## 2 0.03429111 -0.05288759 -0.018394982 0.011945853 Viewing the final cluster assignment, per chemical: head(cbind(rownames(dat.x),clusters_PCA$cluster)) ## [,1] [,2] ## 93762-09-5 &quot;93762-09-5&quot; &quot;1&quot; ## 60270-55-5 &quot;60270-55-5&quot; &quot;1&quot; ## 70225-15-9 &quot;70225-15-9&quot; &quot;1&quot; ## 335-24-0 &quot;335-24-0&quot; &quot;1&quot; ## 647-29-0 &quot;647-29-0&quot; &quot;1&quot; ## 68259-12-1 &quot;68259-12-1&quot; &quot;1&quot; Visualizing k-means clusters from PCA results Let’s now view, again, the results of the main PCA, focusing on the first two principal components; though this time let’s color each chemical according to k-means cluster. ggplot(as.data.frame(my.pca$scores), aes(x=Comp.1, y=Comp.2, color=as.factor(clusters_PCA$cluster))) + geom_point(size=4) + theme_bw() + ggtitle(&#39;Version D: PCA Plot of the First 2 PCs, colored by k-means Clustering&#39;) + xlab(&quot;Principal Component 1&quot;) + ylab(&quot;Principal Component 2&quot;) With this, we can answer Environmental Health Question 5: If we did not have information telling us which chemical belonged to which class, could we use PCA and k-means to accurately predict whether a chemical is a PFAS vs statin? Answer: Yes!! Groupings derived from k-means, displayed in this PCA plot, line up almost exactly with the grouping of chemical classes (see Version C of this plot as the direct comparison). We can also answer Environmental Health Question 6: What kinds of applications/endpoints can be better understood and/or predicted, because of these derived chemical groupings? Answer: - With these well-informed chemical groupings, we can now better understand the variables that attribute to the chemical classifications. - We can also use this information to better understand data trends, and predict environmental fate and transport for these chemicals. - The reduced variables derived through PCA, and/or k-means clustering patterns can also be used as input variables to predict toxicological outcomes. Concluding Remarks In conclusion, this training module provide an example exercise on organizing physicochemical data, and analyzing trends within these data to determine chemical groupings. Results are compared from those produced using just the original data vs. clustered data from k-means vs. reduced data from PCA. K-means is then used in combination with PCA approaches to showcase the power of these machine learning methods, where the classes of each chemical were able to be predicted with high levels of accuracy. These methods represent common tools that are used in high dimensional data analyses within the field of environmental health sciences. For additional case studies that leverage more advanced machine learning techniques, see the following recent publications that also address environmental health questions from our research groups: Clark J, Avula V, Ring C, Eaves LA, Howard T, Santos HP, Smeester L, Bangma JT, O’Shea TM, Fry RC, Rager JE. Comparing the Predictivity of Human Placental Gene, microRNA, and CpG Methylation Signatures in Relation to Perinatal Outcomes. Toxicol Sci. 2021 Sep 28;183(2):269-284. PMID: 34255065 Green AJ, Mohlenkamp MJ, Das J, Chaudhari M, Truong L, Tanguay RL, Reif DM. Leveraging high-throughput screening data, deep neural networks, and conditional generative adversarial networks to advance predictive toxicology. PLoS Comput Biol. 2021 Jul 2;17(7):e1009135. PMID: 3421407 Odenkirk MT, Reif DM, Baker ES. Multiomic Big Data Analysis Challenges: Increasing Confidence in the Interpretation of Artificial Intelligence Assessments. Anal Chem. 2021 Jun 8;93(22):7763-7773. PMID: 34029068 To KT, Truong L, Edwards S, Tanguay RL, Reif DM. Multivariate modeling of engineered nanomaterial features associated with developmental toxicity. NanoImpact. 2019 Apr;16:10.1016. PMID: 32133425 Ring C, Sipes NS, Hsieh JH, Carberry C, Koval LE, Klaren WD, Harris MA, Auerbach SS, Rager JE. Predictive modeling of biological responses in the rat liver using in vitro Tox21 bioactivity: Benefits from high-throughput toxicokinetics. Comput Toxicol. 2021 May;18:100166. PMID: 34013136 "],["mixtures-analyses.html", "2.3 Mixtures Analyses Introduction to Training Module Chemistry-based Approach Toxicity-based Approach Comparing Results Concluding Remarks", " 2.3 Mixtures Analyses This training module was developed by Dr. Cynthia Rider, with contributions from Lauren E. Koval and Dr. Julia E. Rager. Fall 2021 Introduction to Mixtures Toxicology Humans are rarely, if ever, exposed to single chemicals at a time. Instead, humans are often exposed to multiple stressors in their everyday environments in the form of mixtures. These stressors can include environmental chemicals and pharmaceuticals, and they can also include other types of stressors such as socioeconomic factors and other attributes that can place individuals at increased risk of acquiring disease. Because it is not possible to test every possible combination of exposure that an individual might experience in their lifetime, approaches that take into account variable and complex exposure conditions through mixtures modeling are needed. Some helpful resources that provide further background on the topic of mixtures toxicology and mixtures modeling include the following: Carlin DJ, Rider CV, Woychik R, Birnbaum LS. Unraveling the health effects of environmental mixtures: an NIEHS priority. Environ Health Perspect. 2013 Jan;121(1):A6-8. PMID: 23409283. Drakvik E, Altenburger R, Aoki Y, Backhaus T, Bahadori T, Barouki R, Brack W, Cronin MTD, Demeneix B, Hougaard Bennekou S, van Klaveren J, Kneuer C, Kolossa-Gehring M, Lebret E, Posthuma L, Reiber L, Rider C, Rüegg J, Testa G, van der Burg B, van der Voet H, Warhurst AM, van de Water B, Yamazaki K, Öberg M, Bergman Å. Statement on advancing the assessment of chemical mixtures and their risks for human health and the environment. Environ Int. 2020 Jan;134:105267. PMID: 31704565. Rider CV, McHale CM, Webster TF, Lowe L, Goodson WH 3rd, La Merrill MA, Rice G, Zeise L, Zhang L, Smith MT. Using the Key Characteristics of Carcinogens to Develop Research on Chemical Mixtures and Cancer. Environ Health Perspect. 2021 Mar;129(3):35003. PMID: 33784186. Taylor KW, Joubert BR, Braun JM, Dilworth C, Gennings C, Hauser R, Heindel JJ, Rider CV, Webster TF, Carlin DJ. Statistical Approaches for Assessing Health Effects of Environmental Chemical Mixtures in Epidemiology: Lessons from an Innovative Workshop. Environ Health Perspect. 2016 Dec 1;124(12):A227-A229. PMID: 27905274. Computational Approaches to Address Mixtures There are different computational approaches that can be implemented to address this research topic. This training module focuses on a method called Sufficient Similarity to determine which groups of exposure conditions are chemically/biologically similar enough to be regulated for safety together, based on the same set of regulatory criteria. Computational approaches that can be used for exposure groupings include principal component analysis and clustering, to elucidate patterns among high-dimensional data. More example publications that have used this approach to address environmental health questions are detailed at the bottom of this training module. Another approach that is commonly used when evaluating mixtures is called Relative Potency Factors. This is also known as a component-based approach. Component-based approaches use data from individual chemicals (components of the mixture) and additivity models to estimate the effects of the mixture. There are many methods that can be leveraged to also elucidate relationships between individual chemicals/chemical groups in complex mixtures and their resulting toxicity/health effects. As an example, we recently published an environmentally relevant example that implemented quantile g-computation statistics to identify chemicals present in wildfire smoke emissions that impact toxicity. This example study was published in the following: Rager JE, Clark J, Eaves LA, Avula V, Niehoff NM, Kim YH, Jaspers I, Gilmour MI. Mixtures modeling identifies chemical inducers versus repressors of toxicity associated with wildfire smoke. Sci Total Environ. 2021 Jun 25;775:145759. PMID: 33611182. Introduction to Training Module This module serves as an example mixtures analysis characterizing the nutritional supplement, Ginkgo biloba. Ginkgo biloba represents a popular type of botanical supplement currently on the market. People take Ginkgo biloba to improve brain function, but there is conflicting data on its efficacy. Like other botanicals, Ginkgo biloba is a complex mixture with 100s-1000s of constituents. Here, the variability in chemical and toxicological profiles across samples of Ginkgo biloba purchased from different commercial sources is evaluated. We can use data from a well-characterized sample (reference sample) to evaluate the safety of other samples that are ‘sufficiently similar’ to the reference sample. Samples that are different (i.e., do not meet the standards of sufficient similarity) from the reference sample would require additional safety data. A total of 29 Ginkgo biloba extract samples were analyzed. These samples are abbreviated as “GbE_” followed by a unique sample identifier (GbE = Ginkgo biloba Extract). These data have been previously published: Catlin NR, Collins BJ, Auerbach SS, Ferguson SS, Harnly JM, Gennings C, Waidyanatha S, Rice GE, Smith-Roe SL, Witt KL, Rider CV. How similar is similar enough? A sufficient similarity case study with Ginkgo biloba extract. Food Chem Toxicol. 2018 Aug;118:328-339. PMID: 29752982. Collins BJ, Kerns SP, Aillon K, Mueller G, Rider CV, DeRose EF, London RE, Harnly JM, Waidyanatha S. Comparison of phytochemical composition of Ginkgo biloba extracts using a combination of non-targeted and targeted analytical approaches. Anal Bioanal Chem. 2020 Oct;412(25):6789-6809. PMID: 32865633. Ginkgo biloba chemistry dataset background The chemical profiles of these sample extracts were first analyzed using targeted mass spectrometry-based approaches. The concentrations of 12 Ginkgo biloba marker compounds were measured in units of mean weight as a ratio [g chemical / g sample]. Note that in this dataset, non-detects have been replaced with values of zero for simplicity; though there are more advanced methods to impute values for non-detects. Script is provided to evaluate how Ginkgo biloba extracts group together, based on chemical profiles. Ginkgo biloba toxicity dataset background The toxicological profiles of these samples were also analyzed using in vitro test methods. These data represent area under the curve (AUC) values indicating changes in gene expression across various concentrations of the Ginkgo biloba extract samples. Positive AUC values indicate a gene that was collectively increased in expression as concentration increased, and a negative AUC value indicates a gene that was collectively decreased in expression as exposure concentration increased. Script is provided to evaluate how Ginkgo biloba extracts group together, based on toxicity profiles. Training Module’s Environmental Health Questions This training module was specifically developed to answer the following environmental health questions: Based on the chemical analysis, which Ginkgo biloba extract looks the most different? When viewing the variability between chemical profiles, how many groupings of potentially ‘sufficiently similar’ Ginkgo biloba samples do you see? Based on the chemical analysis, which chemicals do you think are important in differentiating between the different Ginkgo biloba samples? After removing two samples that have the most different chemical profiles (and are thus, potential outliers), do we obtain similar chemical groupings? When viewing the variability between toxicity profiles, how many groupings of potentially ‘sufficiently similar’ Ginkgo biloba samples do you see? Based on the toxicity analysis, which genes do you think are important in differentiating between the different Ginkgo biloba samples? Were similar chemical groups identified when looking at just the chemistry vs. just the toxicity? How could this impact regulatory decisions, if we only had one of these datasets? Script Preparations Cleaning the global environment rm(list=ls()) Installing required R packages If you already have these packages installed, you can skip this step, or you can run the below code which checks installation status for you. if (!requireNamespace(&quot;tidyverse&quot;)) install.packages(&quot;tidyverse&quot;); if (!requireNamespace(&quot;readxl&quot;)) install.packages(&quot;readxl&quot;); if (!requireNamespace(&quot;factoextra&quot;)) install.packages(&quot;factoextra&quot;); if (!requireNamespace(&quot;pheatmap&quot;)) install.packages(&quot;pheatmap&quot;); if (!requireNamespace(&quot;gridExtra&quot;)) install.packages(&quot;gridExtra&quot;); if (!requireNamespace(&quot;ggplotify&quot;)) install.packages(&quot;ggplotify&quot;) Loading R packages # Used to read in and work with excel files library(readxl) #Used to run and visualize multivariate analyses, here PCA library(factoextra) # Used to make heat maps. This can be done in ggplot2 but pheatmap is easier and nicer library(pheatmap) # Used to arrange and visualize multiple figures at once library(gridExtra) # Used to make non ggplot figures (like a pheatmap) gg compatible library(ggplotify) # All tidyverse packages, including dplyr and ggplot2 library(tidyverse) Set your working directory setwd(&quot;/filepath to where your input files are&quot;) Loading the Example Ginkgo biloba Datasets We need to first read in the chemistry and toxicity data from the provided excel file. Here, data were originally organized such that the actual observations start on row 2 (dataset descriptions were in the first row). So let’s implement skip=1, which skips reading in the first row. # Loads the chemistry data tab chem &lt;- read_xlsx(&quot;Module2_3/Module2_3_SufficientSimilarity_Data.xlsx&quot; , sheet = &quot;chemistry data&quot;, skip=1) # Loads the toxicity data tab tox &lt;- read_xlsx(&quot;Module2_3/Module2_3_SufficientSimilarity_Data.xlsx&quot; , sheet = &quot;in vitro data&quot;, skip=1) Data Viewing Let’s first see how many rows and columns of data are present in both datasets. dim(chem) ## [1] 29 13 The chemistry dataset contains information on 29 samples (rows); and 1 sample identifier + 12 chemicals (total of 13 columns) dim(tox) ## [1] 29 6 The tox dataset contains information on 29 samples (rows); and 1 sample identifier + 5 genes (total of 6 columns) Let’s also see what kind of data are organized within the datasets. colnames(chem) ## [1] &quot;Sample&quot; &quot;Bilobalide&quot; &quot;Ginkgolide_A&quot; ## [4] &quot;Ginkgolide_B&quot; &quot;Ginkgolide_C&quot; &quot;Ginkgolide_J&quot; ## [7] &quot;Rutin&quot; &quot;Quercetin&quot; &quot;Kaempferol&quot; ## [10] &quot;Isorhamnetin&quot; &quot;Ginkgolic_Acid_C15&quot; &quot;Ginkgolic_Acid_C17&quot; ## [13] &quot;Ginkgotoxin&quot; head(chem) ## # A tibble: 6 × 13 ## Sample Bilobalide Ginkgolide_A Ginkgolide_B Ginkgolide_C Ginkgolide_J Rutin ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 GbE_A 1.28 0 0 0 1.77 0 ## 2 GbE_B 0 0 0 0 0 0.05 ## 3 GbE_C 0 0 0 0 0 0.01 ## 4 GbE_D 1.28 2.6 1.6 2.79 1.18 1.11 ## 5 GbE_E 1.5 2.13 1.46 2.6 1.21 1.21 ## 6 GbE_F 0 0 0 0 0 0.04 ## # … with 6 more variables: Quercetin &lt;dbl&gt;, Kaempferol &lt;dbl&gt;, ## # Isorhamnetin &lt;dbl&gt;, Ginkgolic_Acid_C15 &lt;dbl&gt;, Ginkgolic_Acid_C17 &lt;dbl&gt;, ## # Ginkgotoxin &lt;dbl&gt; colnames(tox) ## [1] &quot;Sample&quot; &quot;ABCB11&quot; &quot;CYP1A2&quot; &quot;CYP2B6&quot; &quot;CYP3A4&quot; &quot;HMGCS2&quot; head(tox) ## # A tibble: 6 × 6 ## Sample ABCB11 CYP1A2 CYP2B6 CYP3A4 HMGCS2 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 GbE_A -0.450 0.778 0.124 -1.39 -0.0208 ## 2 GbE_B -0.210 0.902 0.456 -1.22 -0.149 ## 3 GbE_C -1.10 1.32 1.58 -1.62 0.195 ## 4 GbE_D -0.818 1.61 2.46 0.935 -1.62 ## 5 GbE_E -0.963 2.27 2.44 1.33 -1.54 ## 6 GbE_F -0.0828 1.2 0.587 -1.29 -0.147 Chemistry-based Approach The first method employed in this Sufficient Similarity analysis is Principal Component Analysis (PCA). PCA is a very common dimensionality reduction technique, as detailed in the ‘Machine Learning and Predictive Modeling’ training module. In summary, PCA finds dimensions (eigenvectors) in the higher dimensional original data that capture as much of the variation as possible, which you can then plot. This allows you to project higher dimensional data, in this case 12 dimensions (representing 12 measured chemicals), in fewer dimensions (we’ll use 2). These dimensions, or components, capture the “essence” of the original dataset. Before we can run PCA on this chemistry dataset, we first need to scale the data across samples. We do this here for the chemistry dataset, because we specifically want to evaluate and potentially highlight/emphasize chemicals that may be at relatively low abundance. These low-abundance chemicals may actually be contaminants that drive toxicological effects. Let’s first re-save the original chemistry dataset to compare off of: chem_original &lt;- chem We also can make a scaled version to carry forward in this analysis. Here, we move the sample column the row names then scale and center data: chem &lt;- chem %&gt;% column_to_rownames(&quot;Sample&quot;) chem &lt;- as.data.frame(scale(as.matrix(chem))) Let’s now compare one of the rows of data (here, sample GbE_E) to see what scaling did: chem_original[5,] ## # A tibble: 1 × 13 ## Sample Bilobalide Ginkgolide_A Ginkgolide_B Ginkgolide_C Ginkgolide_J Rutin ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 GbE_E 1.5 2.13 1.46 2.6 1.21 1.21 ## # … with 6 more variables: Quercetin &lt;dbl&gt;, Kaempferol &lt;dbl&gt;, ## # Isorhamnetin &lt;dbl&gt;, Ginkgolic_Acid_C15 &lt;dbl&gt;, Ginkgolic_Acid_C17 &lt;dbl&gt;, ## # Ginkgotoxin &lt;dbl&gt; chem[5,] ## Bilobalide Ginkgolide_A Ginkgolide_B Ginkgolide_C Ginkgolide_J Rutin ## GbE_E -0.4996768 0.02749315 0.2559249 1.36305 0.3842836 0.5758008 ## Quercetin Kaempferol Isorhamnetin Ginkgolic_Acid_C15 Ginkgolic_Acid_C17 ## GbE_E 0.2059607 -0.2420023 -0.61695 -0.2737699 -0.2317415 ## Ginkgotoxin ## GbE_E -0.8114437 You can see that scaling made the concentrations distributed across each chemical center around 0. Now, we can run PCA on the scaled data: chem_pca &lt;- princomp(chem) Looking at the scree plot, we see the first two principal components capture most of the variance in the data (~64%). fviz_eig(chem_pca) Here are the resulting PCA scores for each sample, for each principal component (shown here as components 1-12). head(chem_pca$scores) ## Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 ## GbE_A -2.6256689 -0.53470486 0.38196672 1.5147505 1.34638144 -0.24020735 ## GbE_B -3.7343187 -1.07490206 0.07766108 0.2529618 -0.01731847 -0.02446797 ## GbE_C -3.8019563 -1.06232680 0.08335445 0.2588084 -0.01912736 -0.03423438 ## GbE_D 0.7175782 -0.09566345 0.90082998 -0.7762090 -0.40507924 -0.78361086 ## GbE_E 0.4157644 -0.14794948 1.16402759 -0.4856176 -0.15497152 -0.64206760 ## GbE_F -3.7621819 -1.04784203 0.08687721 0.2503188 -0.02195005 -0.04030373 ## Comp.7 Comp.8 Comp.9 Comp.10 Comp.11 Comp.12 ## GbE_A -0.8890488 0.8145597 0.15021776 -0.54318277 -0.32353295 0.049538024 ## GbE_B 0.3988596 -0.3227102 -0.10344907 -0.12911495 0.11127631 0.008306532 ## GbE_C 0.3817061 -0.2788579 -0.13057528 -0.02613584 0.08148776 0.011485316 ## GbE_D -1.1916851 -0.4306198 0.08460588 0.26115540 0.01065657 -0.053819603 ## GbE_E -1.1412900 -0.5632547 0.12309347 -0.02872126 0.24882196 0.047691048 ## GbE_F 0.3948245 -0.3105324 -0.10539998 -0.11015645 0.10607314 0.012066512 And the resulting loading factors of each chemical’s contribution towards each principal component. Results are arranged by a chemical’s contribution to PC1, the component accounting for most of the variation in the data. head(chem_pca$loadings) ## Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 ## Bilobalide 0.3260729 0.085117226 0.24541927 0.36968834 0.03575030 ## Ginkgolide_A 0.3732546 0.184101347 0.08586157 0.02446723 -0.03299212 ## Ginkgolide_B 0.3611463 0.201280307 0.05996588 -0.19928568 -0.05965128 ## Ginkgolide_C 0.3616497 0.001365838 0.12157943 -0.24092774 -0.10470516 ## Ginkgolide_J 0.2995761 0.178689209 0.09144978 0.35348565 0.47372240 ## Rutin 0.1763429 0.012553984 0.59496115 -0.47712128 0.08063637 ## Comp.6 Comp.7 Comp.8 Comp.9 Comp.10 ## Bilobalide 0.004975969 0.55079870 0.35062913 0.23584371 0.25226337 ## Ginkgolide_A -0.247529927 0.30284388 -0.30206946 -0.01601813 0.20620248 ## Ginkgolide_B -0.068401213 -0.00687696 0.09497565 -0.83752355 0.02062813 ## Ginkgolide_C -0.185456111 -0.45474675 -0.34421625 0.32067335 0.45797955 ## Ginkgolide_J -0.062170758 -0.55785820 0.36066655 0.03820900 -0.14858754 ## Rutin 0.517141873 0.05060803 0.03111005 0.15083257 -0.27469825 ## Comp.11 Comp.12 ## Bilobalide 0.29295985 0.23837797 ## Ginkgolide_A -0.60099943 -0.41172878 ## Ginkgolide_B 0.24851189 0.06938891 ## Ginkgolide_C 0.28752202 0.17463609 ## Ginkgolide_J -0.22581449 -0.03024110 ## Rutin -0.09884752 -0.04092322 # Pulling the chemical-specific loadings into a separate matrix loadings &lt;- as.data.frame.matrix(chem_pca$loadings) # Sorting the loadings from highest to lowest for component #1 loadings %&gt;% arrange(desc(Comp.1)) ## Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 ## Quercetin 0.3801459 -0.001945021 -0.19193647 0.04697879 0.007656212 ## Ginkgolide_A 0.3732546 0.184101347 0.08586157 0.02446723 -0.032992122 ## Ginkgolide_C 0.3616497 0.001365838 0.12157943 -0.24092774 -0.104705164 ## Ginkgolide_B 0.3611463 0.201280307 0.05996588 -0.19928568 -0.059651275 ## Bilobalide 0.3260729 0.085117226 0.24541927 0.36968834 0.035750299 ## Kaempferol 0.3001354 -0.085004317 -0.29667523 -0.15611039 -0.655825688 ## Ginkgolide_J 0.2995761 0.178689209 0.09144978 0.35348565 0.473722400 ## Isorhamnetin 0.2740348 -0.075119327 -0.29665890 0.39008587 -0.060880190 ## Rutin 0.1763429 0.012553984 0.59496115 -0.47712128 0.080636368 ## Ginkgotoxin 0.1675373 -0.034318656 -0.56232119 -0.47935782 0.551341021 ## Ginkgolic_Acid_C15 -0.1201265 0.667543042 -0.11607308 -0.05802100 -0.085384063 ## Ginkgolic_Acid_C17 -0.1418140 0.653142232 -0.09559022 -0.02189315 -0.084006824 ## Comp.6 Comp.7 Comp.8 Comp.9 Comp.10 ## Quercetin -0.249799236 0.13263831 -0.30855302 0.18612332 -0.72929122 ## Ginkgolide_A -0.247529927 0.30284388 -0.30206946 -0.01601813 0.20620248 ## Ginkgolide_C -0.185456111 -0.45474675 -0.34421625 0.32067335 0.45797955 ## Ginkgolide_B -0.068401213 -0.00687696 0.09497565 -0.83752355 0.02062813 ## Bilobalide 0.004975969 0.55079870 0.35062913 0.23584371 0.25226337 ## Kaempferol 0.050018874 -0.12464461 0.51491286 0.16055155 -0.07828551 ## Ginkgolide_J -0.062170758 -0.55785820 0.36066655 0.03820900 -0.14858754 ## Isorhamnetin 0.730543567 -0.06658953 -0.34052044 -0.10456587 0.10158173 ## Rutin 0.517141873 0.05060803 0.03111005 0.15083257 -0.27469825 ## Ginkgotoxin 0.092918281 0.20744490 0.16087302 0.11145659 0.17463719 ## Ginkgolic_Acid_C15 0.057775937 0.03440329 -0.13417826 0.06575084 -0.07780733 ## Ginkgolic_Acid_C17 0.133278823 -0.05017155 0.08415192 0.16593739 0.03564092 ## Comp.11 Comp.12 ## Quercetin 0.273123642 0.008854815 ## Ginkgolide_A -0.600999427 -0.411728782 ## Ginkgolide_C 0.287522018 0.174636086 ## Ginkgolide_B 0.248511890 0.069388910 ## Bilobalide 0.292959851 0.238377968 ## Kaempferol -0.211380567 -0.020939233 ## Ginkgolide_J -0.225814490 -0.030241100 ## Isorhamnetin 0.002690835 -0.006305513 ## Rutin -0.098847524 -0.040923217 ## Ginkgotoxin -0.005807642 0.016904160 ## Ginkgolic_Acid_C15 -0.285797465 0.633437667 ## Ginkgolic_Acid_C17 0.383124914 -0.577639931 These resulting loading factors allow us to identify which constituents (of the 12 total) contribute to the principal components explaining data variabilities. For instance, we can see here that: Quercetin is listed at the top, with the largest loading value for principal component 1. Thus, Quercetin represents the constituents that contributes to the overall variability in the dataset to the greatest extent. The next three chemicals are all Ginkgolide constituents, followed by Bilobalide and Kaempferol, and so forth. If we look at principal component 2, we can now see a different set of chemicals contributing to the variability captured in this component: # Sorting the loadings from highest to lowest for component #2 loadings %&gt;% arrange(desc(Comp.2)) ## Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 ## Ginkgolic_Acid_C15 -0.1201265 0.667543042 -0.11607308 -0.05802100 -0.085384063 ## Ginkgolic_Acid_C17 -0.1418140 0.653142232 -0.09559022 -0.02189315 -0.084006824 ## Ginkgolide_B 0.3611463 0.201280307 0.05996588 -0.19928568 -0.059651275 ## Ginkgolide_A 0.3732546 0.184101347 0.08586157 0.02446723 -0.032992122 ## Ginkgolide_J 0.2995761 0.178689209 0.09144978 0.35348565 0.473722400 ## Bilobalide 0.3260729 0.085117226 0.24541927 0.36968834 0.035750299 ## Rutin 0.1763429 0.012553984 0.59496115 -0.47712128 0.080636368 ## Ginkgolide_C 0.3616497 0.001365838 0.12157943 -0.24092774 -0.104705164 ## Quercetin 0.3801459 -0.001945021 -0.19193647 0.04697879 0.007656212 ## Ginkgotoxin 0.1675373 -0.034318656 -0.56232119 -0.47935782 0.551341021 ## Isorhamnetin 0.2740348 -0.075119327 -0.29665890 0.39008587 -0.060880190 ## Kaempferol 0.3001354 -0.085004317 -0.29667523 -0.15611039 -0.655825688 ## Comp.6 Comp.7 Comp.8 Comp.9 Comp.10 ## Ginkgolic_Acid_C15 0.057775937 0.03440329 -0.13417826 0.06575084 -0.07780733 ## Ginkgolic_Acid_C17 0.133278823 -0.05017155 0.08415192 0.16593739 0.03564092 ## Ginkgolide_B -0.068401213 -0.00687696 0.09497565 -0.83752355 0.02062813 ## Ginkgolide_A -0.247529927 0.30284388 -0.30206946 -0.01601813 0.20620248 ## Ginkgolide_J -0.062170758 -0.55785820 0.36066655 0.03820900 -0.14858754 ## Bilobalide 0.004975969 0.55079870 0.35062913 0.23584371 0.25226337 ## Rutin 0.517141873 0.05060803 0.03111005 0.15083257 -0.27469825 ## Ginkgolide_C -0.185456111 -0.45474675 -0.34421625 0.32067335 0.45797955 ## Quercetin -0.249799236 0.13263831 -0.30855302 0.18612332 -0.72929122 ## Ginkgotoxin 0.092918281 0.20744490 0.16087302 0.11145659 0.17463719 ## Isorhamnetin 0.730543567 -0.06658953 -0.34052044 -0.10456587 0.10158173 ## Kaempferol 0.050018874 -0.12464461 0.51491286 0.16055155 -0.07828551 ## Comp.11 Comp.12 ## Ginkgolic_Acid_C15 -0.285797465 0.633437667 ## Ginkgolic_Acid_C17 0.383124914 -0.577639931 ## Ginkgolide_B 0.248511890 0.069388910 ## Ginkgolide_A -0.600999427 -0.411728782 ## Ginkgolide_J -0.225814490 -0.030241100 ## Bilobalide 0.292959851 0.238377968 ## Rutin -0.098847524 -0.040923217 ## Ginkgolide_C 0.287522018 0.174636086 ## Quercetin 0.273123642 0.008854815 ## Ginkgotoxin -0.005807642 0.016904160 ## Isorhamnetin 0.002690835 -0.006305513 ## Kaempferol -0.211380567 -0.020939233 With Ginkgolic Acids listed first here. Visualizing Samples by PCs We can also visualize sample groupings based on these principal components 1 &amp; 2. To view the PCA plot: # First pull the percent variation captured by each component pca_percent &lt;- round(100*chem_pca$sdev^2/sum(chem_pca$sdev^2),1) # Then make a dataframe for the PCA plot generation script using first three components pca_df &lt;- data.frame(PC1 = chem_pca$scores[,1], PC2 = chem_pca$scores[,2]) # Plot this dataframe chem_pca_plt &lt;- ggplot(pca_df, aes(PC1,PC2))+ geom_hline(yintercept = 0, size=0.3)+ geom_vline(xintercept = 0, size=0.3)+ geom_point(size=3, color=&quot;deepskyblue3&quot;) + geom_text(aes(label=rownames(pca_df)), fontface=&quot;bold&quot;, position=position_jitter(width=0.4,height=0.4))+ labs(x=paste0(&quot;PC1 (&quot;,pca_percent[1],&quot;%)&quot;), y=paste0(&quot;PC2 (&quot;,pca_percent[2],&quot;%)&quot;))+ ggtitle(&quot;GbE Sample PCA by Chemistry Profiles&quot;) # Changing the colors of the titles and axis text chem_pca_plt &lt;- chem_pca_plt + theme(plot.title=element_text(color=&quot;deepskyblue3&quot;, face=&quot;bold&quot;), axis.title.x=element_text(color=&quot;deepskyblue3&quot;, face=&quot;bold&quot;), axis.title.y=element_text(color=&quot;deepskyblue3&quot;, face=&quot;bold&quot;)) # Viewing this resulting plot chem_pca_plt This plot tells us a lot about sample groupings based on chemical profiles! With this, we can answer Environmental Health Question 1: Based on the chemical analysis, which Ginkgo biloba extract looks the most different? Answer: GbE_G We can also answer Environmental Health Question 2: When viewing the variability between chemical profiles, how many groupings of potentially ‘sufficiently similar’ Ginkgo biloba samples do you see? Answer: Approximately 4 (though could argue +1/-1): bottom left group; bottom right group; and two completely separate samples of GbE_G and GbE_N Heat Map of the Mixtures Chemistry Data As an alternative way of viewing the chemical profile data, we can make a heat map of the scaled chemistry data. We concurrently run hierarchical clustering that shows us how closely samples are related to each other, based on different algorithms than data reduction-based PCA. Samples that fall on nearby branches are more similar. Samples that don’t share branches with many/any others are often considered outliers. By default, pheatmap uses a Euclidean distance to cluster the observations, which is a very common clustering algorithm. For more details, see the following description of Euclidean distance. chem_hm &lt;- pheatmap(chem, main=&quot;GbE Sample Heat Map by Chemistry Profiles&quot;, cluster_rows=TRUE, cluster_cols = FALSE, angle_col = 45, fontsize_col = 7, treeheight_row = 60) This plot tells us a lot about the individual chemicals that differentiate the sample groupings With this, we can answer Environmental Health Question 3: Based on the chemical analysis, which chemicals do you think are important in differentiating between the different Ginkgo biloba samples? Answer: All of the chemicals technically contribute to these sample patterns, but here are some that stand out: (i) Ginkgolic_Acid_C15 and Ginkgolic_Acid_C17 appear to drive the clustering of one particular GbE sample, GbE_G, as well as potentially GbE_N; (ii) Isorhamnetin influences the clustering of GbE_T; (iii) Bilobalide, Ginkgolides A &amp; B, and Quercetin are also important because they show a general cluster of abundance at decreased levels at the bottom and increased levels at the top Let’s now revisit the PCA plot. chem_pca_plt GbE_G and GbE_N look so different from the rest of the samples, they could be outliers and potentially influencing overall data trends. Let’s make sure that, if we remove these two samples, our sample groupings still look the same. chem_filt &lt;- chem %&gt;% rownames_to_column(&quot;Sample&quot;) %&gt;% filter(!Sample %in% c(&quot;GbE_G&quot;,&quot;GbE_N&quot;)) %&gt;% column_to_rownames(&quot;Sample&quot;) Now lets re-run PCA and generate a heat map on the chemical data with these outlier samples removed. chem_filt_pca &lt;- princomp(chem_filt) # Get the percent variation captured by each component pca_percent_filt &lt;- round(100*chem_filt_pca$sdev^2/sum(chem_filt_pca$sdev^2),1) # Make dataframe for PCA plot generation using first three components pca_df_filt &lt;- data.frame(PC1 = chem_filt_pca$scores[,1], PC2 = chem_filt_pca$scores[,2]) # Plot this dataframe chem_filt_pca_plt &lt;- ggplot(pca_df_filt, aes(PC1,PC2))+ geom_hline(yintercept = 0, size=0.3)+ geom_vline(xintercept = 0, size=0.3)+ geom_point(size=3, color=&quot;aquamarine2&quot;) + geom_text(aes(label=rownames(pca_df_filt)), fontface=&quot;bold&quot;, position=position_jitter(width=0.5,height=0.5))+ labs(x=paste0(&quot;PC1 (&quot;,pca_percent[1],&quot;%)&quot;), y=paste0(&quot;PC2 (&quot;,pca_percent[2],&quot;%)&quot;))+ ggtitle(&quot;GbE Sample PCA by Chemistry Profiles excluding Potential Outliers&quot;) # Changing the colors of the titles and axis text chem_filt_pca_plt &lt;- chem_filt_pca_plt + theme(plot.title=element_text(color=&quot;aquamarine2&quot;, face=&quot;bold&quot;), axis.title.x=element_text(color=&quot;aquamarine2&quot;, face=&quot;bold&quot;), axis.title.y=element_text(color=&quot;aquamarine2&quot;, face=&quot;bold&quot;)) # Viewing this resulting plot chem_filt_pca_plt We can view the full samples vs filtered samples chemistry PCA plots together. grid.arrange(chem_pca_plt, chem_filt_pca_plt) With these plots, side-by-side, we can now answer Environmental Health Question 4: After removing two samples that have the most different chemical profiles (and are thus, potential outliers), do we obtain similar chemical groupings? Answer: Yes! Removal of the potential outliers basically spreads the rest of the remaining data points out, since there is less variance in the overall dataset, and thus, more room to show variance amongst the remaining samples. The general locations of the samples on the PCA plot, however, remain consistent. We now feel confident that our similarity analysis is producing consistent grouping results. Toxicity-based Approach The first method employed in this Sufficient Similarity analysis is, again, Principal Component Analysis (PCA). Unlike the chemistry dataset, we can use the toxicity dataset as is without scaling. The reason we want to analyze the raw data is because we want to emphasize genes that are showing a large response. Similarly, we want to demphasize genes that are not doing much in response to the exposure condition. If we scale these data, we will reduce this needed variability. So here, we first move the sample column to row names: tox &lt;- tox %&gt;% column_to_rownames(&quot;Sample&quot;) Then, we can run PCA on this tox dataframe: tox_pca &lt;- princomp(tox) Looking at the scree plot, we see the first two principal components capture most of the variation (~93%): fviz_eig(tox_pca) Plot the samples by principal components: # Get the percent variation captured by each component pca_percent &lt;- round(100*tox_pca$sdev^2/sum(tox_pca$sdev^2),1) # Make dataframe for PCA plot generation using first three components tox_pca_df &lt;- data.frame(PC1 = tox_pca$scores[,1], PC2 = tox_pca$scores[,2]) # Plot the first two components tox_pca_plt &lt;- ggplot(tox_pca_df, aes(PC1,PC2))+ geom_hline(yintercept = 0, size=0.3)+ geom_vline(xintercept = 0, size=0.3)+ geom_point(size=3, color=&quot;deeppink3&quot;) + geom_text(aes(label=rownames(pca_df)), fontface=&quot;bold&quot;, position=position_jitter(width=0.25,height=0.25))+ labs(x=paste0(&quot;PC1 (&quot;,pca_percent[1],&quot;%)&quot;), y=paste0(&quot;PC2 (&quot;,pca_percent[2],&quot;%)&quot;))+ ggtitle(&quot;GbE Sample PCA by Toxicity Profiles&quot;) # Changing the colors of the titles and axis text tox_pca_plt &lt;- tox_pca_plt + theme(plot.title=element_text(color=&quot;deeppink3&quot;, face=&quot;bold&quot;), axis.title.x=element_text(color=&quot;deeppink3&quot;, face=&quot;bold&quot;), axis.title.y=element_text(color=&quot;deeppink3&quot;, face=&quot;bold&quot;)) tox_pca_plt With this, we can answer Environmental Health Question 5: When viewing the variability between toxicity profiles, how many groupings of potentially ‘sufficiently similar’ Ginkgo biloba samples do you see? Answer: Approximately 3 (though could argue +1/-1): top left group; top right group; GbE_M and GbE_W Heat Map of the Mixtures Toxicity Data As an alternative way of viewing the toxicity profile data, we can make a heat map of the tox data. tox_hm &lt;- pheatmap(tox, main=&quot;GbE Sample Heat Map by Toxicity Profiles&quot;, cluster_rows=TRUE, cluster_cols = FALSE, angle_col = 45, fontsize_col = 7, treeheight_row = 60) This plot tells us a lot about the individual genes that differentiate the sample groupings. With this, we can answer Environmental Health Question 6: Based on the toxicity analysis, which genes do you think are important in differentiating between the different Ginkgo biloba samples? Answer: It looks like the CYP enzyme genes, particularly CYP2B6, are highly up-regulated in response to several of these sample exposures, and thus dictate a lot of these groupings. Comparing Results Chemistry vs. Toxicity Sufficient Similarity Analyses Let’s view the PCA plots for both datasets together, side-by-side: pca_compare &lt;- grid.arrange(chem_pca_plt,tox_pca_plt, nrow=1) Let’s also view the PCA plots for both datasets together, top-to-bottom, to visualize the trends along both axes better between these two views. pca_compare &lt;- grid.arrange(chem_pca_plt,tox_pca_plt) We can also visual representation of some of the major grouping similarities vs differences. Here is an edited version of the above figures, highlighting with colored circles some chemical groups of interest identified through chemistry vs toxicity-based sufficient similarity analyses: These plots can help us answer Environmental Health Question 7: Were similar chemical groups identified when looking at just the chemistry vs. just the toxicity? How could this impact regulatory action, if we only had one of these datasets? Answer: There are some similarities between groupings, though there are also notable differences. For example, samples GbE_A, GbE_B, GbE_C, GbE_F, and GbE_H group together from the chemistry and toxicity similarity analyses. Though samples GbE_G, GbE_W, GbE_N, and others clearly demonstrate differences in grouping assignments. These differences could impact the accuracy of how regulatory decisions are made, where if regulation was dictated solely on the chemistry (without toxicity data) and/or vice versa, we may miss important information that could aid in accurate health risk evaluations. Concluding Remarks In conclusion, we evaluated the similarity between variable lots of Ginkgo biloba, and identified sample groupings that could be used for chemical risk assessment purposes. Together, this example highlights the utility of sufficient similarity analyses to address environmental health research questions. For more information and additional examples in environmental health research, see the following relevant publications implementing sufficient similarity methods to address complex mixtures: Catlin NR, Collins BJ, Auerbach SS, Ferguson SS, Harnly JM, Gennings C, Waidyanatha S, Rice GE, Smith-Roe SL, Witt KL, Rider CV. How similar is similar enough? A sufficient similarity case study with Ginkgo biloba extract. Food Chem Toxicol. 2018 Aug;118:328-339. PMID: 29752982. Collins BJ, Kerns SP, Aillon K, Mueller G, Rider CV, DeRose EF, London RE, Harnly JM, Waidyanatha S. Comparison of phytochemical composition of Ginkgo biloba extracts using a combination of non-targeted and targeted analytical approaches. Anal Bioanal Chem. 2020 Oct;412(25):6789-6809. PMID: 32865633. Ryan KR, Huang MC, Ferguson SS, Waidyanatha S, Ramaiahgari S, Rice JR, Dunlap PE, Auerbach SS, Mutlu E, Cristy T, Peirfelice J, DeVito MJ, Smith-Roe SL, Rider CV. Evaluating Sufficient Similarity of Botanical Dietary Supplements: Combining Chemical and In Vitro Biological Data. Toxicol Sci. 2019 Dec 1;172(2):316-329. PMID: 31504990. Rice GE, Teuschler LK, Bull RJ, Simmons JE, Feder PI. Evaluating the similarity of complex drinking-water disinfection by-product mixtures: overview of the issues. J Toxicol Environ Health A. 2009;72(7):429-36. PMID: 19267305. "],["omics-analyses-and-systems-biology.html", "2.4 -Omics Analyses and Systems Biology Introduction to Training Module Transcriptomics Data QA/QC Statistical Analysis of Gene Expression MA Plots Volcano Plots Pathway Enrichment Analysis Concluding Remarks", " 2.4 -Omics Analyses and Systems Biology This training module was developed by Lauren E. Koval, Dr. Kyle R. Roell, and Dr. Julia E. Rager Fall 2021 The Field of “-Omics” The field of “-omics” has rapidly evolved since its inception in the mid-1990’s, initiated from information obtained through sequencing of the human genome (see the Human Genome Project) as well as the advent of high-content technologies. High-content technologies have allowed the rapid and economical assessment of genome-wide, or ‘omics’-based, endpoints. Traditional molecular biology techniques typically evaluate the function(s) of individual genes and gene products. Omics-based methods, on the other hand, utilize non-targeted methods to identify many to all genes or gene products in a given environmental/biological sample. These non-targeted approaches allow for the unbiased investigation of potentially unknown or understudied molecular mediators involved in regulating cell health and disease. These molecular profiles have the potential of being altered in response to toxicant exposures and/or during disease initiation/progression. To further understand the molecular consequences of -omics-based alterations, molecules can be overlaid onto molecular networks to uncover biological pathways and molecular functions that are perturbed at the systems biology level. An overview of these general methods, starting with high-content technologies and ending of systems biology, is provided in the below figure (created with BioRender.com). A helpful introduction to the field of -omics in relation to environmental health, as well as methods used to relate -omic-level alterations to systems biology, is provided in the following book chapter: Rager JE, Fry RC. Systems Biology and Environmental Exposures. Chpt 4 of ‘Network Biology’ edited by WenJun Zhang. 2013. ISBN: 978-1-62618-941-3. Nova Science Publishers, Inc. Available at: https://www.novapublishers.com/wp-content/uploads/2019/07/978-1-62618-942-3_ch4.pdf. An additional helpful resource describing computational methods that can be used in systems level analyses is the following book chapter: Meisner M, Reif DM. Computational Methods Used in Systems Biology. Chpt 5 of ‘Systems Biology in Toxicology and Environmental Health’ edited by Fry RC. 2015: 85-115. ISBN 9780128015643. Academic Press. Available at: https://www.sciencedirect.com/science/article/pii/B9780128015643000055. Parallel to human genomics/epigenomics-based research is the newer “-omics” topic of the exposome. The exposome was originally conceptualized as ’all life-course environmental exposures (including lifestyle factors), from the prenatal period onwards (Wild et al. 2005). Since then, this concept has received much attention and additional associated definitions. We like to think of the exposome as including anything in ones environment that may impact the overall health of an individual, excluding the individual’s genome/epigenome. Common elements evaluated as part of the exposome include environmental exposures, such as chemicals and other substances that may impart toxicity. Additional potential stressors include lifestyle factors, socioeconomic factors, infectious agents, therapeutics, and other stressors that may be altered internally (e.g., microbiome). A helpful review of this research field is provided as the following publication: Wild CP. The exposome: from concept to utility. Int J Epidemiol. 2012 Feb;41(1):24-32. doi: 10.1093/ije/dyr236. Epub 2012 Jan 31. PMID: 22296988. Transcriptomics One of the most widely evaluated -omics endpoints is messenger RNA (mRNA) expression (also termed gene expression). As a reminder, mRNA molecules are a major type of RNA produced as the “middle step” in the Central Dogma Theory, which describes how genetic DNA is first transcribed into RNA and then translated into protein. Protein molecules are ultimately the major regulators of cellular processes and overall health. Therefore, any perturbations to this process (including changes to mRNA expression levels) can have tremendous consequences on overall cell function and health. A visualization of these steps in the Central Dogma theory are included below. mRNA expression can be evaluated in a high-throughout/high-content manner, across the genome, and is referred to as the transcriptome when doing so. Transcriptomics can be measured using a variety of technologies, including high-density nucleic acid arrays (e.g., DNA microarrays or GeneChip arrays), high-throughput PCR technologies, or RNA sequencing technologies. These methods are used to obtain relative measures of genes that are being expressed or transcribed from DNA by measuring the abundance of mRNA molecules. Results of these methods are often termed as providing gene expression signatures or ‘transcriptomes’ of a sample under evaluation. Introduction to Training Module This training module incorporates the highly relevant example of RNA sequencing to evaluate the impacts of environmental exposures on cellular responses and general human health. RNA sequencing is the most common method that is currently implemented to measure the transcriptome. Results from an RNA sequencing platform are often summarized as count data, representing the number of relative times a gene (or other annotated portion of the genome) was ‘read’ in a given sample. For more details surrounding the methodological underpinnings of RNA sequencing, see the following recent review: Stark R, Grzelak M, Hadfield J. RNA sequencing: the teenage years. Nat Rev Genet. 2019 Nov;20(11):631-656. doi: 10.1038/s41576-019-0150-2. Epub 2019 Jul 24. PMID: 31341269. In this training module, we guide participants through an example RNA sequencing analysis. Here, we analyze RNA sequencing data collected in a toxicology study evaluating the effects of biomass smoke exposure, representing wildfire-relevant exposure conditions. This study has been previously been described in the following publications: Rager JE, Clark J, Eaves LA, Avula V, Niehoff NM, Kim YH, Jaspers I, Gilmour MI. Mixtures modeling identifies chemical inducers versus repressors of toxicity associated with wildfire smoke. Sci Total Environ. 2021 Jun 25;775:145759. doi: 10.1016/j.scitotenv.2021.145759. Epub 2021 Feb 10. PMID: 33611182. Kim YH, Warren SH, Krantz QT, King C, Jaskot R, Preston WT, George BJ, Hays MD, Landis MS, Higuchi M, DeMarini DM, Gilmour MI. Mutagenicity and Lung Toxicity of Smoldering vs. Flaming Emissions from Various Biomass Fuels: Implications for Health Effects from Wildland Fires. Environ Health Perspect. 2018 Jan 24;126(1):017011. doi: 10.1289/EHP2200. PMID: 29373863. Here, we specifically analyze mRNA sequencing profiles collected in mouse lung tissues. These mice were exposed to two different biomass burn scenarios: smoldering pine needles and flaming pine needles, representing certain wildfire smoke exposure scenarios that can occur. The goal of these analyses is to identify which genes demonstrate altered expression in response to these wildfire-relevant exposures, and identify which biological pathways these genes influence to evaluate findings at the systems biology level. This training module begins by guiding users through the loading, viewing, and formatting of the example transcriptomics datasets and associated metadata. Methods to carry out quality assurance (QA) / quality control (QC) of the transcriptomics data are then described, which are advantageous to ensure high quality data are included in the final statistical analysis. Because these transcriptomic data were derived from bulk lung tissue samples, consisting of mixed cell populations that could have shifted in response to exposures, data are then adjusted for potential sources of heterogeneity using the R package RUVseq. Statistical models are then implemented to identify genes that were significantly differentially expressed between exposed vs unexposed samples. Models are implemented using algorithms within the commonly implemented R package DESeq2. This package is very convenient, well written, and widely used. The main advantage of this package is that it allows you to perform differential expression analyses and easily obtain various statistics and results with minimal script development on the user-end. After obtaining results from differential gene expression analyses, we visualize these results using both MA and volcano plots. Finally, we carry out a systems level analysis through pathway enrichment using the R package PIANO to identify which biological pathways were altered in response to these wildfire-relevant exposure scenarios. Training Module’s Environmental Health Questions This training module was specifically developed to answer the following environmental health questions: What two types of data are commonly needed in the analysis of transcriptomics data? When preparing transcriptomics data for statistical analyses, what are three common data filtering steps that are completed during the data QA/QC process? When identifying potential sample outliers in a typical transcriptomics dataset, what two types of approaches are commonly employed to identify samples with outlying data distributions? What is an approach that analysts can use when evaluating transcriptomic data from tissues of mixed cellular composition to aid in controlling for sources of sample heterogeneity? How many genes showed significant differential expression associated with flaming pine needles exposure in the mouse lung, based on a statistical filter of a multiple test corrected p-value (padj) &lt; 0.05? How many genes showed significant differential expression associated with smoldering pine needles exposure in the mouse lung, based on a statistical filter of a multiple test corrected p-value (padj) &lt; 0.05? How many genes showed significant differential expression associated with lipopolysaccharide (LPS) exposure in the mouse lung, based on a statistical filter of a multiple test corrected p-value (padj) &lt; 0.05? What biological pathways are disrupted in association with flaming pine needles exposure in the mouse lung, identified through systems level analyses? Script Preparations Cleaning the global environment rm(list=ls()) Installing required R packages If you already have these packages installed, you can skip this step, or you can run the below code which checks installation status for you if (!requireNamespace(&quot;tidyverse&quot;)) install.packages(&quot;tidyverse&quot;); if (!requireNamespace(&quot;BiocManager&quot;)) BiocManager::install(&quot;BiocManager&quot;); if (!requireNamespace(&quot;DESeq2&quot;)) BiocManager::install(&quot;DESeq2&quot;); if (!requireNamespace(&quot;edgeR&quot;)) BiocManager::install(&quot;edgeR&quot;); if (!requireNamespace(&quot;RUVSeq&quot;)) BiocManager::install(&quot;RUVSeq&quot;); if (!requireNamespace(&quot;janitor&quot;)) install.packages(&quot;janitor&quot;); if (!requireNamespace(&quot;pheatmap&quot;)) install.packages(&quot;pheatmap&quot;); if (!requireNamespace(&quot;factoextra&quot;)) install.packages(&quot;factoextra&quot;); if (!requireNamespace(&quot;RColorBrewer&quot;)) install.packages(&quot;RColorBrewer&quot;); if (!requireNamespace(&quot;data.table&quot;)) install.packages(&quot;data.table&quot;); if (!requireNamespace(&quot;EnhancedVolcano&quot;)) BiocManager::install(&quot;EnhancedVolcano&quot;); if (!requireNamespace(&quot;piano&quot;)) BiocManager::install(&quot;piano&quot;); Loading R packages required for this session library(tidyverse) library(DESeq2) library(edgeR) library(RUVSeq) library(janitor) library(factoextra) library(pheatmap) library(data.table) library(RColorBrewer) library(EnhancedVolcano) library(piano) Set your working directory setwd(&quot;/filepath to where your input files are&quot;) Loading the Example Transcriptomic Dataset and Metadata First, let’s read in the transcriptional signature data, previously summarized as number of sequence reads per gene (also simply referred to as ‘count data’) and its associated metadata file: # Read in the count data countdata &lt;- read.csv(file = &#39;Module2_4/Module2_4_GeneCounts.csv&#39;, check.names = FALSE) # Read in the metadata (describing information on each sample) sampleinfo &lt;- read.csv(file = &quot;Module2_4/Module2_4_SampleInfo.csv&quot;, check.names = FALSE) Data Viewing Let’s see how many rows and columns of data are present in the countdata dataframe: dim(countdata) ## [1] 30146 23 Let’s also view the column headers: colnames(countdata) ## [1] &quot;Gene&quot; &quot;Plate1-m13-RNA&quot; &quot;Plate2-m14-RNA&quot; &quot;Plate2-m15-RNA&quot; ## [5] &quot;Plate2-m16-RNA&quot; &quot;Plate2-m17-RNA&quot; &quot;Plate2-m18-RNA&quot; &quot;Plate1-m49-RNA&quot; ## [9] &quot;Plate1-m50-RNA&quot; &quot;Plate1-m51-RNA&quot; &quot;Plate1-m52-RNA&quot; &quot;Plate1-m53-RNA&quot; ## [13] &quot;Plate1-m54-RNA&quot; &quot;Plate1-m31-RNA&quot; &quot;Plate1-m32-RNA&quot; &quot;Plate1-m33-RNA&quot; ## [17] &quot;Plate1-m34-RNA&quot; &quot;Plate1-m35-RNA&quot; &quot;Plate1-m36-RNA&quot; &quot;Plate1-m67-RNA&quot; ## [21] &quot;Plate1-m68-RNA&quot; &quot;Plate1-m69-RNA&quot; &quot;Plate1-m70-RNA&quot; And finally let’s view the top few rows of data: head(countdata) ## Gene Plate1-m13-RNA Plate2-m14-RNA Plate2-m15-RNA Plate2-m16-RNA ## 1 Bcat1_29039 0 0 0 0 ## 2 Laptm5_29040 0 2 0 1 ## 3 Skp2_29041 0 0 0 0 ## 4 Nme1_29042 765 1041 1028 856 ## 5 Ctsd_29043 25 30 38 24 ## 6 Camk1_29044 443 445 433 628 ## Plate2-m17-RNA Plate2-m18-RNA Plate1-m49-RNA Plate1-m50-RNA Plate1-m51-RNA ## 1 0 0 0 0 0 ## 2 2 1 0 1 2 ## 3 0 0 0 0 0 ## 4 888 967 816 569 854 ## 5 17 19 38 38 34 ## 6 497 684 574 71 356 ## Plate1-m52-RNA Plate1-m53-RNA Plate1-m54-RNA Plate1-m31-RNA Plate1-m32-RNA ## 1 0 2 0 0 3 ## 2 2 7 1 2 1 ## 3 0 0 0 0 0 ## 4 736 887 930 563 776 ## 5 30 24 23 29 21 ## 6 403 179 302 172 487 ## Plate1-m33-RNA Plate1-m34-RNA Plate1-m35-RNA Plate1-m36-RNA Plate1-m67-RNA ## 1 0 0 0 0 1 ## 2 5 0 0 0 1 ## 3 0 0 0 0 0 ## 4 604 1008 724 632 950 ## 5 27 26 18 22 33 ## 6 487 429 599 683 147 ## Plate1-m68-RNA Plate1-m69-RNA Plate1-m70-RNA ## 1 0 0 0 ## 2 3 2 2 ## 3 0 0 0 ## 4 782 879 860 ## 5 19 12 14 ## 6 265 335 454 Together, this dataframe contains information across 30146 mRNA identifiers, that are labeled according to “Gene name” followed by an underscore and probe number assigned by the platform used in this analysis, BioSpyder TempoSeq Technologies. A total of 23 columns are included in this dataframe, the first of which represents the gene identifier, followed by gene count data across 22 samples. Let’s also see what the metadata dataframe looks like: dim(sampleinfo) ## [1] 22 9 Let’s also view the column headers: colnames(sampleinfo) ## [1] &quot;SampleID_BioSpyderCountFile&quot; &quot;PlateBatch&quot; ## [3] &quot;MouseID&quot; &quot;NumericID&quot; ## [5] &quot;Treatment&quot; &quot;ID&quot; ## [7] &quot;Timepoint&quot; &quot;Tissue&quot; ## [9] &quot;Group&quot; And finally let’s view the top few rows of data: head(sampleinfo) ## SampleID_BioSpyderCountFile PlateBatch MouseID NumericID Treatment ## 1 Plate1-m13-RNA Plate1 M13 13 PineNeedlesSmolder ## 2 Plate2-m14-RNA Plate2 M14 14 PineNeedlesSmolder ## 3 Plate2-m15-RNA Plate2 M15 15 PineNeedlesSmolder ## 4 Plate2-m16-RNA Plate2 M16 16 PineNeedlesSmolder ## 5 Plate2-m17-RNA Plate2 M17 17 PineNeedlesSmolder ## 6 Plate2-m18-RNA Plate2 M18 18 PineNeedlesSmolder ## ID Timepoint Tissue Group ## 1 M13_PineNeedlesSmolder 4h Lung PineNeedlesSmolder_4h_Lung ## 2 M14_PineNeedlesSmolder 4h Lung PineNeedlesSmolder_4h_Lung ## 3 M15_PineNeedlesSmolder 4h Lung PineNeedlesSmolder_4h_Lung ## 4 M16_PineNeedlesSmolder 4h Lung PineNeedlesSmolder_4h_Lung ## 5 M17_PineNeedlesSmolder 4h Lung PineNeedlesSmolder_4h_Lung ## 6 M18_PineNeedlesSmolder 4h Lung PineNeedlesSmolder_4h_Lung Together, this dataframe contains information across the 22 total samples, that are labeled according to “SampleID_BioSpyderCountFile” header. These identifiers match those used as column headers in the countdata dataframe. A total of 9 columns are included in this dataframe, including the following: ‘SampleID_BioSpyderCountFile’: The unique sample identifiers (total n=22) ‘PlateBatch’: The plate number that was used in the generation of these data. ‘MouseID’: The unique identifier, that starts with “M” followed by a number, for each mouse used in this study ‘NumericID’: The unique numeric identifier for each mouse. ‘Treatment’: The type of exposure condition that each mouse was administered. These include smoldering pine needles, flaming pine needles, vehicle control (saline), and positive inflammation control (LPS, or lipopolysaccharide) ‘ID’: Another form of identifier that combines the mouse identifier with the exposure condition ‘Timepoint’: The timepoint at which samples were collected (here, all 4h post-exposure) ‘Tissue’: The type of tissue that was collected and analyzed (here, all lung tissue) ‘Group’: The higher level identifier that groups samples together based on exposure condition, timepoint, and tissue One common QC/preparation step that is helpful when organizing transcriptomics data is to check for potential duplicate mRNA IDs in the countdata. # Visualize this data quickly by viewing top left corner, # to check where ID column is located: countdata[1:3,1:5] ## Gene Plate1-m13-RNA Plate2-m14-RNA Plate2-m15-RNA Plate2-m16-RNA ## 1 Bcat1_29039 0 0 0 0 ## 2 Laptm5_29040 0 2 0 1 ## 3 Skp2_29041 0 0 0 0 # Then check for duplicates within column 1 (where the ID column is located): Dups &lt;- duplicated(countdata[,1]) summary(Dups) ## Mode FALSE ## logical 30146 In this case, because all potential duplicate checks turn up “FALSE”, these data do not contain duplicate mRNA identifiers in its current organized format. With this, we can answer Environmental Health Question 1: What two types of data are commonly needed in the analysis of transcriptomics data? Answer: A file containing the raw -omics signatures is needed (in this case, the count data summarized per gene acquired from RNA sequencing technologies), and a file containing the associated metadata describing the actual samples, where they were derived from, what they represent, etc, is needed. Formatting Data for Downstream Statistics Most of the statistical analyses included in this training module will be carried out using the DESeq2 pipeline. This package requires that the count data and sample information data be formatted in a certain manner, which will expedite the downstream coding needed to carry out the statistics. Here, we will walk users through these initial formatting steps. DESeq2 first requires a ‘coldata’ dataframe, which includes the sample information (i.e., metadata). Let’s create this new dataframe based on the original ‘sampleinfo’ dataframe: coldata &lt;- sampleinfo DESeq2 also requires a ‘countdata’ dataframe, which we’ve previously created; however, this dataframe requires some minor formatting before it can be used as input for downstream script. First, the gene identifiers need to be converted into row names: countdata &lt;- countdata %&gt;% column_to_rownames(&quot;Gene&quot;) Then, the column names need to be edited. Let’s remind ourselves what the column names are currently: colnames(countdata) ## [1] &quot;Plate1-m13-RNA&quot; &quot;Plate2-m14-RNA&quot; &quot;Plate2-m15-RNA&quot; &quot;Plate2-m16-RNA&quot; ## [5] &quot;Plate2-m17-RNA&quot; &quot;Plate2-m18-RNA&quot; &quot;Plate1-m49-RNA&quot; &quot;Plate1-m50-RNA&quot; ## [9] &quot;Plate1-m51-RNA&quot; &quot;Plate1-m52-RNA&quot; &quot;Plate1-m53-RNA&quot; &quot;Plate1-m54-RNA&quot; ## [13] &quot;Plate1-m31-RNA&quot; &quot;Plate1-m32-RNA&quot; &quot;Plate1-m33-RNA&quot; &quot;Plate1-m34-RNA&quot; ## [17] &quot;Plate1-m35-RNA&quot; &quot;Plate1-m36-RNA&quot; &quot;Plate1-m67-RNA&quot; &quot;Plate1-m68-RNA&quot; ## [21] &quot;Plate1-m69-RNA&quot; &quot;Plate1-m70-RNA&quot; These column identifiers need to be converted into more intuitive sample IDs, that also indicate treatment. This information can be found in the coldata dataframe. Specifically, information in the column labeled ‘SampleID_BioSpyderCountFile’ will be helpful for these purposes. To replace these original column identifiers with these more helpful sample identifiers, let’s first make sure the order of the countdata columns are in the same order as the coldata column of ‘SampleID_BioSpyderCountFile’: countdata &lt;- setcolorder(countdata, as.character(coldata$SampleID_BioSpyderCountFile)) Now, we can rename the column names within the countdata dataframe with these more helpful identifiers, since both dataframes are now arranged in the same order: # Rename the countdata column names with the treatment IDs. colnames(countdata) &lt;- coldata$ID # Viewing these new column names colnames(countdata) ## [1] &quot;M13_PineNeedlesSmolder&quot; &quot;M14_PineNeedlesSmolder&quot; &quot;M15_PineNeedlesSmolder&quot; ## [4] &quot;M16_PineNeedlesSmolder&quot; &quot;M17_PineNeedlesSmolder&quot; &quot;M18_PineNeedlesSmolder&quot; ## [7] &quot;M49_PineNeedlesFlame&quot; &quot;M50_PineNeedlesFlame&quot; &quot;M51_PineNeedlesFlame&quot; ## [10] &quot;M52_PineNeedlesFlame&quot; &quot;M53_PineNeedlesFlame&quot; &quot;M54_PineNeedlesFlame&quot; ## [13] &quot;M31_Saline&quot; &quot;M32_Saline&quot; &quot;M33_Saline&quot; ## [16] &quot;M34_Saline&quot; &quot;M35_Saline&quot; &quot;M36_Saline&quot; ## [19] &quot;M67_LPS&quot; &quot;M68_LPS&quot; &quot;M69_LPS&quot; ## [22] &quot;M70_LPS&quot; These new column identifiers look much better, and can better inform downstream statistical analysis script. Remember that these identifiers indicate that these are mouse samples (“M”), with unique numbers, followed by an underscore and the exposure condition. When relabeling dataframes, it’s always important to triple check any of these major edits. For example, here, let’s double check that the same samples appear in the same order between the two working dataframes required for dowstream DESeq2 code: setequal(as.character(coldata$ID), colnames(countdata)) ## [1] TRUE identical(as.character(coldata$ID), colnames(countdata)) ## [1] TRUE Transcriptomics Data QA/QC After preparing your transcriptomic data and sample information dataframes for statistical analyses, it is very important to carry out QA/QC on your organized datasets, prior to including all samples and all genes in the actual statistical model. It is critical to only include high quality data that inform underlying biology of exposure responses/disease etiology, rather than data that may contribute noise to the overall data distributions. Some common QA/QC steps and associated data pre-filters carried out in transcriptomics analyses are detailed below. Background Filter It is very common to perform a background filter step when preparing transcriptomic data for statistical analyses. The goal of this step is to remove genes that are very lowly expressed across the majority of samples, and thus are referred to as universally lowly expressed. Signals from these genes can mute the overall signals that may be identified in -omics analyses. The specific threshold that you may want to apply as the background filter to your dataset will depend on the distribution of your dataset and analysis goal(s). For this example, we apply a background threshold, to remove genes that are lowly expressed across the majority of samples, specifically defined as genes that have expression levels across at least 20% of the samples that are less than (or equal to) the median expression of all genes across all samples. This will result in including only genes that are expressed above background, that have expression levels in at least 20% of samples that are greater than the overall median expression. Script to apply this filter is detailed below: # First count the total number of samples, and save it as a value in the global environment nsamp &lt;- ncol(countdata) # Then, calculate the median expression level across all genes and all samples, # and save it as a value total_median &lt;- median(as.matrix(countdata)) # We need to temporarily add back in the Gene column to the countdata # so we can filter for genes that pass the background filter countdata &lt;- countdata %&gt;% rownames_to_column(&quot;Gene&quot;) # Then we can apply a set of filters and organization steps (using the tidyverse) # to result in a list of genes that have an expression greater than the total median # in at least 20% of the samples genes_above_background &lt;- countdata %&gt;% # Start from the &#39;countdata&#39; dataframe pivot_longer(cols=!Gene, names_to = &quot;sampleID&quot;, values_to=&quot;expression&quot;) %&gt;% # Melt the data so that we have three columns: gene, exposure condition, and expression counts mutate(above_median=ifelse(expression&gt;total_median,1,0)) %&gt;% # Add a column that indicates whether the expression of a gene for the corresponding exposure condition is above (1) or not above (0) the median of all count data group_by(Gene) %&gt;% # Group the dataframe by the gene summarize(total_above_median=sum(above_median)) %&gt;% # For each gene, count the number of exposure conditions where the expression was greater than the median of all count data filter(total_above_median&gt;=.2*nsamp) %&gt;% # Filter for genes that have expression above the median in at least 20% of the samples select(Gene) # Select just the genes that pass the filter # Then filter the original &#39;countdata&#39; dataframe for only the genes above background. countdata &lt;- left_join(genes_above_background, countdata, by=&quot;Gene&quot;) Here, the ‘countdata’ dataframe went from having 30,146 rows of data (representing genes) to 16,664 rows of data (representing genes with expression levels that passed this background filter) Sample Filtering Another common QA/QC check is to evaluate whether there are any samples that did not produce adequate RNA material to be measured using the technology employed. Thus, a sample filter can be applied to remove samples that have inadequate data. Here, we demonstrate this filter by checking to see whether there were any samples that resulted in mRNA expression values of zero across all genes. If any sample demonstrates this issue, it should be removed prior to any statistical analysis. Note, there are other filter cut-offs you can use depending on your specific study. Below is example script that checks for the presence of samples that meet the above criteria: # Transpose filtered &#39;countdata&#39;, while keeping data in dataframe format, # to allow for script that easily sums the total expression levels per sample countdata_T &lt;- countdata %&gt;% pivot_longer(cols=!Gene, names_to=&quot;sampleID&quot;,values_to=&quot;expression&quot;) %&gt;% pivot_wider(names_from=Gene, values_from=expression) # Then add in a column to the transposed countdata dataframe that sums expression # across all genes for each exposure condition countdata_T$rowsum &lt;- rowSums(countdata_T[2:ncol(countdata_T)]) # Remove samples that have no expression. All samples have some expression # in this example, so all samples are retained. countdata_T &lt;- countdata_T %&gt;% filter(rowsum!=0) # Take the count data filtered for correct samples, remove the &#39;rowsums&#39; column countdata_T &lt;- countdata_T %&gt;% select(!rowsum) # Then, transpose it back to the correct format for analysis countdata &lt;- countdata_T %&gt;% pivot_longer(cols=!sampleID, names_to = &quot;Gene&quot;,values_to=&quot;expression&quot;) %&gt;% pivot_wider(names_from = sampleID, values_from = &quot;expression&quot;) Identifying &amp; Removing Sample Outliers Prior to final statistical analysis, raw transcriptomic data are commonly evaluated for the presence of potential sample outliers. Outliers can result from experimental error, technical error/measurement error, and/or huge sources of variation in biology. For many analyses, it is beneficial to remove such outliers to enhance computational abilities to identify biologically meaningful signals across data. Here, we present two methods to check for the presence of sample outliers: 1. Principal component analysis (PCA) can be used to identify potential outliers in a dataset through visualization of summary-level values illustrating reduced representations of the entire dataset. Note that a more detailed description of PCA is provided in Training Module 2.2. Here, PCA is run on the raw count data and further analyzed using scree plots, assessing principal components (PCs), and visualized using biplots displaying the first two principal components as a scatter plot. 2. Hierarchical clustering is another approach that can be used to identify potential outliers. Hierarchical clustering aims to cluster data based on a similarity measure, defined by the function and/or specified by the user. There are several R packages and functions that will run hierarchical clustering, but it is often helpful visually to do this in conjuction with a heat map. Here, we use the package pheatmap (introduced in Training Module 1.4) with hierarchical clustering across samples to identify potential outliers. Let’s start by using PCA to identify potential outliers, while providing a visualization of potential sources of variation across the dataset. First we need to move the Gene column back to the rownames so our dataframe is numeric and we can run the PCA script: countdata &lt;- countdata %&gt;% column_to_rownames(&quot;Gene&quot;) # Let&#39;s remind ourselves what these data look like countdata[1:10,1:5] #viewing first 10 rows and 5 columns ## M13_PineNeedlesSmolder M14_PineNeedlesSmolder ## 0610009B22Rik_56046 123 118 ## 0610010F05Rik_69119 1378 1511 ## 0610010F05Rik_74637 83 55 ## 0610010K14Rik_31619 462 580 ## 0610010K14Rik_31873 560 814 ## 0610010K14Rik_68949 207 182 ## 0610012G03Rik_58446 243 299 ## 0610030E20Rik_65200 152 105 ## 0610040J01Rik_55628 125 106 ## 1110004F10Rik_77756 324 377 ## M15_PineNeedlesSmolder M16_PineNeedlesSmolder ## 0610009B22Rik_56046 110 167 ## 0610010F05Rik_69119 1534 936 ## 0610010F05Rik_74637 57 13 ## 0610010K14Rik_31619 625 560 ## 0610010K14Rik_31873 680 799 ## 0610010K14Rik_68949 104 202 ## 0610012G03Rik_58446 294 327 ## 0610030E20Rik_65200 92 87 ## 0610040J01Rik_55628 127 107 ## 1110004F10Rik_77756 356 233 ## M17_PineNeedlesSmolder ## 0610009B22Rik_56046 147 ## 0610010F05Rik_69119 1301 ## 0610010F05Rik_74637 54 ## 0610010K14Rik_31619 604 ## 0610010K14Rik_31873 707 ## 0610010K14Rik_68949 166 ## 0610012G03Rik_58446 262 ## 0610030E20Rik_65200 150 ## 0610040J01Rik_55628 93 ## 1110004F10Rik_77756 276 Then we can calculate principal components using transposed count data: pca &lt;- prcomp(t(countdata)) We can visualize the percent variation captured by each principal component (PC) with a scree plot: # We can generate a scree plot that shows the eigenvalues of each component, # indicating how much of the total variation is captured by each component fviz_eig(pca) This scree plot indicates that nearly all variation is explained in PC1 and PC2, so we are comfortable with viewing these first two PCs when evaluating whether or not potential outliers exist in this dataset. Further visualization of how these transcriptomic data appear through PCA can be produced through a scatter plot showing the data reduced values per sample: # Calculate the percent variation captured by each PC pca_percent &lt;- round(100*pca$sdev^2/sum(pca$sdev^2),1) # Make dataframe for PCA plot generation using first two components and the sample name pca_df &lt;- data.frame(PC1 = pca$x[,1], PC2 = pca$x[,2], Sample=colnames(countdata)) # Organize dataframe so we can color our points by the exposure condition pca_df &lt;- pca_df %&gt;% separate(Sample, into = c(&quot;mouse_num&quot;, &quot;expo_cond&quot;), sep=&quot;_&quot;) # Plot PC1 and PC2 for each sample and color the point by the exposure condition ggplot(pca_df, aes(PC1,PC2, color = expo_cond))+ geom_hline(yintercept = 0, size=0.3)+ geom_vline(xintercept = 0, size=0.3)+ geom_point(size=3) + geom_text(aes(label=mouse_num), vjust =-1, size=4)+ labs(x=paste0(&quot;PC1 (&quot;,pca_percent[1],&quot;%)&quot;), y=paste0(&quot;PC2 (&quot;,pca_percent[2],&quot;%)&quot;))+ ggtitle(&quot;PCA for 4h Lung Pine Needles &amp; Control Exposure Conditions&quot;) With this plot, we can see that samples do not demonstrate obvious groupings, where certain samples group far apart from others. Therefore, our PCA analysis indicates that there are unlikely any sample outliers in this dataset. Hierarchical clustering Now lets implement hierarchical clustering to identify potential outliers. First we need to create a dataframe of our transposed ‘countdata’ such that samples are rows and genes are columns to input into the clustering algorithm. countdata_for_clustering &lt;- t(countdata) # Viewing what this transposed dataframe looks like countdata_for_clustering[1:5,1:10] ## 0610009B22Rik_56046 0610010F05Rik_69119 ## M13_PineNeedlesSmolder 123 1378 ## M14_PineNeedlesSmolder 118 1511 ## M15_PineNeedlesSmolder 110 1534 ## M16_PineNeedlesSmolder 167 936 ## M17_PineNeedlesSmolder 147 1301 ## 0610010F05Rik_74637 0610010K14Rik_31619 ## M13_PineNeedlesSmolder 83 462 ## M14_PineNeedlesSmolder 55 580 ## M15_PineNeedlesSmolder 57 625 ## M16_PineNeedlesSmolder 13 560 ## M17_PineNeedlesSmolder 54 604 ## 0610010K14Rik_31873 0610010K14Rik_68949 ## M13_PineNeedlesSmolder 560 207 ## M14_PineNeedlesSmolder 814 182 ## M15_PineNeedlesSmolder 680 104 ## M16_PineNeedlesSmolder 799 202 ## M17_PineNeedlesSmolder 707 166 ## 0610012G03Rik_58446 0610030E20Rik_65200 ## M13_PineNeedlesSmolder 243 152 ## M14_PineNeedlesSmolder 299 105 ## M15_PineNeedlesSmolder 294 92 ## M16_PineNeedlesSmolder 327 87 ## M17_PineNeedlesSmolder 262 150 ## 0610040J01Rik_55628 1110004F10Rik_77756 ## M13_PineNeedlesSmolder 125 324 ## M14_PineNeedlesSmolder 106 377 ## M15_PineNeedlesSmolder 127 356 ## M16_PineNeedlesSmolder 107 233 ## M17_PineNeedlesSmolder 93 276 Next we can run hierarchical clustering in conjunction with the generation of a heat map. Note that we scale these data for improved visualization. pheatmap(scale(countdata_for_clustering), main=&quot;Hierarchical Clustering&quot;, cluster_rows=TRUE, cluster_cols = FALSE, fontsize_col = 7, treeheight_row = 60, show_colnames = FALSE) Like the PCA findings, hierarchical clustering demonstrated an overall lack of potential sample outliers because there were no obvious sample(s) that grouped separately from the rest along the clustering dendrograms. Therefore, neither approach points to outliers that should be removed in this analysis. With this, we can answer Environmental Health Question 2: When preparing transcriptomics data for statistical analyses, what are three common data filtering steps that are completed during the data QA/QC process? Answer: (1) Background filter to remove genes that are universally lowly expressed; (2) Sample filter to remove samples that may be not have any detectable mRNA; (3) Sample outlier filter to remove samples with underlying data distributions outside of the overall, collective dataset. We can also answer Environmental Health Question 3: When identifying potential sample outliers in a typical transcriptomics dataset, what two types of approaches are commonly employed to identify samples with outlying data distributions? Answer: Principal component analysis (PCA) and hierarchical clustering. Controlling for Sources of Sample Heterogeneity Because these transcriptomic data were generated from mouse lung tissues, there is potential for these samples to show heterogeneity based on underlying shifts in cell populations (e.g., neutrophil influx) or other aspects of sample heterogeneity (e.g., batch effects from plating, among other sources of heterogeneity that we may want to control for). For these kinds of complex samples, there are data processing methods that can be leveraged to minimize the influence of these sources of heterogeneity. Example methods include Remove Unwanted Variable (RUV), which is discussed here, as well as others (e.g., Surrogate Variable Analysis (SVA)). Here, we leverage the package called RUVseq to employ RUV on this sequencing dataset. Script was developed based off Bioconductor website, vignette, and original publication. Steps in carrying out RUV using RUVseq on this example dataset: # First we store the treatment IDs and exposure conditions as a separate vector ID &lt;- coldata$ID # And differentiate our treatments and control conditions, first by grabbing # the groups associated with each sample groups &lt;- as.factor(coldata$Group) # Let&#39;s view all the groups groups ## [1] PineNeedlesSmolder_4h_Lung PineNeedlesSmolder_4h_Lung ## [3] PineNeedlesSmolder_4h_Lung PineNeedlesSmolder_4h_Lung ## [5] PineNeedlesSmolder_4h_Lung PineNeedlesSmolder_4h_Lung ## [7] PineNeedlesFlame_4h_Lung PineNeedlesFlame_4h_Lung ## [9] PineNeedlesFlame_4h_Lung PineNeedlesFlame_4h_Lung ## [11] PineNeedlesFlame_4h_Lung PineNeedlesFlame_4h_Lung ## [13] Saline_4h_Lung Saline_4h_Lung ## [15] Saline_4h_Lung Saline_4h_Lung ## [17] Saline_4h_Lung Saline_4h_Lung ## [19] LPS_4h_Lung LPS_4h_Lung ## [21] LPS_4h_Lung LPS_4h_Lung ## 4 Levels: LPS_4h_Lung PineNeedlesFlame_4h_Lung ... Saline_4h_Lung # Then setting a control label ctrl &lt;- &quot;Saline_4h_Lung&quot; # And extracting a vector of just our treatment groups trt_groups &lt;- setdiff(groups,ctrl) # Let&#39;s view this vector trt_groups ## [1] &quot;PineNeedlesSmolder_4h_Lung&quot; &quot;PineNeedlesFlame_4h_Lung&quot; ## [3] &quot;LPS_4h_Lung&quot; RUVseq contains its own set of plotting and normalization functions, though requires input of what’s called an object of S4 class SeqExpressionSet. Let’s go ahead and make this object, using the RUVseq function ‘newSeqExpressionSet’: exprSet &lt;- newSeqExpressionSet(as.matrix(countdata),phenoData = data.frame(groups,row.names=colnames(countdata))) And then use this object to generate some exploratory plots using built-in tools within RUVseq. First starting with some boxplots summarizing overall data distributions per sample: colors &lt;- brewer.pal(4, &quot;Set2&quot;) plotRLE(exprSet, outline=FALSE, ylim=c(-4, 4), col=colors[groups]) We can see from this plot that some of the samples show distributions that may vary from the overall - for instance, one of the flaming pine needles-exposed samples (in orange) is far lower than the rest. Then viewing a PCA plot of these samples: colors &lt;- brewer.pal(4, &quot;Set2&quot;) plotPCA(exprSet, col=colors[groups], cex=1.2) This PCA plot shows pretty good data distributions, with samples mainly showing groupings based upon exposure condition (e.g., LPS), which is to be expected. With this, we can conclude that there may be some sources of unwanted variation, but not a huge amount. Let’s see what the data look like after running RUV. Now to actually run the RUVseq algorithm, to control for potential sources of sample heterogeneity, we need to first construct a matrix specifying the replicates (samples of the same exposure condition): # Construct a matrix specifying the replicates (samples of the same exposure condition) # for running RUV differences &lt;- makeGroups(groups) # Viewing this new matrix head(differences) ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 19 20 21 22 -1 -1 ## [2,] 7 8 9 10 11 12 ## [3,] 1 2 3 4 5 6 ## [4,] 13 14 15 16 17 18 This matrix groups the samples by exposure condition. Here, each of the four rows represents one of the four exposure conditions, and each of the six columns represents a possible sample. Since the LPS exposure condition only had four samples, instead of six like the rest of the exposure conditions, a value of -1 is automatically used as a place holder to fill out the matrix. The samples in the matrix are identified by the index of the sample in the previously defined ‘groups’ factor that was used to generate the matrix. For example, the PineNeedlesSmolder_4h_Lung samples are the the first six samples contained in the ‘groups’ factor, so in the matrix, samples of this exposure condition are identified as ‘1’,‘2’,‘3’,‘4’,‘5’, and ‘6’. Let’s now implement the RUVseq algorithm and, for this example, capture one factor (k=1) of unwanted variation. Note that the k parameter can be modified to capture additional factors, if necessary. # Now capture 1 factor (k=1) of unwanted variation ruv_set &lt;- RUVs(exprSet, rownames(countdata), k=1, differences) This results in a list of objects within ‘ruv_set’, which include the following important pieces of information: Estimated factors of unwanted variation are provided in the phenoData object, as viewed using the following: # viewing the estimated factors of unwanted variation in the column W_1 pData(ruv_set) ## groups W_1 ## M13_PineNeedlesSmolder PineNeedlesSmolder_4h_Lung 1.3777900 ## M14_PineNeedlesSmolder PineNeedlesSmolder_4h_Lung 1.2392662 ## M15_PineNeedlesSmolder PineNeedlesSmolder_4h_Lung 1.0687702 ## M16_PineNeedlesSmolder PineNeedlesSmolder_4h_Lung 1.2448747 ## M17_PineNeedlesSmolder PineNeedlesSmolder_4h_Lung 1.3156003 ## M18_PineNeedlesSmolder PineNeedlesSmolder_4h_Lung 1.4136600 ## M49_PineNeedlesFlame PineNeedlesFlame_4h_Lung 1.3343115 ## M50_PineNeedlesFlame PineNeedlesFlame_4h_Lung 0.4446084 ## M51_PineNeedlesFlame PineNeedlesFlame_4h_Lung 1.0172990 ## M52_PineNeedlesFlame PineNeedlesFlame_4h_Lung 1.0332373 ## M53_PineNeedlesFlame PineNeedlesFlame_4h_Lung 0.7746752 ## M54_PineNeedlesFlame PineNeedlesFlame_4h_Lung 1.0011086 ## M31_Saline Saline_4h_Lung 0.6582117 ## M32_Saline Saline_4h_Lung 1.2518829 ## M33_Saline Saline_4h_Lung 1.1892456 ## M34_Saline Saline_4h_Lung 1.2607852 ## M35_Saline Saline_4h_Lung 1.3141520 ## M36_Saline Saline_4h_Lung 1.4335200 ## M67_LPS LPS_4h_Lung 1.0341947 ## M68_LPS LPS_4h_Lung 1.2823240 ## M69_LPS LPS_4h_Lung 1.4460347 ## M70_LPS LPS_4h_Lung 1.4376930 Normalized counts obtained by regressing the original counts on the unwanted factors (normalizedCounts object within ‘ruv_set’). Note that the normalized counts should only used for exploratory purposes and not subsequent differential expression analyses. For additional information on this topic, please refer official RUVSeq documentation. The normalized counts can be viewed using the following: # Viewing the head of the normalized count data, accounting for unwanted variation head(normCounts(ruv_set)) ## M13_PineNeedlesSmolder M14_PineNeedlesSmolder ## 0610009B22Rik_56046 4 5 ## 0610010F05Rik_69119 2552 2630 ## 0610010F05Rik_74637 17 13 ## 0610010K14Rik_31619 1602 1774 ## 0610010K14Rik_31873 711 1009 ## 0610010K14Rik_68949 23 25 ## M15_PineNeedlesSmolder M16_PineNeedlesSmolder ## 0610009B22Rik_56046 8 8 ## 0610010F05Rik_69119 2474 1634 ## 0610010F05Rik_74637 17 3 ## 0610010K14Rik_31619 1639 1722 ## 0610010K14Rik_31873 819 992 ## 0610010K14Rik_68949 19 28 ## M17_PineNeedlesSmolder M18_PineNeedlesSmolder ## 0610009B22Rik_56046 6 4 ## 0610010F05Rik_69119 2343 2352 ## 0610010F05Rik_74637 12 7 ## 0610010K14Rik_31619 1979 1726 ## 0610010K14Rik_31873 889 888 ## 0610010K14Rik_68949 20 30 ## M49_PineNeedlesFlame M50_PineNeedlesFlame ## 0610009B22Rik_56046 4 5 ## 0610010F05Rik_69119 1767 1563 ## 0610010F05Rik_74637 12 6 ## 0610010K14Rik_31619 1480 1134 ## 0610010K14Rik_31873 586 599 ## 0610010K14Rik_68949 23 27 ## M51_PineNeedlesFlame M52_PineNeedlesFlame ## 0610009B22Rik_56046 4 4 ## 0610010F05Rik_69119 1918 1711 ## 0610010F05Rik_74637 4 9 ## 0610010K14Rik_31619 1390 1763 ## 0610010K14Rik_31873 683 797 ## 0610010K14Rik_68949 26 27 ## M53_PineNeedlesFlame M54_PineNeedlesFlame M31_Saline ## 0610009B22Rik_56046 5 5 3 ## 0610010F05Rik_69119 1751 1582 1891 ## 0610010F05Rik_74637 6 3 10 ## 0610010K14Rik_31619 1370 1419 1340 ## 0610010K14Rik_31873 593 647 561 ## 0610010K14Rik_68949 20 24 19 ## M32_Saline M33_Saline M34_Saline M35_Saline M36_Saline ## 0610009B22Rik_56046 2 4 3 4 4 ## 0610010F05Rik_69119 2507 2518 2524 1831 1878 ## 0610010F05Rik_74637 7 7 5 7 8 ## 0610010K14Rik_31619 1563 1632 1438 1280 967 ## 0610010K14Rik_31873 685 815 703 646 526 ## 0610010K14Rik_68949 16 21 19 22 20 ## M67_LPS M68_LPS M69_LPS M70_LPS ## 0610009B22Rik_56046 3 2 4 5 ## 0610010F05Rik_69119 1906 1597 1461 1201 ## 0610010F05Rik_74637 6 6 7 5 ## 0610010K14Rik_31619 964 1012 776 638 ## 0610010K14Rik_31873 622 702 522 496 ## 0610010K14Rik_68949 21 21 18 15 Let’s again generate an exploratory plot using this updated dataset, focusing on the bar chart view since that was the most informative pre-RUV. Here are the updated bar charts summarizing overall data distributions per sample: colors &lt;- brewer.pal(4, &quot;Set2&quot;) plotRLE(ruv_set, outline=FALSE, ylim=c(-4, 4), col=colors[groups]) This plot shows overall tighter data that are more similarly distributed across samples. Therefore, it is looking like this RUV addition improved the overall distribution of this dataset. It is important not to over-correct/over-smooth your datasets, so implement these types of pre-processing steps with caution. One strategy that we commonly employ to gauge whether data smoothing is needed/applied correctly is to run the statistical models with and without correction of potential sources of heterogeneity, and critically evaluate similarities vs differences produced in the results. With this, we can answer Environmental Health Question 4: What is an approach that analysts can use when evaluating transcriptomic data from tissues of mixed cellular composition to aid in controlling for sources of sample heterogeneity? Answer: Remove unwanted variation (RUV), among other approaches, including surrogate variable analysis (SVA). Statistical Analysis of Gene Expression Significantly Differentially Expressed Genes Here, we identify genes that are significantly differentially expressed by environmental exposure conditions (e.g., biomass smoke exposure). At this point, we have completed several data pre-processing, QA/QC, and additional steps to prepare our example transcriptomics data for statistical analysis. Finally, we are ready to run the overall statistical model to identify genes that are altered in expression in association with different biomass burn conditions. We will leverage the DESeq2 package to carry out these statistical comparisons. This package is now the most commonly implemented analysis pipeline used for transcriptomic data, including sequencing data as well as transcriptomic data produced via other technologies (e.g., Nanostring, Fluidigm, and other gene expression technologies). This package is extremely well-documented and we encourage trainees to leverage these resources in parallel with the current training module when carrying out their own transcriptomics analyses in R: Bioconductor website Vignette Manual Primary citation: Love MI, Huber W, Anders S. Moderated estimation of fold change and dispersion for RNA-seq data with DESeq2. Genome Biol. 2014;15(12):550. doi: 10.1186/s13059-014-0550-8. PMID: 25516281 In brief, the basic calculations employed within the DESeq2 underlying algorithms include the following: 1. Estimate size factors. In the first step, size factors are estimated to help account for potential differences in the sequencing depth across samples. It is similar to a normalization parameter in the model. 2. Normalize count data. DESeq2 employs different normalization algorithms depending on the parameters selected / stage of analysis. The most commonly employed method is called the median of ratios, which takes into account sequencing depth and RNA composition, as described here. Specifically, these normalized values are calculated as counts divided by sample-specific size factors determined by median ratio of gene counts relative to geometric mean per gene. DESeq2 then transforms these data using variance stabilization within the final statistical model. Because of these two steps, we prefer to export both the median of ratios normalized data as well as the variance stabilization transformed data, to save in our records and use when generating plots of expression levels for specific genes we are interested in. These steps are detailed below. 3. Estimate dispersion. The dispersion estimate takes into account the relationship between the variance of an observed count and its mean value. It is similar to a variance parameter. In DESeq2, dispersion is estimated using a maximum likelihood and empirical bayes approach. 4. Fit negative binomial generalized linear model (GLM). Finally, a negative binomial model is fit for each gene using the design formula that will be described within the proceeding code. The Wald test is performed to test if log fold changes in expression (typically calculated as log(average exposed / average unexposed)) significantly differ from zero. Statistical p-values are reported from this test and also adjusted for multiple testing using the Benjamini and Hochberg procedure. Note that these calculations, among others, are embedded within the DESeq2 functions, so we do not need to code for them ourselves. Instead, we just need to make sure that we set-up the DESeq2 functions correctly, such that these calculations are carried out appropriately in our final transcriptomics analyses. Setting up the DESeq2 experiment Here we provide example script that is used to identify which genes are significantly differentially expressed in association with the example biomass smoke exposures, smoldering pine needles and flaming pine needles, as well as a positive inflammation control, LPS. First, we need to set-up the DESeq2 experiment: # Set up our experiment using our RUV adjusted count and phenotype data. # Our design indicates that our count data is dependent on the exposure condition # (groups variable) and our factor of unwanted variation, and we have specified # that there not be an intercept term through the use of &#39;~0&#39; dds &lt;- DESeqDataSetFromMatrix(countData = counts(ruv_set), # Grabbing count data from the &#39;ruv_set&#39; object colData = pData(ruv_set), # Grabbing the phenotype data and corresponding factor of unwanted variation from the &#39;ruv_set&#39; object design = ~0+groups+W_1) # Setting up the statistical formula (see below) For the forumula design, we use a ‘~0’ at the front to not include an intercept term, and then also account for the exposure condition (groups) and the previously calculated factors of unwanted variation (W_1) of the samples. Formula design is an important step and should be carefully considered for each individual analysis. Other resources, including official DESeq2 documentation, are avialble for consultation regarding formula design, as the specifics of formula design are beyond the scope of this training module. Estimating size factors # Estimate size factors from the dds object that was just created as the experiment above dds &lt;- estimateSizeFactors(dds) sizeFactors(dds) # viewing the size factors ## M13_PineNeedlesSmolder M14_PineNeedlesSmolder M15_PineNeedlesSmolder ## 1.2668895 1.3040762 1.1782071 ## M16_PineNeedlesSmolder M17_PineNeedlesSmolder M18_PineNeedlesSmolder ## 1.1723749 1.2904950 1.3405500 ## M49_PineNeedlesFlame M50_PineNeedlesFlame M51_PineNeedlesFlame ## 1.1313237 0.6313099 0.9328970 ## M52_PineNeedlesFlame M53_PineNeedlesFlame M54_PineNeedlesFlame ## 1.0050414 0.7649632 0.8747854 ## M31_Saline M32_Saline M33_Saline ## 0.8050127 1.0480725 1.0389623 ## M34_Saline M35_Saline M36_Saline ## 1.0282261 0.9519266 1.0409103 ## M67_LPS M68_LPS M69_LPS ## 0.9308588 1.1002076 0.9793051 ## M70_LPS ## 0.8575718 Calculating and exporting normalized counts # Calculating normalized count data normcounts &lt;- as.data.frame(counts(dds, normalized=TRUE)) write.csv(normcounts, &quot;Module2_4_Output_NormalizedCounts.csv&quot;) # Transforming normalized counts through variance stabilization vsd &lt;- varianceStabilizingTransformation(dds, blind=FALSE) vsd_matrix &lt;- as.matrix(assay(vsd)) write.csv(vsd_matrix, &quot;Module2_4_Output_VSDCounts.csv&quot;, row.names=TRUE) Running the final DESeq2 experiment Here, we are finally ready to run the actual statistical comparisons (exposed vs control samples) to calculate fold changes and p-values that describe the degree to which each gene may or may not be altered at the expression level in association with treatment. For this example, we would like to run three different comparisons: (1) Smoldering Pine Needles vs. Control (2) Flaming Pine Needles vs. Control (3) LPS vs. Control which we can easily code for using a loop function, as detailed below. # Run experiment dds_run &lt;- DESeq(dds, betaPrior=FALSE) # Loop through and extract and export results for all contrasts (treatments vs. control) for (trt in trt_groups){ # Iterate for each of the treatments listed in &#39;trt_groups&#39; cat(trt) # Print which treatment group we are on in the loop res &lt;- results(dds_run, pAdjustMethod = &quot;BH&quot;, contrast = c(&quot;groups&quot;,trt,ctrl)) # Extract the results of the DESeq2 analysis specifically for the comparison of the treatment group for the current iteration of the loop with the control group summary(res) # Print out a high-level summary of the results ordered &lt;- as.data.frame(res[order(res$padj),]) # Make a dataframe of the results and order them by adjusted p-value from lowest to highest top10 &lt;- head(ordered, n=10) # Make dataframe of the first ten rows of the ordered results cat(&quot;\\nThe 10 most significantly differentially expressed genes by adjusted p-value:\\n\\n&quot;) print(top10) # View the first ten rows of the ordered results pfilt.05 &lt;- nrow(ordered %&gt;% filter(padj&lt;0.05)) # Get the number of genes that are significantly differentially expressed where padj &lt; 0.05 cat(&quot;\\nThe number of genes showing significant differential expression where padj &lt; 0.05 is &quot;, pfilt.05) pfilt.10 &lt;- nrow(ordered %&gt;% filter(padj&lt;0.1)) # Get the number of genes that are significantly differentially expressed where padj &lt; 0.10 cat(&quot;\\nThe number of genes showing significant differential expression where padj &lt; 0.10 is &quot;, pfilt.10,&quot;\\n\\n&quot;) write.csv(ordered, paste0(&quot;Module2_4_Output_StatisticalResults_&quot;,trt ,&quot;.csv&quot;)) # Export the full dataframe of ordered results as a csv } ## PineNeedlesSmolder_4h_Lung ## out of 16664 with nonzero total read count ## adjusted p-value &lt; 0.1 ## LFC &gt; 0 (up) : 457, 2.7% ## LFC &lt; 0 (down) : 697, 4.2% ## outliers [1] : 0, 0% ## low counts [2] : 1293, 7.8% ## (mean count &lt; 21) ## [1] see &#39;cooksCutoff&#39; argument of ?results ## [2] see &#39;independentFiltering&#39; argument of ?results ## ## ## The 10 most significantly differentially expressed genes by adjusted p-value: ## ## baseMean log2FoldChange lfcSE stat pvalue ## Hbs1l_72436 367.64772 -1.1465230 0.13614949 -8.421060 3.730933e-17 ## Cetn3_30290 2350.31984 -0.7197604 0.09379276 -7.673944 1.667868e-14 ## Grasp_56900 277.43936 0.9759274 0.14177298 6.883734 5.830390e-12 ## Pcsk7_57804 180.24040 0.8261941 0.13529825 6.106465 1.018617e-09 ## Plscr4_55764 178.95216 1.3193654 0.21751536 6.065620 1.314459e-09 ## Socs3_79479 183.28293 1.2535533 0.20740514 6.043983 1.503550e-09 ## Kat2b_64780 444.97603 -0.7197105 0.12034633 -5.980327 2.226896e-09 ## Ptp4a2_62762 1840.75247 -0.7802857 0.13076704 -5.966990 2.416698e-09 ## Rmi1_56327 377.57936 -0.8548903 0.14288443 -5.983089 2.189449e-09 ## Abcd2_62271 57.46455 -1.2504459 0.21464501 -5.825646 5.689193e-09 ## padj ## Hbs1l_72436 5.734817e-13 ## Cetn3_30290 1.281840e-10 ## Grasp_56900 2.987297e-08 ## Pcsk7_57804 3.851843e-06 ## Plscr4_55764 3.851843e-06 ## Socs3_79479 3.851843e-06 ## Kat2b_64780 4.127451e-06 ## Ptp4a2_62762 4.127451e-06 ## Rmi1_56327 4.127451e-06 ## Abcd2_62271 8.744859e-06 ## ## The number of genes showing significant differential expression where padj &lt; 0.05 is 679 ## The number of genes showing significant differential expression where padj &lt; 0.10 is 1154 ## ## PineNeedlesFlame_4h_Lung ## out of 16664 with nonzero total read count ## adjusted p-value &lt; 0.1 ## LFC &gt; 0 (up) : 411, 2.5% ## LFC &lt; 0 (down) : 575, 3.5% ## outliers [1] : 0, 0% ## low counts [2] : 0, 0% ## (mean count &lt; 10) ## [1] see &#39;cooksCutoff&#39; argument of ?results ## [2] see &#39;independentFiltering&#39; argument of ?results ## ## ## The 10 most significantly differentially expressed genes by adjusted p-value: ## ## baseMean log2FoldChange lfcSE stat pvalue ## Tmem109_70229 351.49267 1.2681226 0.1637785 7.742913 9.716419e-15 ## Rhof_64396 58.57258 1.4767798 0.2210640 6.680328 2.384081e-11 ## Plscr4_55764 178.95216 1.5349460 0.2393976 6.411701 1.439049e-10 ## Pip4k2a_67992 796.64597 0.8262520 0.1299839 6.356571 2.063073e-10 ## Zranb2_68671 739.91379 0.8131628 0.1330277 6.112735 9.793822e-10 ## Cul1_31957 896.52286 -0.7061150 0.1203618 -5.866602 4.448172e-09 ## Ppfibp1_75630 655.07197 1.4148222 0.2481843 5.700692 1.193223e-08 ## Mbnl1_30670 851.18586 -1.0159639 0.1811247 -5.609197 2.032670e-08 ## Atp5k_68198 1096.49479 1.1003470 0.1983469 5.547588 2.896380e-08 ## Gclm_29258 138.54250 1.0993904 0.1985396 5.537387 3.070173e-08 ## padj ## Tmem109_70229 1.619144e-10 ## Rhof_64396 1.986416e-07 ## Plscr4_55764 7.993439e-07 ## Pip4k2a_67992 8.594761e-07 ## Zranb2_68671 3.264085e-06 ## Cul1_31957 1.235406e-05 ## Ppfibp1_75630 2.840553e-05 ## Mbnl1_30670 4.234052e-05 ## Atp5k_68198 5.116136e-05 ## Gclm_29258 5.116136e-05 ## ## The number of genes showing significant differential expression where padj &lt; 0.05 is 515 ## The number of genes showing significant differential expression where padj &lt; 0.10 is 986 ## ## LPS_4h_Lung ## out of 16664 with nonzero total read count ## adjusted p-value &lt; 0.1 ## LFC &gt; 0 (up) : 2960, 18% ## LFC &lt; 0 (down) : 2947, 18% ## outliers [1] : 0, 0% ## low counts [2] : 0, 0% ## (mean count &lt; 10) ## [1] see &#39;cooksCutoff&#39; argument of ?results ## [2] see &#39;independentFiltering&#39; argument of ?results ## ## ## The 10 most significantly differentially expressed genes by adjusted p-value: ## ## baseMean log2FoldChange lfcSE stat pvalue ## Usp18_32270 1319.2259 3.829526 0.1459140 26.24508 8.133515e-152 ## Usp18_67659 414.5959 3.739864 0.1679851 22.26307 8.428171e-110 ## Ifit1_29859 862.3286 5.000365 0.2259553 22.12989 1.629877e-108 ## Oasl2_67419 1437.6182 3.710164 0.1904583 19.48019 1.616902e-84 ## Gbp5_77887 600.7219 5.272287 0.2769982 19.03365 8.977555e-81 ## Oasl2_67043 292.4482 3.898484 0.2096435 18.59578 3.476310e-77 ## Rnf213_57840 1052.0454 3.168052 0.1731212 18.29962 8.332621e-75 ## Cmpk2_70664 429.4232 4.592488 0.2518916 18.23200 2.875888e-74 ## Cmpk2_29189 695.1997 4.264136 0.2386279 17.86939 2.042134e-71 ## Eif2ak2_30238 1585.6071 2.671974 0.1495526 17.86645 2.152596e-71 ## padj ## Usp18_32270 1.355369e-147 ## Usp18_67659 7.022352e-106 ## Ifit1_29859 9.053424e-105 ## Oasl2_67419 6.736013e-81 ## Gbp5_77887 2.992039e-77 ## Oasl2_67043 9.654872e-74 ## Rnf213_57840 1.983640e-71 ## Cmpk2_70664 5.990475e-71 ## Cmpk2_29189 3.587085e-68 ## Eif2ak2_30238 3.587085e-68 ## ## The number of genes showing significant differential expression where padj &lt; 0.05 is 4813 ## The number of genes showing significant differential expression where padj &lt; 0.10 is 5907 With this, we can answer Environmental Health Question 5: How many genes showed significant differential expression associated with flaming pine needles exposure in the mouse lung, based on a statistical filter of a multiple test corrected p-value (padj) &lt; 0.05? Answer: 515 genes. With this, we can answer Environmental Health Question 6: How many genes showed significant differential expression associated with smoldering pine needles exposure in the mouse lung, based on a statistical filter of a multiple test corrected p-value (padj) &lt; 0.05? Answer: 679 genes. With this, we can answer Environmental Health Question 7: How many genes showed significant differential expression associated with lipopolysaccharide (LPS) exposure in the mouse lung, based on a statistical filter of a multiple test corrected p-value (padj) &lt; 0.05? Answer: 4813 genes. Together, we find that exposure to both flaming and smoldering of pine needles caused substantial disruptions in gene expression profiles. LPS serves as a positive control for inflammation and produced the greatest transcriptomic response. Visualizing Statistical Results MA Plots MA plots represent a common method of visualization that illustrates differences between measurements taken in two samples, by transforming the data onto M (log ratio) and A (mean average) scales, then plotting these values. Here, we leverage MA plots to show how log fold changes relate to expression levels. In these plots, the log fold change is plotted on the y-axis and expression values are plotted along the x-axis, and dots are colored according to statistical significance (using padj&lt;0.05 as the statistical filter). Here we will generate an MA plot for Flaming Pine Needles. res &lt;- results(dds_run, pAdjustMethod = &quot;BH&quot;, contrast = c(&quot;groups&quot;,&quot;PineNeedlesFlame_4h_Lung&quot;,ctrl)) # Re-extract the DESeq2 results for the flamming pine needles MA &lt;- data.frame(res) # Make a prelimiary dataframe of the flaming pine needle results MA_ns &lt;- MA[ which(MA$padj&gt;=0.05),] # Non-significant genes to plot MA_up &lt;- MA[ which(MA$padj&lt;0.05 &amp; MA$log2FoldChange &gt; 0),] # Significant up-regulated genes to plot MA_down &lt;- MA[ which(MA$padj&lt;0.05 &amp; MA$log2FoldChange &lt; 0),] #Significant down-regulated genes to plot ggplot(MA_ns, aes(x = baseMean, y = log2FoldChange)) + # Plot data with counts on x-axis and log2 fold change on y-axis geom_point(color=&quot;gray75&quot;, size = .5) + # Set point size and color geom_point(data = MA_up, color=&quot;firebrick&quot;, size=1, show.legend = TRUE) + # Plot the up-regulated significant genes geom_point(data = MA_down, color=&quot;dodgerblue2&quot;, size=1, show.legend = TRUE) + # Plot down-regulated significant genes theme_bw() + # Change theme of plot from gray to black and white # We want to log10 transform x-axis for better visualizations scale_x_continuous(trans = &quot;log10&quot;, breaks=c(1,10,100, 1000, 10000, 100000, 1000000), labels=c(&quot;1&quot;,&quot;10&quot;,&quot;100&quot;, &quot;1000&quot;, &quot;10000&quot;, &quot;100000&quot;, &quot;1000000&quot;)) + # We will bound y axis as well to better fit data while not leaving out too many points scale_y_continuous(limits=c(-2, 2)) + xlab(&quot;Expression (Normalized Count)&quot;) + ylab(&quot;Fold Change (log2)&quot;) + # Add labels for axes labs(title=&quot;MA Plot Flaming Pine Needles 4h Lung&quot;) + # Add title geom_hline(yintercept=0) # Add horizontal line at 0 Volcano Plots Similar to MA plots, volcano plots provide visualizations of fold changes in expression from transcriptomic data. However, instead of plotting these values against expression, log fold change is plotted against (adjusted) p-values in volcano plots. Here, we use functions within the EnhancedVolcano package to generate a volcano plot for Flaming Pine Needles. Running the ‘EnhancedVolcano’ function to generate an example volcano plot: Vol &lt;- data.frame(res) # Dataset to use for plotting EnhancedVolcano(Vol, lab = rownames(res), # Label information from dataset (can be a column name) x = &#39;log2FoldChange&#39;, # Column name in dataset with l2fc information y = &#39;padj&#39;, # Column name in dataset with adjusted p-value information ylab = &quot;-Log(FDR-adjusted p)&quot;, # Y-axis label pCutoff= 0.05, # Set p-value cutoff ylim=c(0,5), # Limit y-axis for better plot visuals xlim=c(-2,2), # Limit x-axis (similar to in MA plot y-axis) title=&quot;Volcano Plot&quot;, # Set title subtitle = &quot;Flaming Pine Needles 4h Lung&quot;, # Set subtitle legendPosition = &#39;bottom&#39;) # Put legend on bottom Pathway Enrichment Analysis Interpretting Findings using Pathway Enrichment Analysis Pathway enrichment analysis is a very helpful tool that can be applied to interpret transcriptomic changes of interest in terms of systems biology. In these types of analyses, gene lists of interest are used to identify biological pathways that include genes present in your dataset more often than expected by chance alone. There are many tools that can be used to carry out pathway enrichment analyses. Here, we are using the R package, PIANO, to carry out the statistical enrichment analysis based on the lists of genes we previously identified with differential expression associated with flaming pine needles exposure. To detail, the following input data are required to run PIANO: Your background gene sets, which represent all genes queried from your experiment (aka your ‘gene universe’) The list of genes you are interested in evaluating pathway enrichment of; here, this represents the genes identified with significant differential expression associated with flaming pine needles A underlying pathway dataset; here, we’re using the KEGG PATHWAY Database (KEGG), summarized through the Molecular Signature Database (MSigDB) into pre-formatted input files (.gmt) ready for PIANO. Let’s organize these three required data inputs. Background gene set: # First grab the rownames of the &#39;res&#39; object, which was redefined as the DESeq2 # results for flaming pine needles prior to MA plot generation, and remove the # BioSpyder numeric identifier using a sub function, while maintaining the gene # symbol and place these IDs into a new list within the &#39;res&#39; object (saved as &#39;id&#39;) res$id &lt;- gsub(&quot;_.*&quot;, &quot;&quot;, rownames(res)); # Because these IDs now contain duplicate gene symbols, we need to remove duplicates # One way to do this is to preferentially retain rows of data with the largest # fold change (it doesn&#39;t really matter here, because we&#39;re just identifying # unique genes within the background set) res.ordered &lt;- res[order(res$id, -abs(res$log2FoldChange) ), ] # sort by id and reverse of abs(log2foldchange) res.ordered &lt;- res.ordered[ !duplicated(res.ordered$id), ] # removing gene duplicates # Setting this as the background list Background &lt;- toupper(as.character(res.ordered$id)) Background[1:200] # viewing the first 200 genes in this background list ## [1] &quot;0610009B22RIK&quot; &quot;0610010F05RIK&quot; &quot;0610010K14RIK&quot; &quot;0610012G03RIK&quot; ## [5] &quot;0610030E20RIK&quot; &quot;0610040J01RIK&quot; &quot;1110004F10RIK&quot; &quot;1110008P14RIK&quot; ## [9] &quot;1110012L19RIK&quot; &quot;1110017D15RIK&quot; &quot;1110032A03RIK&quot; &quot;1110038F14RIK&quot; ## [13] &quot;1110059E24RIK&quot; &quot;1110059G10RIK&quot; &quot;1110065P20RIK&quot; &quot;1190002N15RIK&quot; ## [17] &quot;1190007I07RIK&quot; &quot;1500011B03RIK&quot; &quot;1500011K16RIK&quot; &quot;1600002K03RIK&quot; ## [21] &quot;1600012H06RIK&quot; &quot;1600014C10RIK&quot; &quot;1700001L19RIK&quot; &quot;1700003E16RIK&quot; ## [25] &quot;1700007K13RIK&quot; &quot;1700012B09RIK&quot; &quot;1700013F07RIK&quot; &quot;1700016K19RIK&quot; ## [29] &quot;1700017B05RIK&quot; &quot;1700020D05RIK&quot; &quot;1700024G13RIK&quot; &quot;1700025G04RIK&quot; ## [33] &quot;1700028P14RIK&quot; &quot;1700029I15RIK&quot; &quot;1700029J07RIK&quot; &quot;1700030K09RIK&quot; ## [37] &quot;1700037C18RIK&quot; &quot;1700037H04RIK&quot; &quot;1700086D15RIK&quot; &quot;1700088E04RIK&quot; ## [41] &quot;1700102P08RIK&quot; &quot;1700109H08RIK&quot; &quot;1700123O20RIK&quot; &quot;1810009A15RIK&quot; ## [45] &quot;1810010H24RIK&quot; &quot;1810013L24RIK&quot; &quot;1810022K09RIK&quot; &quot;1810030O07RIK&quot; ## [49] &quot;1810037I17RIK&quot; &quot;1810043G02RIK&quot; &quot;1810055G02RIK&quot; &quot;2010005H15RIK&quot; ## [53] &quot;2010111I01RIK&quot; &quot;2010300C02RIK&quot; &quot;2010309G21RIK&quot; &quot;2200002D01RIK&quot; ## [57] &quot;2210011C24RIK&quot; &quot;2210016H18RIK&quot; &quot;2210016L21RIK&quot; &quot;2210408I21RIK&quot; ## [61] &quot;2300009A05RIK&quot; &quot;2310007B03RIK&quot; &quot;2310009B15RIK&quot; &quot;2310011J03RIK&quot; ## [65] &quot;2310022A10RIK&quot; &quot;2310022B05RIK&quot; &quot;2310030G06RIK&quot; &quot;2310033P09RIK&quot; ## [69] &quot;2310057M21RIK&quot; &quot;2310061I04RIK&quot; &quot;2410002F23RIK&quot; &quot;2410004B18RIK&quot; ## [73] &quot;2410015M20RIK&quot; &quot;2410131K14RIK&quot; &quot;2510002D24RIK&quot; &quot;2510009E07RIK&quot; ## [77] &quot;2600001M11RIK&quot; &quot;2610001J05RIK&quot; &quot;2610002M06RIK&quot; &quot;2610008E11RIK&quot; ## [81] &quot;2610028H24RIK&quot; &quot;2610301B20RIK&quot; &quot;2610507B11RIK&quot; &quot;2700049A03RIK&quot; ## [85] &quot;2700062C07RIK&quot; &quot;2700081O15RIK&quot; &quot;2700097O09RIK&quot; &quot;2810004N23RIK&quot; ## [89] &quot;2810021J22RIK&quot; &quot;2810408A11RIK&quot; &quot;2900026A02RIK&quot; &quot;3110001I22RIK&quot; ## [93] &quot;3110040N11RIK&quot; &quot;3110082I17RIK&quot; &quot;3830406C13RIK&quot; &quot;4430402I18RIK&quot; ## [97] &quot;4732423E21RIK&quot; &quot;4833414E09RIK&quot; &quot;4833420G17RIK&quot; &quot;4833427G06RIK&quot; ## [101] &quot;4833439L19RIK&quot; &quot;4921524J17RIK&quot; &quot;4930402H24RIK&quot; &quot;4930430F08RIK&quot; ## [105] &quot;4930451G09RIK&quot; &quot;4930453N24RIK&quot; &quot;4930486L24RIK&quot; &quot;4930503L19RIK&quot; ## [109] &quot;4930523C07RIK&quot; &quot;4930550C14RIK&quot; &quot;4930562C15RIK&quot; &quot;4931406C07RIK&quot; ## [113] &quot;4931406P16RIK&quot; &quot;4931414P19RIK&quot; &quot;4932438A13RIK&quot; &quot;4933408B17RIK&quot; ## [117] &quot;4933415F23RIK&quot; &quot;4933427D14RIK&quot; &quot;4933434E20RIK&quot; &quot;5330417C22RIK&quot; ## [121] &quot;5430427O19RIK&quot; &quot;5730480H06RIK&quot; &quot;5830417I10RIK&quot; &quot;6030458C11RIK&quot; ## [125] &quot;6030468B19RIK&quot; &quot;6330403K07RIK&quot; &quot;6330417A16RIK&quot; &quot;6430531B16RIK&quot; ## [129] &quot;6430548M08RIK&quot; &quot;6430550D23RIK&quot; &quot;6720489N17RIK&quot; &quot;8030462N17RIK&quot; ## [133] &quot;9030624G23RIK&quot; &quot;9130008F23RIK&quot; &quot;9130019O22RIK&quot; &quot;9130023H24RIK&quot; ## [137] &quot;9130230L23RIK&quot; &quot;9230104L09RIK&quot; &quot;9330182L06RIK&quot; &quot;9530068E07RIK&quot; ## [141] &quot;9930012K11RIK&quot; &quot;9930021J03RIK&quot; &quot;9930111J21RIK1&quot; &quot;9930111J21RIK2&quot; ## [145] &quot;A130071D04RIK&quot; &quot;A230050P20RIK&quot; &quot;A2ML1&quot; &quot;A430005L14RIK&quot; ## [149] &quot;A430033K04RIK&quot; &quot;A430078G23RIK&quot; &quot;A530032D15RIK&quot; &quot;A630001G21RIK&quot; ## [153] &quot;A730034C02&quot; &quot;A730049H05RIK&quot; &quot;A830018L16RIK&quot; &quot;A930002H24RIK&quot; ## [157] &quot;A930004D18RIK&quot; &quot;AA415038&quot; &quot;AA986860&quot; &quot;AACS&quot; ## [161] &quot;AAED1&quot; &quot;AAGAB&quot; &quot;AAK1&quot; &quot;AAMDC&quot; ## [165] &quot;AAMP&quot; &quot;AAR2&quot; &quot;AARS&quot; &quot;AARS2&quot; ## [169] &quot;AARSD1&quot; &quot;AASDH&quot; &quot;AASS&quot; &quot;AATF&quot; ## [173] &quot;AATK&quot; &quot;AB124611&quot; &quot;ABCA1&quot; &quot;ABCA17&quot; ## [177] &quot;ABCA2&quot; &quot;ABCA3&quot; &quot;ABCA5&quot; &quot;ABCA7&quot; ## [181] &quot;ABCA8A&quot; &quot;ABCA8B&quot; &quot;ABCA9&quot; &quot;ABCB10&quot; ## [185] &quot;ABCB1A&quot; &quot;ABCB1B&quot; &quot;ABCB6&quot; &quot;ABCB7&quot; ## [189] &quot;ABCB8&quot; &quot;ABCC1&quot; &quot;ABCC3&quot; &quot;ABCC4&quot; ## [193] &quot;ABCC5&quot; &quot;ABCD1&quot; &quot;ABCD2&quot; &quot;ABCD3&quot; ## [197] &quot;ABCD4&quot; &quot;ABCE1&quot; &quot;ABCF1&quot; &quot;ABCF3&quot; The list of genes identified with significant differential expression associated with flaming pine needles: # Similar to the above script, but starting with the res$id object # and filtering for genes with padj &lt; 0.05 # Ordering by negative absolute value of log2FoldChange res.ordered &lt;- res[order(res$id, -abs(res$log2FoldChange) ), ] # Pulling the genes with padj &lt; 0.05 SigGenes &lt;- toupper(as.character(res.ordered[which(res.ordered$padj&lt;.05),&quot;id&quot;])) # Removing gene duplicates SigGenes &lt;- SigGenes[ !duplicated(SigGenes)] # Viewing the length of this significant gene list length(SigGenes) ## [1] 488 Therefore, this gene set includes 488 unique genes significantly associated with the Flaming Pine Needles condition, based on padj&lt;0.05. The underlying KEGG pathway dataset. Note that this file was simply downloaded from MSigDB, ready for upload as a .gmt file. Here, we use the ‘loadGSC’ function enabled through the PIANO package to upload and organize these pathways. KEGG_Pathways &lt;- loadGSC(file=&quot;Module2_4/Module2_4_kegg_v7_symbols.gmt&quot;, type=&quot;gmt&quot;) # Viewing the number of biological pathways contained in the database length(KEGG_Pathways$gsc) ## [1] 186 This KEGG pathway database therefore includes 186 biological pathways available to query. With these data inputs ready, we can now run the pathway enrichment analysis. The enrichment statistic that is commonly employed through the PIANO package is based of a hypergeometric test, run through the ‘runGSAhyper’ function. This returns a p-value for each gene set from which you can determine enrichment status. # Running the piano function based on the hypergeometric statistic Results_GSA &lt;- piano::runGSAhyper(genes=SigGenes, universe=Background, gsc=KEGG_Pathways, gsSizeLim=c(1,Inf), adjMethod = &quot;fdr&quot;) # Pulling the pathway enrichment results into a separate dataframe PathwayResults &lt;- as.data.frame(Results_GSA$resTab) # Viewing the top of these pathway enrichment results (which are not ordered at the moment) head(PathwayResults) ## p-value Adjusted p-value ## KEGG_N_GLYCAN_BIOSYNTHESIS 0.77021314 1.0000000 ## KEGG_OTHER_GLYCAN_DEGRADATION 1.00000000 1.0000000 ## KEGG_O_GLYCAN_BIOSYNTHESIS 0.03553158 0.6656139 ## KEGG_GLYCOSAMINOGLYCAN_DEGRADATION 0.42056787 1.0000000 ## KEGG_GLYCOSAMINOGLYCAN_BIOSYNTHESIS_KERATAN_SULFATE 1.00000000 1.0000000 ## KEGG_GLYCEROLIPID_METABOLISM 0.16241736 1.0000000 ## Significant (in gene set) ## KEGG_N_GLYCAN_BIOSYNTHESIS 1 ## KEGG_OTHER_GLYCAN_DEGRADATION 0 ## KEGG_O_GLYCAN_BIOSYNTHESIS 3 ## KEGG_GLYCOSAMINOGLYCAN_DEGRADATION 1 ## KEGG_GLYCOSAMINOGLYCAN_BIOSYNTHESIS_KERATAN_SULFATE 0 ## KEGG_GLYCEROLIPID_METABOLISM 3 ## Non-significant (in gene set) ## KEGG_N_GLYCAN_BIOSYNTHESIS 34 ## KEGG_OTHER_GLYCAN_DEGRADATION 13 ## KEGG_O_GLYCAN_BIOSYNTHESIS 15 ## KEGG_GLYCOSAMINOGLYCAN_DEGRADATION 12 ## KEGG_GLYCOSAMINOGLYCAN_BIOSYNTHESIS_KERATAN_SULFATE 12 ## KEGG_GLYCEROLIPID_METABOLISM 31 ## Significant (not in gene set) ## KEGG_N_GLYCAN_BIOSYNTHESIS 487 ## KEGG_OTHER_GLYCAN_DEGRADATION 488 ## KEGG_O_GLYCAN_BIOSYNTHESIS 485 ## KEGG_GLYCOSAMINOGLYCAN_DEGRADATION 487 ## KEGG_GLYCOSAMINOGLYCAN_BIOSYNTHESIS_KERATAN_SULFATE 488 ## KEGG_GLYCEROLIPID_METABOLISM 485 ## Non-significant (not in gene set) ## KEGG_N_GLYCAN_BIOSYNTHESIS 11355 ## KEGG_OTHER_GLYCAN_DEGRADATION 11376 ## KEGG_O_GLYCAN_BIOSYNTHESIS 11374 ## KEGG_GLYCOSAMINOGLYCAN_DEGRADATION 11377 ## KEGG_GLYCOSAMINOGLYCAN_BIOSYNTHESIS_KERATAN_SULFATE 11377 ## KEGG_GLYCEROLIPID_METABOLISM 11358 This dataframe therefore summarizes the enrichment p-value for each pathway, FDR adjusted p-value, number of significant genes in the gene set that intersect with genes in the pathway, etc. With these results, let’s identify which pathways meet a statistical enrichment p-value filter of 0.05: SigPathways &lt;- PathwayResults[which(PathwayResults$`p-value` &lt; 0.05),] rownames(SigPathways) ## [1] &quot;KEGG_O_GLYCAN_BIOSYNTHESIS&quot; ## [2] &quot;KEGG_HYPERTROPHIC_CARDIOMYOPATHY_HCM&quot; ## [3] &quot;KEGG_ARRHYTHMOGENIC_RIGHT_VENTRICULAR_CARDIOMYOPATHY_ARVC&quot; ## [4] &quot;KEGG_PROTEASOME&quot; ## [5] &quot;KEGG_OOCYTE_MEIOSIS&quot; ## [6] &quot;KEGG_VASCULAR_SMOOTH_MUSCLE_CONTRACTION&quot; ## [7] &quot;KEGG_WNT_SIGNALING_PATHWAY&quot; ## [8] &quot;KEGG_HEDGEHOG_SIGNALING_PATHWAY&quot; ## [9] &quot;KEGG_FOCAL_ADHESION&quot; ## [10] &quot;KEGG_ECM_RECEPTOR_INTERACTION&quot; ## [11] &quot;KEGG_COMPLEMENT_AND_COAGULATION_CASCADES&quot; ## [12] &quot;KEGG_GNRH_SIGNALING_PATHWAY&quot; With this, we can answer Environmental Health Question 8: What biological pathways are disrupted in association with flaming pine needles exposure in the lung, identified through systems level analyses? Answer: Biological pathways involved in cardiopulmonary function (e.g., arrhythmogenic right ventricular cardiomyopathy, hypertrophic cardiomyopathy, vascular smooth muscle contraction), carcinogenesis signaling (e.g., Wnt signaling pathway, hedgehog signaling pathway), and hormone signaling (e.g., Gnrh signaling pathway), among others. Concluding Remarks In this module, users are guided through the uploading, organization, QA/QC, statistical analysis, and systems level analysis of an example -omics dataset based on transcriptomic responses to biomass burn scenarios, representing environmental exposure scenarios of growing concern worldwide. It is worth noting that the methods described herein represent a fraction of the approaches and tools that can be leveraged in the analysis of -omics datasets, and methods should be tailored to the purposes of each individual analysis’ goal. For additional example research projects that have leveraged -omics and systems biology to address environmental health questions, see the following select relevant publications: Genomic publications evaluating gene-environment interactions and relations to disease etiology: Balik-Meisner M, Truong L, Scholl EH, La Du JK, Tanguay RL, Reif DM. Elucidating Gene-by-Environment Interactions Associated with Differential Susceptibility to Chemical Exposure. Environ Health Perspect. 2018 Jun 28;126(6):067010. PMID: 29968567. Ward-Caviness CK, Neas LM, Blach C, Haynes CS, LaRocque-Abramson K, Grass E, Dowdy ZE, Devlin RB, Diaz-Sanchez D, Cascio WE, Miranda ML, Gregory SG, Shah SH, Kraus WE, Hauser ER. A genome-wide trans-ethnic interaction study links the PIGR-FCAMR locus to coronary atherosclerosis via interactions between genetic variants and residential exposure to traffic. PLoS One. 2017 Mar 29;12(3):e0173880. PMID: 28355232. Transcriptomic publications evaluating gene expression responses to environmental exposures and relations to disease etiology: Chang Y, Rager JE, Tilton SC. Linking Coregulated Gene Modules with Polycyclic Aromatic Hydrocarbon-Related Cancer Risk in the 3D Human Bronchial Epithelium. Chem Res Toxicol. 2021 Jun 21;34(6):1445-1455. PMID: 34048650. Chappell GA, Rager JE, Wolf J, Babic M, LeBlanc KJ, Ring CL, Harris MA, Thompson CM. Comparison of Gene Expression Responses in the Small Intestine of Mice Following Exposure to 3 Carcinogens Using the S1500+ Gene Set Informs a Potential Common Adverse Outcome Pathway. Toxicol Pathol. 2019 Oct;47(7):851-864. PMID: 31558096. Manuck TA, Eaves LA, Rager JE, Fry RC. Mid-pregnancy maternal blood nitric oxide-related gene and miRNA expression are associated with preterm birth. Epigenomics. 2021 May;13(9):667-682. PMID: 33890487. Epigenomic publications evaluating microRNA, CpG methylation, and/or histone methylation responses to environmental exposures and relations to disease etiology: Chappell GA, Rager JE. Epigenetics in chemical-induced genotoxic carcinogenesis. Curr Opinion Toxicol. 2017 Oct; 6:10-17. Rager JE, Bailey KA, Smeester L, Miller SK, Parker JS, Laine JE, Drobná Z, Currier J, Douillet C, Olshan AF, Rubio-Andrade M, Stýblo M, García-Vargas G, Fry RC. Prenatal arsenic exposure and the epigenome: altered microRNAs associated with innate and adaptive immune signaling in newborn cord blood. Environ Mol Mutagen. 2014 Apr;55(3):196-208. PMID: 24327377. Rager JE, Bauer RN, Müller LL, Smeester L, Carson JL, Brighton LE, Fry RC, Jaspers I. DNA methylation in nasal epithelial cells from smokers: identification of ULBP3-related effects. Am J Physiol Lung Cell Mol Physiol. 2013 Sep 15;305(6):L432-8. PMID: 23831618. Smeester L, Rager JE, Bailey KA, Guan X, Smith N, García-Vargas G, Del Razo LM, Drobná Z, Kelkar H, Stýblo M, Fry RC. Epigenetic changes in individuals with arsenicosis. Chem Res Toxicol. 2011 Feb 18;24(2):165-7. PMID: 21291286. Metabolomic publications evaluating changes in the metabolome in response to environmental exposures and involved in disease etiology: Lu K, Abo RP, Schlieper KA, Graffam ME, Levine S, Wishnok JS, Swenberg JA, Tannenbaum SR, Fox JG. Arsenic exposure perturbs the gut microbiome and its metabolic profile in mice: an integrated metagenomics and metabolomics analysis. Environ Health Perspect. 2014 Mar;122(3):284-91. PMID: 24413286; PMCID: PMC3948040. Manuck TA, Lai Y, Ru H, Glover AV, Rager JE, Fry RC, Lu K. Metabolites from midtrimester plasma of pregnant patients at high risk for preterm birth. Am J Obstet Gynecol MFM. 2021 Jul;3(4):100393. PMID: 33991707. Microbiome publications evaluating changes in microbiome profiles in relation to the environment and human disease: Chi L, Bian X, Gao B, Ru H, Tu P, Lu K. Sex-Specific Effects of Arsenic Exposure on the Trajectory and Function of the Gut Microbiome. Chem Res Toxicol. 2016 Jun 20;29(6):949-51.PMID: 27268458. Cho I, Blaser MJ. The human microbiome: at the interface of health and disease. Nat Rev Genet. 2012 Mar 13;13(4):260-70. PMID: 22411464. Lu K, Abo RP, Schlieper KA, Graffam ME, Levine S, Wishnok JS, Swenberg JA, Tannenbaum SR, Fox JG. Arsenic exposure perturbs the gut microbiome and its metabolic profile in mice: an integrated metagenomics and metabolomics analysis. Environ Health Perspect. 2014 Mar;122(3):284-91. PMID: 24413286. Exposome publications evaluating changes in chemical signatures in relation to the environment and human disease: Rager JE, Strynar MJ, Liang S, McMahen RL, Richard AM, Grulke CM, Wambaugh JF, Isaacs KK, Judson R, Williams AJ, Sobus JR. Linking high resolution mass spectrometry data with exposure and toxicity forecasts to advance high-throughput environmental monitoring. Environ Int. 2016 Mar;88:269-280. PMID: 26812473. Rappaport SM, Barupal DK, Wishart D, Vineis P, Scalbert A. The blood exposome and its role in discovering causes of disease. Environ Health Perspect. 2014 Aug;122(8):769-74. PMID: 24659601. Viet SM, Falman JC, Merrill LS, Faustman EM, Savitz DA, Mervish N, Barr DB, Peterson LA, Wright R, Balshaw D, O’Brien B. Human Health Exposure Analysis Resource (HHEAR): A model for incorporating the exposome into health studies. Int J Hyg Environ Health. 2021 Jun;235:113768. PMID: 34034040. "],["toxicokinetic-modeling.html", "2.5 Toxicokinetic Modeling Introduction to Training Module Data and Models used in Toxicokinetic Modeling (TK) Calculating steady-state concentration Reverse Toxicokinetics Monte Carlo Approach Chemical-Specific Example Calculating Bioactivity-Exposure Ratios (BERs) Concluding Remarks", " 2.5 Toxicokinetic Modeling This training module was developed by Dr. Caroline Ring Fall 2021 Disclaimer: The views expressed in this document are those of the author and do not necessarily reflect the views or policies of the U.S. EPA. Introduction to Toxicokinetic Modeling To understand what toxicokinetic modeling is, consider the following scenario: Simply put, toxicokinetics answers these questions by describing “what the body does to the chemical” after an exposure scenario. More technically, toxicokinetic modeling refers to the evaluation of the uptake and disposition of a chemical in the body. Notes on terminology Pharmacokinetics (PK) is a synonym for toxicokinetics (TK). They are often used interchangeably. PK connotes pharmaceuticals; TK connotes environmental chemicals – but those connotations are weak. A common abbreviation that you will also see in this research field is ADME, which stands for: Absorption: How does the chemical get absorbed into the body tissues? Distribution: Where does the chemical go inside the body? Metabolism: How do enzymes in the body break apart the chemical molecules? Excretion: How does the chemical leave the body? To place this term into the context of TK, TK models describe ADME mathematically by representing the body as compartments and flows. Types of TK models TK models describe the body mathematically as one or more “compartments” connected by “flows.” The compartments represent organs or tissues. Using mass balance equations, the amount or concentration of chemical in each compartment is described as a function of time. Types of models discussed throughout this training module are described here. 1 Compartment Model The simplest TK model is a 1-compartment model, where the body is assumed to be one big well-mixed compartment. 3 Compartment Model A 3-compartment model mathematically incorporates three distinct body compartments, that can exhibit different parameters contributing to their individual mass balance. Commonly used compartments in 3-compartment modeling can include tissues like blood plasma, liver, gut, kidney, and/or ‘rest of body’ terms; though the specific compartments included depend on the chemical under evaluation, exposure scenario, and modeling assumptions. PBTK Model A physiologically-based TK (PBTK) model incorporates compartments and flows that represent real physiological quantities (as opposed to the aforementioned empirical 1- and 3-compartment models). PBTK models have more parameters overall, including parameters representing physiological quantities that are known a priori based on studies of anatomy. The only PBTK model parameters that need to be estimated for each new chemical are parameters representing chemical-body interactions, which can include the following: Rate of hepatic metabolism of chemical: How fast does liver break down chemical? Plasma protein binding: How tightly does the chemical bind to proteins in blood plasma? Liver may not be able to break down chemical that is bound to plasma protein. Blood:tissue partition coefficients: Assuming chemical diffuses between blood and other tissues very fast compared to the rate of blood flow, the ratio of concentration in blood to concentration in each tissue is approximately constant = partition coefficient. Rate of active transport into/out of a tissue: If chemical moves between blood and tissues not just by passive diffusion, but by cells actively transporting it in or out of the tissue Binding to other tissues: Some chemical may be bound inside a tissue and not available for diffusion or transport in/out Types of TK modeling can also fall into the following major categories: Forward TK Modeling: Where external exposure doses are converted into internal doses (or concentrations of chemicals/drugs in one or more body tissues of interest). Reverse TK Modeling: The reverse of the above, where internal doses are converted into external exposure doses. Other TK modeling resources For further information on TK modeling background, math, and example models, there are additional resources online including a helpful course website on Basic Pharmacokinetics by Dr. Bourne. Introduction to Training Module This module serves as an example to guide trainees through the basics of toxicokinetic (TK) modeling and how this type of modeling can be used in the high-throughput setting for environmental health research applications. In this activity, the capabilities of a high-throughput toxicokinetic modeling package titled ‘httk’ are demonstrated on a suite of environmentally relevant chemicals. The httk R package implements high-throughput toxicokinetic modeling (hence, ‘httk’), including a generic physiologically based toxicokinetic (PBTK) model as well as tables of chemical-specific parameters needed to solve the model for hundreds of chemicals. In this activity, the capabilities of ‘httk’ are demonstrated and explored. Example modeling estimates are produced for the high interest environmental chemical, bisphenol-A. Then, an example script is provided to derive the plasma concentration at steady state for an example environmental chemical, bisphenol-A. The concept of reverse toxicokinetics is explained and demonstrated, again using bisphenol-A as an example chemical. This module then demonstrates the derivation of the bioactivity-exposure ratio (BER) across many chemicals leveraging the capabilities of httk, while incorporating exposure measures. BERs are particularly useful in the evaluation of chemical risk, as they take into account both toxicity (i.e., in vitro potency) and exposure rates, the two essential components used in risk calculations for chemical safety and prioritization evaluations. Therefore, the estimates of both potency and exposure and needed to calculate BERs, which are described in this training module. For potency estimates, the ToxCast high-throughput screening library is introduced as an example high-throughput dataset to carry out in vitro to in vivo extrapolation (IVIVE) modeling through httk. ToxCast activity concentrations that elicit 50% maximal bioactivity (AC50) are uploaded and organized as inputs, and then the tenth percentile ToxCast AC50 is calculated for each chemical (in other words, across all ToxCast screening assays, the tenth percentile of AC50 values were carried forward). These concentration estimates then serve as concentration estimates for potency. For exposure estimates, previously generated exposure estimates that have been inferred from CDC NHANES urinary biomonitoring data are used. The bioactivity-exposure ratio (BER) is then calculated across chemicals with both potency and exposure estimate information. This ratio is simply calculated as the ratio of the lower-end equivalent dose (for the most-sensitive 5% of the population) divided by the upper-end estimated exposure (here, the upper bound on the inferred population median exposure). Chemicals are then ranked based on resulting BERs and visualized through plots. The importance of these chemical prioritization are then discussed in relation to environmental health research and corresponding regulatory decisions. Training Module’s Environmental Health Questions This training module was specifically developed to answer the following environmental health questions: After solving the TK model that evaluates bisphenol-A, what is the maximum concentration of bisphenol-A estimated to occur in human plasma, after 1 exposure dose of 1 mg/kg/day? After solving the TK model that evaluates bisphenol-A, what is the steady-state concentration of bisphenol-A estimated to occur in human plasma, for a long-term oral infusion dose of 1 mg/kg/day? What is the predicted range of bisphenol-A concentrations in plasma that can occur in a human population, assuming a long-term exposure rate of 1 mg/kg/day and steady-state conditions? Provide estimates at the 5th, 50th, and 95th percentile? Considering the chemicals evaluated in the above TK modeling example, do the \\(C_{ss}\\)-dose slope distributions become wider as the median \\(C_{ss}\\)-dose slope increases? How many chemicals have available AC50 values to evaluate in the current ToxCast/Tox21 high-throughput screening database? What are the chemicals with the ten lowest predicted equivalent doses (for tenth-percentile ToxCast AC50s), for the most-sensitive 5% of the population? Based on httk modeling estimates, are chemicals with higher bioactivity-exposure ratios always less potent than chemicals with lower bioactivity-exposure ratios? Based on httk modeling estimates, do chemicals with higher bioactivity-exposure ratios always have lower estimated exposures than chemicals with lower bioactivity-exposure ratios? How are chemical prioritization results different when using only hazard information vs. only exposure information vs. bioactivity-exposure ratios? Script Preparations Cleaning the global environment rm(list=ls()) Set your working directory setwd(&quot;/filepath to where your input files are&quot;) Installing required R packages If you already have these packages installed, you can skip this step, or you can run the below code which checks installation status for you: if(!nzchar(system.file(package = &quot;ggplot2&quot;))){ install.packages(&quot;ggplot2&quot;)} if(!nzchar(system.file(package = &quot;reshape2&quot;))){ install.packages(&quot;reshape2&quot;)} if(!nzchar(system.file(package = &quot;stringr&quot;))){ install.packages(&quot;stringr&quot;)} if(!nzchar(system.file(package = &quot;httk&quot;))){ install.packages(&quot;httk&quot;)} if(!nzchar(system.file(package = &quot;eulerr&quot;))){ install.packages(&quot;eulerr&quot;)} Loading R packages required for this session library(ggplot2) # ggplot2 will be used to generate associated graphics library(reshape2) # reshape2 will be used to organize and transform datasets library(stringr) # stringr will be used to aid in various data manipulation steps through this module library(httk) # httk package will be used to carry out all toxicokinetic modeling steps library(eulerr) #eulerr package will be used to generate Venn/Euler diagram graphics For more information on the ggplot2 package, see its associated CRAN webpage and RDocumentation webpage. For more information on the reshape2 package, see its associated CRAN webpage and RDocumentation webpage. For more information on the stringr package, see its associated CRAN webpage and RDocumentation webpage. For more information on the httk package, see its associated CRAN webpage and parent publication by Pearce et al. (2017). More information on the httk package You can see an overview of the httk package by typing ?httk at the R command line. You can see a browsable index of all functions in the httk package by typing help(package=\"httk\") at the R command line. You can see a browsable list of vignettes by typing browseVignettes(\"httk\") at the R command line. (Please note that some of these vignettes were written using older versions of the package and may no longer work as written – specifically the Ring (2017) vignette, which I wrote back in 2016. The httk team is actively working on updating these.) You can get information about any function in httk, or indeed any function in any R package, by typing help() and placing the function name in quotation marks inside the parentheses. For example, to get information about the httk function solve_model(), type this: help(&quot;solve_model&quot;) Data and Models used in Toxicokinetic Modeling (TK) Common Models used in TK Modeling There are five TK models currently built into httk. They are: pbtk: A physiologically-based TK model with oral absorption. Contains the following compartments: gutlumen, gut, liver, kidneys, veins, arteries, lungs, and the rest of the body. Chemical is metabolized by the liver and excreted by the kidneys via glomerular filtration. gas_pbtk: A PBTK model with absorption via inhalation. Contains the same compartments as pbtk. 1compartment: A simple one-compartment TK model with oral absorption. 3compartment: A three-compartment TK model with oral absorption. Compartments are gut, liver, and rest of body. 3compartmentss: The steady-state solution to the 3-compartment model under an assumption of constant infusion dosing, without considering tissue partitioning. This was the first httk model (see Wambaugh et al. 2015, Wetmore et al. 2012, Rotroff et al. 2010). Chemical-Specific TK Data Built Into ‘httk’ Each of these TK models has chemical-specific parameters. The chemical-specific TK information needed to parameterize these models is built into httk, in the form of a built-in lookup table in a data.frame called chem.physical_and_invitro.data. This lookup table means that in order to run a TK model for a particular chemical, you only need to specify the chemical. Look at the first few rows of this data.frame to see everything that’s in there (it is a lot of information). head(chem.physical_and_invitro.data) ## Compound CAS ## 2971-36-0 2,2-bis(4-hydroxyphenyl)-1,1,1-trichloroethane (hpte) 2971-36-0 ## 94-75-7 2,4-d 94-75-7 ## 94-82-6 2,4-db 94-82-6 ## 90-43-7 2-phenylphenol 90-43-7 ## 1007-28-9 6-desisopropylatrazine 1007-28-9 ## 71751-41-2 Abamectin 71751-41-2 ## CAS.Checksum DTXSID Formula ## 2971-36-0 TRUE DTXSID8022325 C14H11Cl3O2 ## 94-75-7 TRUE DTXSID0020442 C8H6Cl2O3 ## 94-82-6 TRUE DTXSID7024035 C10H10Cl2O3 ## 90-43-7 TRUE DTXSID2021151 C12H10O ## 1007-28-9 TRUE DTXSID0037495 C5H8ClN5 ## 71751-41-2 TRUE DTXSID8023892 - ## SMILES.desalt ## 2971-36-0 OC1=CC=C(C=C1)C(C1=CC=C(O)C=C1)C(Cl)(Cl)Cl ## 94-75-7 OC(=O)COC1=C(Cl)C=C(Cl)C=C1 ## 94-82-6 OC(=O)CCCOC1=CC=C(Cl)C=C1Cl ## 90-43-7 OC1=C(C=CC=C1)C1=CC=CC=C1 ## 1007-28-9 CCNC1=NC(N)=NC(Cl)=N1 ## 71751-41-2 - ## All.Compound.Names ## 2971-36-0 2,2-bis(4-hydroxyphenyl)-1,1,1-trichloroethane (hpte)|2,2-bis(4-hydroxyphenyl)-1,1,1-trichloroethane|2971-36-0 ## 94-75-7 2,4-d|Dichlorophenoxy|2,4-dichlorophenoxyacetic acid|94-75-7 ## 94-82-6 2,4-db|2,4-dichlorophenoxybutyric acid|94-82-6 ## 90-43-7 2-phenylphenol|90-43-7 ## 1007-28-9 6-desisopropylatrazine|Deisopropylatrazine|1007-28-9 ## 71751-41-2 Abamectin|71751-41-2 ## logHenry logHenry.Reference logMA logMA.Reference logP ## 2971-36-0 -7.179 EPA NA &lt;NA&gt; 4.622 ## 94-75-7 -8.529 EPA NA &lt;NA&gt; 2.809 ## 94-82-6 -8.833 EPA NA &lt;NA&gt; 3.528 ## 90-43-7 -7.143 EPA 3.46 Endo 2011 3.091 ## 1007-28-9 -8.003 EPA NA &lt;NA&gt; 1.150 ## 71751-41-2 -15.420 EPISuite NA &lt;NA&gt; 4.480 ## logP.Reference logPwa logPwa.Reference logWSol logWSol.Reference ## 2971-36-0 EPA 10.540 EPISuite -3.707 EPA ## 94-75-7 EPA 5.525 EPISuite -2.165 EPA ## 94-82-6 EPA 6.685 EPISuite -3.202 EPA ## 90-43-7 EPA 3.995 EPISuite -1.812 EPA ## 1007-28-9 EPA 6.953 EPISuite -2.413 EPA ## 71751-41-2 Tonnelier 2012 30.490 EPISuite NA &lt;NA&gt; ## MP MP.Reference MW MW.Reference pKa_Accept ## 2971-36-0 171.40 EPA 317.6 EPA None ## 94-75-7 140.60 EPA 221.0 EPA None ## 94-82-6 118.10 EPA 249.1 EPA None ## 90-43-7 59.03 EPA 170.2 EPA None ## 1007-28-9 155.00 EPA 173.6 EPA 1.59 ## 71751-41-2 349.80 EPISuite 819.0 Tonnelier 2012 None ## pKa_Accept.Reference pKa_Donor pKa_Donor.Reference ## 2971-36-0 Sipes 2017 9.63,10.24 Strope 2018 ## 94-75-7 Sipes 2017 2.81 Pearce 2017 ## 94-82-6 Sipes 2017 3.58 Strope 2018 ## 90-43-7 Sipes 2017 9.69 Strope 2018 ## 1007-28-9 Strope 201. None Sipes 2017 ## 71751-41-2 Sipes 2017 12.47,13.17,13.80 Strope 2018 ## All.Species DTXSID.Reference Formula.Reference Human.Clint ## 2971-36-0 Human EPA EPA 136.5 ## 94-75-7 Human|Rat EPA EPA 0 ## 94-82-6 Human EPA EPA 0 ## 90-43-7 Human EPA EPA 2.077 ## 1007-28-9 Human EPA EPA 0 ## 71751-41-2 Human EPA EPA 5.24 ## Human.Clint.pValue Human.Clint.pValue.Reference ## 2971-36-0 0.0000357 Wetmore 2012 ## 94-75-7 0.1488000 Wetmore 2012 ## 94-82-6 0.1038000 Wetmore 2012 ## 90-43-7 0.1635000 Wetmore 2012 ## 1007-28-9 0.5387000 Wetmore 2012 ## 71751-41-2 0.0009170 Wetmore 2012 ## Human.Clint.Reference Human.Fgutabs Human.Fgutabs.Reference ## 2971-36-0 Wetmore 2012 NA &lt;NA&gt; ## 94-75-7 Wetmore 2012 NA &lt;NA&gt; ## 94-82-6 Wetmore 2012 NA &lt;NA&gt; ## 90-43-7 Wetmore 2012 NA &lt;NA&gt; ## 1007-28-9 Wetmore 2012 NA &lt;NA&gt; ## 71751-41-2 Wetmore 2012 NA &lt;NA&gt; ## Human.Funbound.plasma Human.Funbound.plasma.Reference ## 2971-36-0 0 Wetmore 2012 ## 94-75-7 0.04001 Wetmore 2012 ## 94-82-6 0.006623 Wetmore 2012 ## 90-43-7 0.04105 Wetmore 2012 ## 1007-28-9 0.4588 Wetmore 2012 ## 71751-41-2 0.06687 Wetmore 2012 ## Human.Rblood2plasma Human.Rblood2plasma.Reference ## 2971-36-0 NA &lt;NA&gt; ## 94-75-7 2.11 TNO ## 94-82-6 NA &lt;NA&gt; ## 90-43-7 NA &lt;NA&gt; ## 1007-28-9 NA &lt;NA&gt; ## 71751-41-2 NA &lt;NA&gt; ## Mouse.Funbound.plasma Mouse.Funbound.plasma.Reference ## 2971-36-0 &lt;NA&gt; &lt;NA&gt; ## 94-75-7 &lt;NA&gt; &lt;NA&gt; ## 94-82-6 &lt;NA&gt; &lt;NA&gt; ## 90-43-7 &lt;NA&gt; &lt;NA&gt; ## 1007-28-9 &lt;NA&gt; &lt;NA&gt; ## 71751-41-2 &lt;NA&gt; &lt;NA&gt; ## Rabbit.Funbound.plasma Rabbit.Funbound.plasma.Reference Rat.Clint ## 2971-36-0 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 94-75-7 &lt;NA&gt; &lt;NA&gt; 0 ## 94-82-6 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 90-43-7 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 1007-28-9 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 71751-41-2 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## Rat.Clint.pValue Rat.Clint.pValue.Reference Rat.Clint.Reference ## 2971-36-0 NA &lt;NA&gt; &lt;NA&gt; ## 94-75-7 0.1365 Wetmore 2013 Wetmore 2013 ## 94-82-6 NA &lt;NA&gt; &lt;NA&gt; ## 90-43-7 NA &lt;NA&gt; &lt;NA&gt; ## 1007-28-9 NA &lt;NA&gt; &lt;NA&gt; ## 71751-41-2 NA &lt;NA&gt; &lt;NA&gt; ## Rat.Fgutabs Rat.Fgutabs.Reference Rat.Funbound.plasma ## 2971-36-0 NA &lt;NA&gt; &lt;NA&gt; ## 94-75-7 NA &lt;NA&gt; 0.02976 ## 94-82-6 NA &lt;NA&gt; &lt;NA&gt; ## 90-43-7 NA &lt;NA&gt; &lt;NA&gt; ## 1007-28-9 NA &lt;NA&gt; &lt;NA&gt; ## 71751-41-2 NA &lt;NA&gt; &lt;NA&gt; ## Rat.Funbound.plasma.Reference Rat.Rblood2plasma ## 2971-36-0 &lt;NA&gt; NA ## 94-75-7 Wetmore 2013 NA ## 94-82-6 &lt;NA&gt; NA ## 90-43-7 &lt;NA&gt; NA ## 1007-28-9 &lt;NA&gt; NA ## 71751-41-2 &lt;NA&gt; NA ## Rat.Rblood2plasma.Reference SMILES.desalt.Reference Chemical.Class ## 2971-36-0 &lt;NA&gt; EPA ## 94-75-7 &lt;NA&gt; EPA ## 94-82-6 &lt;NA&gt; EPA ## 90-43-7 &lt;NA&gt; EPA ## 1007-28-9 &lt;NA&gt; EPA ## 71751-41-2 &lt;NA&gt; EPA The table contains chemical identifiers: name, CASRN (Chemical Abstract Service Registry Number), and DTXSID (DSSTox ID, a chemical identifier from the EPA Distributed Structure-Searchable Toxicity Database, DSSTox for short – more information can be found at https://www.epa.gov/chemical-research/distributed-structure-searchable-toxicity-dsstox-database). The table also contains physical-chemical properties for each chemical. These are used in predicting tissue partitioning. The table contains in vitro measured chemical-specific TK parameters, if available. These chemical-specific parameters include intrinsic hepatic clearance (Clint) and fraction unbound to plasma protein (Funbound.plasma) for each chemical. It also contains measured values for oral absorption fraction Fgutabs, and for the partition coefficient between blood and plasma Rblood2plasma, if these values have been measured for a given chemical. If available, there may be chemical-specific TK values for multiple species. Listing chemicals for which a TK model can be parameterized You can easily get a list of all the chemicals for which a specific TK model can be parameterized (for a given species, if needed) using the function get_cheminfo(). For example, here is how you get a list of all the chemicals for which the PBTK model can be parameterized for humans. chems_pbtk &lt;- get_cheminfo(info = c(&quot;Compound&quot;, &quot;CAS&quot;, &quot;DTXSID&quot;), model = &quot;pbtk&quot;, species = &quot;Human&quot;) head(chems_pbtk) # First few rows ## Compound CAS DTXSID ## 1 2,4-d 94-75-7 DTXSID0020442 ## 2 2,4-db 94-82-6 DTXSID7024035 ## 3 2-phenylphenol 90-43-7 DTXSID2021151 ## 4 6-desisopropylatrazine 1007-28-9 DTXSID0037495 ## 5 Abamectin 71751-41-2 DTXSID8023892 ## 6 Acephate 30560-19-1 DTXSID8023846 How many such chemicals have parameter data to run a PBTK model in this package? nrow(chems_pbtk) ## [1] 859 Here is how you get all the chemicals for which the 3-compartment steady-state model can be parameterized for humans. chems_3compss &lt;- get_cheminfo(info = c(&quot;Compound&quot;, &quot;CAS&quot;, &quot;DTXSID&quot;), model = &quot;3compartmentss&quot;, species = &quot;Human&quot;) How many such chemicals have parameter data to run a 3-compartment steady-state model in this package? nrow(chems_3compss) ## [1] 943 The 3-compartment steady-state model can be parameterized for a few more chemicals than the PBTK model, because it is a simpler model and requires less data to parameterize. Specifically, the 3-compartment steady-state model does not require estimating tissue partition coefficients, unlike the PBTK model. Solving Toxicokinetic Models to Obtain Internal Chemical Concentration vs. Time Predictions You can solve any of the models for a specified chemical and specified dosing protocol, and get concentration vs. time predictions, using the function solve_model(). For example: sol_pbtk &lt;- solve_model(chem.name = &quot;Bisphenol-A&quot;, # Chemical to simulate model = &quot;pbtk&quot;, # TK model to use dosing = list(initial.dose = NULL, # For repeated dosing, if first dose is different from the rest, specify first dose here doses.per.day = 1, # Number of doses per day daily.dose = 1, # Total daily dose in mg/kg units dosing.matrix = NULL), # Used to specify more complicated dosing protocols days = 1) # Number of days to simulate ## None of the monitored components undergo unit conversions (i.e. conversion factor of 1). ## ## AUC is area under the plasma concentration curve in uM*days units with Rblood2plasma = 0.795. ## The model outputs are provided in the following units: ## umol: Agutlumen, Atubules, Ametabolized ## uM: Cgut, Cliver, Cven, Clung, Cart, Crest, Ckidney, Cplasma ## uM*days: AUC There are some cryptic-sounding warnings that can safely be ignored. (They are providing information about certain assumptions that were made while solving the model). Then there is a final message providing the units of the output. The output, assigned to sol_pbtk, is a matrix with concentration vs. time data for each of the compartments in the pbtk model. Time is in units of days. Additionally, the output traces the amount excreted via passive renal filtration (Atubules), the amount metabolized in the liver (Ametabolized), and the cumulative area under the curve for plasma concentration vs. time (AUC). AUC is the area under the curve of the plasma concentration. Here are the first few rows of sol_pbtk so you can see the format. head(sol_pbtk) ## time Agutlumen Cgut Cliver Cven Clung Cart Crest Ckidney ## [1,] 0.00000 0.00 0.0000 0.000e+00 0.000e+00 0.000 0.0000 0.00000 0.00 ## [2,] 0.00001 306.40 0.1449 4.391e-05 5.000e-09 0.000 0.0000 0.00000 0.00 ## [3,] 0.01042 177.80 72.1600 2.380e+01 2.859e-01 2.353 0.2491 0.06595 3.14 ## [4,] 0.02083 103.10 73.2600 4.919e+01 6.795e-01 5.928 0.6513 0.41400 12.38 ## [5,] 0.03125 59.77 59.4500 5.912e+01 8.971e-01 7.970 0.8854 0.97920 19.65 ## [6,] 0.04167 34.66 45.5300 5.797e+01 9.545e-01 8.546 0.9539 1.60500 22.77 ## Cplasma Atubules Ametabolized AUC ## [1,] 0.000e+00 0.00000 0.000e+00 0.000000 ## [2,] 6.000e-09 0.00000 2.000e-09 0.000000 ## [3,] 3.596e-01 0.00197 1.104e+00 0.001391 ## [4,] 8.548e-01 0.01985 5.485e+00 0.007854 ## [5,] 1.129e+00 0.05810 1.192e+01 0.018390 ## [6,] 1.201e+00 0.10840 1.879e+01 0.030660 You can plot the results, for example plasma concentration vs. time. sol_pbtk &lt;- as.data.frame(sol_pbtk) # Because ggplot2 requires data.frame input, not matrix ggplot(sol_pbtk) + geom_line(aes(x = time, y = Cplasma)) + theme_bw() + xlab(&quot;Time, days&quot;) + ylab(&quot;Cplasma, uM&quot;) + ggtitle(&quot;Plasma concentration vs. time for single dose 1 mg/kg Bisphenol-A&quot;) Calculating summary metrics of internal dose produced from TK models We can calculate summary metrics of internal dose – peak concentration, average concentration, and AUC – using the function calc_tkstats(). We have to specify the dosing protocol and length of simulation. Here, we use the same dosing protocol and simulation length as in the plot above. tkstats &lt;- calc_tkstats(chem.name = &quot;Bisphenol-A&quot;, # Chemical to simulate stats = c(&quot;AUC&quot;, &quot;peak&quot;, &quot;mean&quot;), # Which metrics to return (these are the only three choices) model = &quot;pbtk&quot;, # Model to use tissue = &quot;plasma&quot;, # Tissue for which to return internal dose metrics days = 1, # Length of simulation daily.dose = 1, # Total daily dose in mg/kg/day doses.per.day = 1) # Number of doses per day ## Human plasma concentrations returned in uM units. ## AUC is area under plasma concentration curve in uM * days units with Rblood2plasma = 0.7949 . print(tkstats) ## $AUC ## [1] 0.5299 ## ## $peak ## [1] 1.201 ## ## $mean ## [1] 0.5299 With this, we can answer Environmental Health Question 1: After solving the TK model that evaluates bisphenol-A, what is the maximum concentration of bisphenol-A estimated to occur in human plasma, after 1 exposure dose of 1 mg/kg/day? Answer: The peak plasma concentration estimate for bisphenol-A, under the conditions tested, is 1.25 uM. Calculating steady-state concentration Another summary metric is the steady-state concentration: If the same dose is given repeatedly over many days, the body concentration will (usually) reach a steady state after some time. The value of this steady-state concentration, and the time needed to achieve steady state, are different for different chemicals. Steady-state concentrations are useful when considering long-term, low=level exposures, which is frequently the situation in environmental health. For example, here is a plot of plasma concentration vs. time for 1 mg/kg/day Bisphenol-A, administered for 12 days. You can see how the average plasma concentration reaches a steady state around 1.5 uM. Each peak represents one day’s dose. plasma_conc_plot &lt;- as.data.frame(solve_pbtk( chem.name=&#39;Bisphenol-A&#39;, daily.dose=1, days=12, doses.per.day=1, tsteps=2)) ## None of the monitored components undergo unit conversions (i.e. conversion factor of 1). ## ## AUC is area under the plasma concentration curve in uM*days units with Rblood2plasma = 0.795. ## The model outputs are provided in the following units: ## umol: Agutlumen, Atubules, Ametabolized ## uM: Cgut, Cliver, Cven, Clung, Cart, Crest, Ckidney, Cplasma ## uM*days: AUC ggplot(plasma_conc_plot) + geom_line(aes(x = time, y= Cplasma)) + scale_x_continuous(breaks = seq(0,12)) + xlab(&quot;Time, days&quot;) + ylab(&quot;Cplasma, uM&quot;) httk includes a function calc_analytic_css() to calculate the steady-state plasma concentration (\\(C_{ss}\\) for short) analytically for each model, for a specified chemical and daily oral dose. This function assumes that the daily oral dose is administered as an oral infusion, rather than a single oral bolus dose – in effect, that the daily dose is divided into many small doses over the day. Therefore, the result of calc_analytic_css() may be slightly different than our previous estimate based on the concentration vs. time plot from a single oral bolus dose every day. Here is the result of calc_analytic_css() for a 1 mg/kg/day dose of bisphenol-A. calc_analytic_css(chem.name = &quot;Bisphenol-A&quot;, daily.dose = 1, output.units = &quot;uM&quot;, model = &quot;pbtk&quot;, concentration = &quot;plasma&quot;) ## Plasma concentration returned in uM units. ## [1] 1.164 With this, we can answer Environmental Health Question 2: After solving the TK model that evaluates bisphenol-A, what is the steady-state concentration of bisphenol-A estimated to occur in human plasma, for a long-term oral infusion dose of 1 mg/kg/day? Answer: The steady-state plasma concentration estimate for bisphenol-A, under the conditions tested, is 1.156 uM. Steady-state concentration is linear with dose for httk models For the TK models included in the httk package, steady-state concentration is linear with dose for a given chemical. The slope of the line is simply the steady-state concentration for a dose of 1 mg/kg/day. This can be shown by solving calc_analytic_css() for several doses, and plotting the dose-\\(C_{ss}\\) points along a line whose slope is equal to \\(C_{ss}\\) for 1 mg/kg/day. # Choose five doses at which to find the Css doses &lt;- c(0.1, # Qll mg/kg/day 0.5, 1.0, 1.5, 2.0) suppressWarnings(bpa_css &lt;- sapply(doses, function(dose) calc_analytic_css(chem.name = &quot;Bisphenol-A&quot;, daily.dose = dose, output.units = &quot;uM&quot;, model = &quot;pbtk&quot;, concentration = &quot;plasma&quot;, suppress.messages = TRUE))) DF &lt;- data.frame(dose = doses, Css = bpa_css) #Plot the results Cssdosefig &lt;- ggplot(DF) + geom_point(aes(x = dose, y = Css), size = 3) + geom_abline( # Plot a straight line intercept = 0, #intercept 0 slope = DF[DF$dose==1, # Slope = Css for 1 mg/kg/day &quot;Css&quot;], linetype = 2 ) + xlab(&quot;Daily dose, mg/kg/day&quot;) + ylab(&quot;Css, uM&quot;) print(Cssdosefig) Reverse Toxicokinetics In the previous TK examples, we started with a specified dosing protocol, then solved the TK models to find the resulting concentration in the body (e.g., in plasma). This allows us to convert from external exposure metrics to internal exposure metrics. However, many environmental health questions require the reverse: converting from internal exposure metrics to external exposure metrics. For example, when health effects of environmental chemicals are studied in epidemiological cohorts, adverse health effects are often related to internal exposure metrics, such as blood or plasma concentration of a chemical. Similarly, in vitro studies of chemical bioactivity (for example, the ToxCast program) relate bioactivity to in vitro concentration, which can be consdered analogous to internal exposure or body concentration. So we may know the internal exposure level associated with some adverse health effect of a chemical. However, risk assessors and risk managers typically control external exposure to reduce the risk of adverse health effects. They need some way to start from an internal exposure associated with adverse health effects, and convert to the corresponding external exposure. The solution is reverse toxicokinetics (reverse TK). Starting with a specified internal exposure metric (body concentration), solve the TK model in reverse to find the corresponding external exposure that produced that concentration. When exposures are long-term and low-level (as environmental exposures often are), then the relevant internal exposure metric is the steady-state concentration. In this case, it is useful to remember the linear relationship between \\(C_{ss}\\) and dose for the httk TK models. It gives you a quick and easy way to perform reverse TK for the steady-state case. The procedure is illustrated graphically below. Begin with a “target” concentration on the y-axis (labeled \\(C_{\\textrm{target}}\\)). For example, \\(C_{\\textrm{target}}\\) may be the in vitro concentration associated with bioactivity in a ToxCast assay, or the plasma concentration associated with an adverse health effect in an epidemiological study. Draw a horizontal line over to the \\(C_{ss}\\)-dose line. Drop down vertically to the x-axis and read off the corresponding dose. This is the administered equivalent dose (AED): the the external dose or exposure rate, in mg/kg/day, that would produce an internal steady-state plasma concentration equal to the target concentration. Mathematically, the relation is very simple: \\[ AED = \\frac{C_{\\textrm{target}}}{C_{ss}\\textrm{-dose slope}} \\] Since the \\(C_{ss}\\)-dose slope is simply \\(C_{ss}\\) for a daily dose of 1 mg/kg/day, this equation can be rewritten as \\[ AED = \\frac{C_{\\textrm{target}}}{C_{ss}\\textrm{ for 1 mg/kg/day}} \\] Capturing Population Variability and Uncertainty For a given dose, \\(C_{ss}\\) is determined by the values of the parameters of the TK model. These parameters describe absorption, distribution, metabolism, and excretion (ADME) of each chemical. They include both chemical-specific parameters, describing hepatic clearance and protein binding, and chemical-independent parameters, describing physiology. A table of these parameters is presented below. paramtable &lt;- data.frame(&quot;Parameter&quot; = c(&quot;Intrinsic hepatic clearance rate&quot;, &quot;Fraction unbound to plasma protein&quot;, &quot;Tissue:plasma partition coefficients&quot;, &quot;Tissue masses&quot;, &quot;Tissue blood flows&quot;, &quot;Glomerular filtration rate&quot;, &quot;Hepatocellularity&quot;), &quot;Details&quot; = c(&quot;Rate at which liver removes chemical from blood&quot;, &quot;Free fraction of chemical in plasma&quot;, &quot;Ratio of concentration in body tissues to concentration in plasma&quot;, &quot;Mass of each body tissue (including total body weight)&quot;, &quot;Blood flow rate to each body tissue&quot;, &quot;Rate at which kidneys remove chemical from blood&quot;, &quot;Number of cells per mg liver&quot;), &quot;Estimated&quot; = c(&quot;Measured *in vitro*&quot;, &quot;Measured *in vitro*&quot;, &quot;Estimated from chemical and tissue properties&quot;, rep(&quot;From anatomical literature&quot;, 4) ), &quot;Type&quot; = c(rep(&quot;Chemical-specific&quot;, 3), rep(&quot;Chemical-independent&quot;, 4)) ) knitr::kable(paramtable) Parameter Details Estimated Type Intrinsic hepatic clearance rate Rate at which liver removes chemical from blood Measured in vitro Chemical-specific Fraction unbound to plasma protein Free fraction of chemical in plasma Measured in vitro Chemical-specific Tissue:plasma partition coefficients Ratio of concentration in body tissues to concentration in plasma Estimated from chemical and tissue properties Chemical-specific Tissue masses Mass of each body tissue (including total body weight) From anatomical literature Chemical-independent Tissue blood flows Blood flow rate to each body tissue From anatomical literature Chemical-independent Glomerular filtration rate Rate at which kidneys remove chemical from blood From anatomical literature Chemical-independent Hepatocellularity Number of cells per mg liver From anatomical literature Chemical-independent Because these parameters represent physiology and chemical-body interactions, their exact values will vary across individuals in a population, reflecting population physiological variability. Additionally, parameters are subject to measurement uncertainty. Since the \\(C_{ss}\\)-dose relation is determined by these parameters, variability and uncertainty in the TK parameters translates directly into variability and uncertainty in \\(C_{ss}\\) for a given dose. In other words, there is a distribution of \\(C_{ss}\\) values for each daily dose level of a chemical. The \\(C_{ss}\\)-dose relationship is still linear when variability and uncertainty are taken into account. However, rather than a single \\(C_{ss}\\)-dose slope, there is a distribution of \\(C_{ss}\\)-dose slopes. Because the \\(C_{ss}\\)-dose slope is simply the \\(C_{ss}\\) value for an exposure rate of 1 mg/kg/day, the distribution of the \\(C_{ss}\\)-dose slope is the same as the \\(C_{ss}\\) distribution for an exposure rate of 1 mg/kg/day. A distribution of \\(C_{ss}\\)-dose slopes is illustrated in the figure below, along with boxplots illustrating the distributions for \\(C_{ss}\\) itself at five different dose levels: 0.05, 0.25, 0.5, 0.75, and 0.95 mg/kg/day. Figure 1: Boxplots: Distributions of Css for five daily dose levels of Bisphenol-A. Boxes extend from 25th to 75th percentile. Lower whisker = 5th percentile; upper whisker = 95th percentile. Lines: Css-dose relations for each quantile. Variability and Uncertainty in Reverse Toxicokinetics Earlier, we found that with a linear \\(C_{ss}\\)-dose relation, reverse toxicokinetics became a matter of a simple linear equation. For a given target concentration – for example, a plasma concentration associated with adverse health effects in vivo, or a concentration associated with bioactivity in vitro – we could predict an AED (administered equivalent dose), the external exposure rate in mg/kg/day that would produce the target concentration at steady state. \\[ AED = \\frac{C_{\\textrm{target}}}{C_{ss}\\textrm{-dose slope}} \\] Since AED depends on the \\(C_{ss}\\)-dose slope, variability and uncertainty in that slope will induce variability and uncertainty in the AED. A distribution of slopes will lead to a distribution of AEDs for the same target concentration. For example, a graphical representation of finding the AED distribution for a target concentration of 1 uM looks like this, for the same arbitrary example chemical used to illustrate the distribution of \\(C_{ss}\\)-dose slopes above. (The lines shown in this plot are the same as the previous plot, but the plot has been “zoomed in” on the y-axis.) The steps are the same as before: Begin with a “target” concentration on the y-axis, here 1 uM. Draw a horizontal line over to intersect each \\(C_{ss}\\)-dose line. Where the horizontal line intersects each \\(C_{ss}\\)-dose line, drop down vertically to the x-axis and read off each corresponding AED (marked with colored circles matching the color of each \\(C_{ss}\\)-dose line). Notice that the line with the steepest, 95th-percentile slope (the purple line) yields the lowest AED (the purple dot, approximately 0.07 mg/kg/day for this example chemical), and the line with the shallowest, 5th-percentile slope (the turquoise blue line) yields the highest AED (the turquoise dot, approximately 2 mg/kg/day for this example chemical). In general, the 95th-percentile \\(C_{ss}\\)-dose slope represents the most-sensitive 5% of the population – individuals who will reach the target concentration in their body with the smallest daily doses. Therefore, using the AED for the 95th-percentile \\(C_{ss}\\)-dose slope is a conservative choice, health-protective for 95% of the estimated population. Monte Carlo Approach The httk package implements a Monte Carlo approach for simulating variability and uncertainty in TK. httk first defines distributions for the TK model parameters, representing population variabilty. These distributions are defined based on real data about U.S. population demographics and physiology collected as part of the Centers for Disease Control’s National Health and Nutrition Examination Survey (NHANES) (Ring et al., 2017). TK parameters with known measurement uncertainty (intrinsic hepatic clearance rate and fraction of chemical unbound in plasma) additionally have distributions defined to represent their uncertainty (Wambaugh et al., 2019). Then, httk samples sets of TK parameter values from these distributions (including appropriate correlations: for example, liver mass is correlated with body weight). Each sampled set of TK parameter values represents one “simulated individual.” Next, httk calculates the \\(C_{ss}\\)-dose slope for each “simulated individual.” The resulting sample of \\(C_{ss}\\)-dose slopes can be used to characterize the distribution of \\(C_{ss}\\)-dose slopes – for example, by calculating percentiles. httk makes this whole Monte Carlo process simple and transparent for the user, You just need to call one function, calc_mc_css(), specifying the chemical whose \\(C_{ss}\\)-dose slope distribution you want to calculate. Behind the scenes, httk will perform all the Monte Carlo calculations. It will return percentiles of the \\(C_{ss}\\)-dose slope (by default), or it can return all individual samples of \\(C_{ss}\\)-dose slope (if you want to do some calculations of your own). Chemical-Specific Example Here is an example of capturing population variability for bisphenol-A plasma concentration estimates. The following code estimates the 5th percentile, 50th percentile, and 95th percentile of the \\(C_{ss}\\)-dose slope for the chemical bisphenol-A. For the sake of simplicity, we will use the 3-compartment steady-state model (rather than the PBTK model used in the previous examples). css_examp &lt;- calc_mc_css(chem.name = &quot;Bisphenol-A&quot;, which.quantile = c(0.05, # Specify which quantiles to return 0.5, 0.95), model = &quot;3compartmentss&quot;, # Which model to use to calculate Css output.units = &quot;uM&quot;) # Could also choose mg/Lo ## Human plasma concentration returned in uM units for 0.05 0.5 0.95 quantile. print(css_examp) ## 5% 50% 95% ## 0.1123 0.4752 3.6900 Recall that the \\(C_{ss}\\)-dose slope is the same as \\(C_{ss}\\) for a daily dose of 1 mg/kg/day. The function calc_mc_css() therefore assumes a dose of 1 mg/kg/day and calculates the resulting \\(C_{ss}\\) distribution. If you need to calculate the \\(C_{ss}\\) distribution for a different dose, e.g. 2 mg/kg/day, you can simply multiply the \\(C_{ss}\\) percentiles from calc_mc_css() by your desired dose. The steady-state plasma concentration for 1 mg/kg/day dose is returned in units of uM. The three requested quantiles are returned as a named numeric vector (whose names in this case are 5%, 50%, and 95%). With this, we can answer Environmental Health Question 3: What is the predicted range of bisphenol-A concentrations in plasma that can occur in a human population, assuming a long-term exposure rate of 1 mg/kg/day and steady-state conditions? Provide estimates at the 5th, 50th, and 95th percentile? Answer: For a human population exposed to 1 mg/kg/day bisphenol-A, plasma concentrations are estimated to be 0.5125 uM at the 5th percentile, 2.07 uM at the 50th percentile, and 14.2 uM at the 95th percentile. High-Throughput Example Capturing Population Variability for ~1000 Chemicals We can easily and (fairly) quickly do this for all 998 chemicals for which the 3-compartment steady-state model can be parameterized, using sapply() to loop over the chemicals. This will take a few minutes to run (for example, it takes about 10-15 minutes on a Dell Latitude with an Intel i7 processor). In order to make the Monte Carlo sampling reproducible, set a seed for the random number generator. It doesn’t matter what seed you choose – it can be any integer. Here, the seed is set to 42, because it’s the answer to the ultimate question of life, the universe, and everything (Adams, 1979). set.seed(42) system.time( suppressWarnings( css_3compss &lt;- sapply(chems_3compss$CAS, calc_mc_css, # Additional arguments to calc_mc_css() model = &quot;3compartmentss&quot;, which.quantile = c(0.05, 0.5, 0.95), output.units = &quot;uM&quot;, suppress.messages = TRUE) ) ) ## user system elapsed ## 540.800 17.389 560.436 Organizing the results: # Css_3compss comes out as a 3 x 998 array, # Where rows are quantiles and columns are chemicals # Transpose it so that rows are chemicals and columns are quantiles css_3compss &lt;- t(css_3compss) # Convert to data.frame css_3compss &lt;- as.data.frame(css_3compss) # Make a column for CAS, rather than just leaving it as the row names css_3compss$CAS &lt;- row.names(css_3compss) head(css_3compss) #View first few rows ## 5% 50% 95% CAS ## 2971-36-0 0.1256 0.5985 4.346 2971-36-0 ## 94-75-7 4.3830 16.1200 102.100 94-75-7 ## 94-82-6 25.4900 102.0000 648.100 94-82-6 ## 90-43-7 8.0200 27.2800 164.800 90-43-7 ## 1007-28-9 0.4231 1.2750 5.264 1007-28-9 ## 71751-41-2 2.2150 9.5990 59.470 71751-41-2 Plotting the \\(C_{ss}\\)-dose slope distribution quantiles Here, we will plot the resulting concentration distribution quantiles for each chemical, while sorting the chemicals from lowest to highest median value. By default, ggplot2 will plot the chemical CASRNs in alphabetically-sorted order. To force it to plot them in another order, we have to explicitly specify the desired order. The easiest way to do this is to add a column in the data.frame that contains the chemical names as a factor (categorical) variable, whose levels (categories) are explicitly set to be the CASRNs in our desired plotting order. Then we can tell ggplot2 to plot that factor variable on the x-axis, rather than the original CASRN variable. Set the ordering of the chemical CASRNs from lowest to highest median value: chemical_order &lt;- order(css_3compss$`50%`) Create a factor (categorical) CAS column where the factor levels are given by the CASRNs with this ordering. css_3compss$CAS_factor &lt;- factor(css_3compss$CAS, levels = css_3compss$CAS[chemical_order]) For plotting ease, reshape the data.frame into “long” format – rather than having one column for each quantile of the \\(C_{ss}\\) distribution, have a row for each chemical/quantile combination. We use the melt function from the reshape2 package. css_3compss_melt &lt;- melt(css_3compss, id.vars = &quot;CAS_factor&quot;, measure.vars = c(&quot;5%&quot;, &quot;50%&quot;, &quot;95%&quot;), variable.name = &quot;Percentile&quot;, value.name = &quot;Css_slope&quot;) head(css_3compss_melt) ## CAS_factor Percentile Css_slope ## 1 2971-36-0 5% 0.1256 ## 2 94-75-7 5% 4.3830 ## 3 94-82-6 5% 25.4900 ## 4 90-43-7 5% 8.0200 ## 5 1007-28-9 5% 0.4231 ## 6 71751-41-2 5% 2.2150 Plot the slope percentiles. Use a log scale for the y-axis because the slopes span orders of magnitude. Suppress the x-axis labels (the CASRNs) because they are not readable anyway. ggplot(css_3compss_melt) + geom_point(aes(x=CAS_factor, y = Css_slope, color = Percentile)) + scale_color_brewer(palette = &quot;Set2&quot;) + # Use better color scheme than default scale_y_log10() + # Use log scale for y axis xlab(&quot;Chemical&quot;) + ylab(&quot;Css-dose slope (uM per mg/kg/day)&quot;) + annotation_logticks(sides = &quot;l&quot;) + # Add log ticks to y axis theme_bw() + # Plot with white plot background instead of gray theme(axis.text.x = element_blank(), # Suppress x-axis labels panel.grid.major.x = element_blank(), # Suppress vertical grid lines legend.position = c(0.1,0.8) # Place legend in lower right corner ) Chemicals along the x-axis are in order from lowest to highest median (50th percentile) predicted \\(C_{ss}\\)-dose slope. The orange points represent that 50th percentile \\(C_{ss}\\)-dose slope for each chemical. The green points represent the 5th percentile \\(C_{ss}\\)-dose slopes, and the purple points represent the 95th percentile \\(C_{ss}\\)-dose slope for each chemical. Each chemical has one orange point (50th percentile), one green point (5th percentile), and one purple point (95th percentile), characterizing the distribution of \\(C_{ss}\\)-dose slopes across the U.S. population for that chemical. The width of the distribution for each chemical is roughly represented by the vertical distance between the green and purple points for that chemical. With this, we can answer Environmental Health Question 4: Considering the chemicals evaluated in the above TK modeling example, do the \\(C_{ss}\\)-dose slope distributions become wider as the median \\(C_{ss}\\)-dose slope increases? Answer: No – the \\(C_{ss}\\)-dose slope distributions generally become narrower as the median \\(C_{ss}\\)-dose slope increases. This can be seen by looking at the right end of the plot, where the highest-median chemicals are located – the distance between the green points and purple points, representing the 5th and 95th percentiles, are much smaller for these higher-median chemicals. Reverse TK: Calculating Administered Equivalent Doses for ToxCast Bioactive Concentrations As described in an earlier section of this training module, the slope defining the linear relation between \\(C_{ss}\\) and dose is useful for reverse toxicokinetics: converting an internal dose metric to an external dose metric. The internal dose metric may, for example, be a concentration associated with an in vivo health effect, or in vitro bioactivity. Here, we will consider in vitro bioactivity – specifically, from the ToxCast program. ToxCast tests chemicals in multiple concentration-response format across a battery of in vitro assays that measure activity in a wide variety of biological endpoints. If a chemical showed any activity in an assay at any of its tested concentrations, then one metric of concentration associated with bioactivity is AC50 – the concentration at which the assay response is halfway between its minimum and its maximum. The module won’t address the details of how ToxCast determines assay activity and AC50s from raw concentration-response data. There is an entire R package for the ToxCast data processing workflow, called tcpl. If you want to learn more about those details, start here. Lots of information is available if you install the tcpl R package and look at the package vignette; it essentially walks you through the full ToxCast data processing workflow. In this module, we will begin with pre-computed ToxCast AC50 values for various chemicals and assays. We will use httk to convert ToxCast AC50 values into administered equivalent doses (AEDs). Loading ToxCast AC50s The latest public release of ToxCast high-throughput screening assay data can be downloaded here. Previous public releases of ToxCast data included a matrix of AC50s by chemical and assay. The data format of the latest public release does not contain this kind of matrix. So this dataset was pre-processed to prepare a simple data.frame of AC50s for each chemical/assay combination for the purposes of this training module. Read in the pre-processed data set and view the first few rows. toxcast &lt;- read.csv(&quot;Module2_5/Module2_5_TK_Toxcast_Data.csv&quot;) head(toxcast) ## Compound CAS DTXSID aenm ## 1 Acetohexamide 968-81-0 DTXSID7020007 ACEA_ER_80hr ## 2 2-Methoxyaniline hydrochloride 134-29-2 DTXSID8020092 ACEA_ER_80hr ## 3 Sodium L-ascorbate 134-03-2 DTXSID0020105 ACEA_ER_80hr ## 4 Sodium azide 26628-22-8 DTXSID8020121 ACEA_ER_80hr ## 5 Benzotrichloride 98-07-7 DTXSID1020148 ACEA_ER_80hr ## 6 Benzyl acetate 140-11-4 DTXSID0020151 ACEA_ER_80hr ## log10_ac50 ## 1 0.6524155 ## 2 -1.3141432 ## 3 0.8248535 ## 4 1.9839338 ## 5 1.8370790 ## 6 -0.3299611 The columns of this data frame are: Compound: The compound name. CAS: The compound’s CASRN. DTXSID: The compound’s DSSTox Substance ID. aenm: Assay identifier. “aenm” stands for “Assay Endpoint Name.” More information about the ToxCast assays is available on the ToxCast data download page. log10_ac50: The AC50 for the chemical/assay combination on each row, in log10 uM units. How many ToxCast chemicals are in this data set? length(unique(toxcast$DTXSID)) ## [1] 7863 With this, we can answer Environmental Health Question 5: How many chemicals have available AC50 values to evaluate in the current ToxCast/Tox21 high-throughput screening database? Answer: 7863 chemicals. Subsetting the ToxCast Chemicals Not all of the ToxCast chemicals have TK data built into httk such that we can perform reverse TK using the HTTK models. Let’s subset the ToxCast data to include only the chemicals for which we can run the 3-compartment steady-state models. Previously, we used get_cheminfo() to get a list of chemicals for which we could run the 3-compartment steady state model, including the names, CASRNs, and DSSTox IDs of those chemicals. That list is stored in variable chems_3compss, a data.frame with compound name, CASRN, and DTXSID. Now, we can use that chemical list to subset the ToxCast data. toxcast_httk &lt;- subset(toxcast, subset = toxcast$DTXSID %in% chems_3compss$DTXSID) How many chemicals are in this subset? length(unique(toxcast_httk$DTXSID)) ## [1] 852 There were 998 httk chemicals for which we could run the 3-compartment steady-state model; only 911 of them had ToxCast data. Conversely, most of the 7863 ToxCast chemicals do not have TK data in httk such that we can run the 3-compartment steady state model. Identifying the Lower-Bound ToxCast/Tox21 screens chemicals across multiple assays, such that each chemical has multiple resulting AC50 values, spanning a range of values. For example, here are boxplots of the AC50s for the first 20 chemicals listed in chems_3compss. Note that the chemical identifiers, DTXSID, are used here in these visualizations to represent unique chemicals. ggplot(toxcast_httk[toxcast_httk$DTXSID %in% chems_3compss[1:20, &quot;DTXSID&quot;], ] ) + geom_boxplot(aes(x=DTXSID, y = log10_ac50)) + ylab(&quot;log10 AC50&quot;) + theme_bw() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) Sometimes we have an interest in getting the equivalent dose for an AC50 for one specific assay. For example, if we happen to be interested in estrogen-receptor activity, we might look specifically at one of the assays that measures estrogen receptor activity. However, sometimes we just want a general idea of what concentrations showed bioactivity in any of the ToxCast assays, regardless of the specific biological endpoint of each assay. In this case, typically, we are interested in a “reasonable lower bound” of bioactive concentrations across assays for each chemical. Intuitively, we suspect that the very lowest AC50s for each chemical might represent false activity. Therefore, we often select the tenth percentile of ToxCast AC50s for each chemical as that “reasonable lower bound” on bioactive concentrations. Let’s calculate the tenth percentile ToxCast AC50 for each chemical. Here, we use the base-R function aggregate, which groups a vector (specified in the x argument) by a list of factors (specified in the by argument), and applies a function to each group (specified in the FUN argument). You can add any extra arguments to the FUN function as named arguments to aggregate. toxcast_httk_P10 &lt;- aggregate(x = toxcast_httk$log10_ac50, # Aggregate the AC50s by = list(DTXSID = toxcast_httk$DTXSID), # Group AC50s by DTXSID FUN = quantile, # The function to apply to each group prob = 0.1) # An argument to the quantile() function # By default the names of the output data.frame will be &#39;DTXSID&#39; and &#39;x&#39; # Let&#39;s change &#39;x&#39; to be a more informative name names(toxcast_httk_P10) &lt;- c(&quot;DTXSID&quot;, &quot;log10_ac50_P10&quot;) Let’s transform the tenth-percentile AC50 values back to the natural scale (they are currently on the log10 scale) and put them in a new column AC50. These AC50s will be in uM. toxcast_httk_P10$AC50 &lt;- 10^(toxcast_httk_P10$log10_ac50_P10) View the first few rows: head(toxcast_httk_P10) ## DTXSID log10_ac50_P10 AC50 ## 1 DTXSID0020022 0.8932512 7.82079968 ## 2 DTXSID0020232 0.2903537 1.95143342 ## 3 DTXSID0020286 -1.3763735 0.04203649 ## 4 DTXSID0020311 1.1513461 14.16922669 ## 5 DTXSID0020319 -0.1934652 0.64052306 ## 6 DTXSID0020365 0.4308058 2.69653361 Calculating Equivalent Doses for 10th Percentile ToxCast AC50s We can calculate equivalent doses in one line of R code – again including all of the Monte Carlo for TK uncertainty and variability – just by using the httk function calc_mc_oral_equiv(). Note that in calc_mc_oral_equiv(), the which.quantile argument refers to the quantile of the \\(C_{ss}\\)-dose slope, not the quantile of the equivalent dose itself. So specifying which.quantile = 0.95 will yield a lower equivalent dose than which.quantile = 0.05. Under the hood, calc_mc_oral_equiv() first calls calc_mc_css() to get percentiles of the \\(C_{ss}\\)-dose slope for a chemical. It then divides a user-specified target concentration (specified in argument conc) by each quantile of \\(C_{ss}\\)-dose slope to get the equivalent dose corresponding to that target concentration for each slope quantile. Here, we’re using the mapply() function in base R to call calc_mc_oral_equiv() in a loop over chemicals. This is because calc_mc_oral_equiv() requires two chemical-specific arguments – the chemical identifier and the concentration for which to compute the equivalent dose. mapply() lets us provide vectors of values for each argument (in the named arguments dtxsid and conc), and will automatically loop over those vectors. We also use the argument MoreArgs, a named list of additional arguments to the function in FUN that will be the same for every iteration of the loop. Note that this line of code takes a few minutes to run. set.seed(42) system.time( suppressWarnings( toxcast_equiv_dose &lt;- mapply(FUN = calc_mc_oral_equiv, conc = toxcast_httk_P10$AC50, dtxsid = toxcast_httk_P10$DTXSID, MoreArgs = list(model = &quot;3compartmentss&quot;, # Model to use which.quantile = c(0.05, 0.5, 0.95), # Quantiles of Css-dose slope suppress.messages = TRUE) ) ) ) ## user system elapsed ## 517.187 15.754 535.099 # By default, the results are a 3 x 911 matrix, where rows are quantiles and columns are chemicals toxcast_equiv_dose &lt;- t(toxcast_equiv_dose) # Transpose so that rows are chemicals toxcast_equiv_dose &lt;- as.data.frame(toxcast_equiv_dose) # Convert to data.frame head(toxcast_equiv_dose) # Look at first few rows ## 5% 50% 95% ## 1 0.5492 0.1983 0.06561 ## 2 11.3700 4.6710 1.71800 ## 3 5.5740 1.2590 0.20500 ## 4 196.5000 51.2600 9.38400 ## 5 0.3182 0.1319 0.05079 ## 6 1.9810 0.5406 0.11590 Let’s add the DTXSIDs back into this data.frame. toxcast_equiv_dose$DTXSID &lt;- toxcast_httk_P10$DTXSID We can get the names of these chemicals by using the list of chemicals for which the 3-compartment steady-state model can be parameterized, which was stored in the variable `chems_3compss. In that dataframe, we have the compound name and CASRN corresponding to each DTXSID. head(chems_3compss) ## Compound CAS ## 1 2,2-bis(4-hydroxyphenyl)-1,1,1-trichloroethane (hpte) 2971-36-0 ## 2 2,4-d 94-75-7 ## 3 2,4-db 94-82-6 ## 4 2-phenylphenol 90-43-7 ## 5 6-desisopropylatrazine 1007-28-9 ## 6 Abamectin 71751-41-2 ## DTXSID ## 1 DTXSID8022325 ## 2 DTXSID0020442 ## 3 DTXSID7024035 ## 4 DTXSID2021151 ## 5 DTXSID0037495 ## 6 DTXSID8023892 Merge chems_3compss with toxcast_equiv_dose. toxcast_equiv_dose &lt;- merge(chems_3compss, toxcast_equiv_dose, by = &quot;DTXSID&quot;, all.x = FALSE, all.y = TRUE) head(toxcast_equiv_dose) ## DTXSID Compound CAS 5% 50% 95% ## 1 DTXSID0020022 Acifluorfen 50594-66-6 0.5492 0.1983 0.06561 ## 2 DTXSID0020232 Caffeine 58-08-2 11.3700 4.6710 1.71800 ## 3 DTXSID0020286 3-chloro-4-methylaniline 95-74-9 5.5740 1.2590 0.20500 ## 4 DTXSID0020311 Monuron 150-68-5 196.5000 51.2600 9.38400 ## 5 DTXSID0020319 Chlorothalonil 1897-45-6 0.3182 0.1319 0.05079 ## 6 DTXSID0020365 Cyclosporin a 59865-13-3 1.9810 0.5406 0.11590 To find the chemicals with the lowest equivalent doses at the 95th percentile level (corresponding to the most-sensitive 5% of the population), sort this data.frame in ascending order on the 95% column. toxcast_equiv_dose &lt;- toxcast_equiv_dose[order(toxcast_equiv_dose$`95%`), ] head(toxcast_equiv_dose, 10) # First ten rows of sorted table ## DTXSID Compound CAS 5% ## 713 DTXSID8023216 3,5,3&#39;-triiodothyronine 6893-02-3 1.031e-05 ## 8 DTXSID0020442 2,4-d 94-75-7 1.360e-05 ## 712 DTXSID8023214 Levothyroxine 51-48-9 2.311e-05 ## 759 DTXSID8037594 Secbumeton 26259-45-0 1.582e-04 ## 783 DTXSID9020453 Dieldrin 60-57-1 7.422e-05 ## 302 DTXSID3031862 Perfluorohexanoic acid 307-24-4 2.850e-04 ## 102 DTXSID1021243 Rhodamine 6g 989-38-8 1.472e-04 ## 349 DTXSID4020533 1,4-dioxane 123-91-1 3.833e-04 ## 129 DTXSID1026035 Sodium 2-mercaptobenzothiolate 2492-26-4 7.859e-04 ## 682 DTXSID7047306 Cp-634384 290352-28-2 3.036e-04 ## 50% 95% ## 713 7.750e-07 3.840e-07 ## 8 3.635e-06 6.410e-07 ## 712 8.371e-06 3.905e-06 ## 759 3.655e-05 5.861e-06 ## 783 2.696e-05 8.060e-06 ## 302 7.037e-05 1.053e-05 ## 102 3.309e-05 1.657e-05 ## 349 1.046e-04 1.790e-05 ## 129 1.385e-04 1.899e-05 ## 682 8.899e-05 1.932e-05 With this, we can answer Environmental Health Question 6: What are the chemicals with the ten lowest predicted equivalent doses (for tenth-percentile ToxCast AC50s), for the most-sensitive 5% of the population? Answer: 2,4-d; 3,5,3’-triiodothyronine; Octane; 1,4-dioxane; Secbumeton; Dieldrin; Perfluorohexanoic acid; Levothyroxine; Sodium 2-mercaptobenzothiolate; and Imazapyr. Comparing Equivalent Doses Estimated to Elicit Toxicity (Hazard) to External Exposure Estimates (Exposure), for Chemical Prioritization by Bioactivity-Exposure Ratios (BERs) To estimate potential risk, hazard – in the form of the equivalent dose for the 10th percentile Toxcast AC50 – now needs to be compared to exposure. A quantitative metric for this comparison is the ratio of the lowest 5% of equivalent doses to the highest 5% of potential exposures. This metric is termed the Bioactivity-Exposure Ratio, or BER. Lower BER corresponds to higher potential risk. With BERs calculated for each chemical, we can ultimately rank all of the chemicals from lowest to highest BER, to achieve a chemical prioritization based on potential risk. Human Exposure Estimates Here, we will use exposure estimates that have been inferred from CDC NHANES urinary biomonitoring data (Ring et al., 2019). These estimates consist of an estimated median, and estimated upper and lower 95% credible interval bounds representing uncertainty in that estimated median. These estimates are provided here in the following csv file: exposure &lt;- read.csv(&quot;Module2_5/Module2_5_TK_Exposure_Data.csv&quot;) head(exposure) # View first few rows ## Compound ## 1 1,2,3,4,5,6-Hexachlorocyclohexane (mixed isomers) ## 2 1,2,4-Trichlorobenzene ## 3 1,3,5-Trichlorobenzene ## 4 1,3-Dichlorobenzene ## 5 1,4-Dichlorobenzene ## 6 2,3-Dihydro-2,2-dimethyl-7-benzofuryl 2,4-dimethyl-6-oxa-5-oxo-3-thia-2,4-diazadecanoate ## DTXSID CAS Median low95 up95 ## 1 DTXSID7020687 608-73-1 1.237622e-07 1.144743e-10 8.464811e-06 ## 2 DTXSID0021965 120-82-1 1.157387e-08 5.005691e-11 2.950528e-07 ## 3 DTXSID8026195 108-70-3 8.970557e-08 1.292361e-10 2.563596e-06 ## 4 DTXSID6022056 541-73-1 9.802174e-08 9.421797e-11 8.343616e-06 ## 5 DTXSID1020431 106-46-7 9.050628e-05 8.456633e-05 9.731353e-05 ## 6 DTXSID3052725 65907-30-4 4.245608e-08 1.070856e-10 1.236776e-06 Merging Exposure Estimates with Equivalent Dose Estimates of Toxicity (Hazard) To calculate a BER for a chemical, it needs to have both an equivalent dose and an exposure estimate. Not all of the chemicals for which equivalent doses could be computed (i.e., chemicals with both ToxCast AC50s and httk data) also have exposure estimates inferred from NHANES. Find out how many do. length(intersect(toxcast_equiv_dose$DTXSID, exposure$DTXSID)) ## [1] 58 This means that, using the ToxCast AC50 data for bioactive concentrations, the NHANES urinary inference data for exposures, and the httk package to convert bioactive concentrations to equivalent doses, we can compute BERs for 58 chemicals. Merge together the ToxCast equivalent doses and the exposure data into a single data frame. Keep only the chemicals that have data in both ToxCast equivalent doses and exposure data frames. hazard_exposure &lt;- merge(toxcast_equiv_dose, exposure, by = &quot;DTXSID&quot;, all = FALSE) head(hazard_exposure) # View first few rows of result ## DTXSID Compound.x CAS.x 5% 50% 95% ## 1 DTXSID0020442 2,4-d 94-75-7 1.360e-05 3.635e-06 6.410e-07 ## 2 DTXSID0021389 Trichlorfon 52-68-6 6.929e+01 1.770e+01 2.799e+00 ## 3 DTXSID0024266 Pirimiphos-methyl 29232-93-7 2.255e+01 5.362e+00 9.796e-01 ## 4 DTXSID1020855 Methyl parathion 298-00-0 1.125e+01 2.169e+00 2.634e-01 ## 5 DTXSID1021956 Di-n-octyl phthalate 117-84-0 6.022e-04 3.029e-04 1.552e-04 ## 6 DTXSID1022265 Alachlor 15972-60-8 5.953e+01 1.261e+01 1.732e+00 ## Compound.y CAS.y Median low95 ## 1 2,4-Dichlorophenoxyacetic acid 94-75-7 6.349713e-06 3.100467e-06 ## 2 Trichlorfon 52-68-6 5.021397e-08 8.309014e-11 ## 3 Pirimiphos-methyl 29232-93-7 2.569640e-07 1.765961e-10 ## 4 Methyl parathion 298-00-0 7.396964e-08 1.559956e-10 ## 5 Dioctyl phthalate 117-84-0 8.039695e-05 7.674705e-05 ## 6 Alachlor 15972-60-8 2.249506e-07 1.325551e-07 ## up95 ## 1 1.815981e-05 ## 2 3.302746e-06 ## 3 6.640505e-05 ## 4 3.575740e-06 ## 5 8.422537e-05 ## 6 3.111993e-07 Plotting Hazard and Exposure Together We can visually compare the equivalent doses and the inferred exposure estimates by plotting them together. ggplot(hazard_exposure) + geom_crossbar(aes(x = Compound.x, # Boxes for equivalent doses y = `50%`, ymax = `5%`, ymin = `95%`, color = &quot;Equiv. dose&quot;)) + geom_crossbar(aes( x= Compound.x, # Boxes for exposures y = Median, ymax = up95, ymin = low95, color = &quot;Exposure&quot;)) + scale_color_manual(values = c(&quot;Equiv. dose&quot; = &quot;black&quot;, &quot;Exposure&quot; = &quot;Orange&quot;), name = NULL) + scale_x_discrete(label = function(x) str_trunc(x, 20) ) + # Truncate chemical names to 20 chars scale_y_log10() + annotation_logticks(sides = &quot;l&quot;) + ylab(&quot;Equiv. dose or Exposure, mg/kg/day&quot;) + theme_bw() + theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 6), axis.title.x = element_blank(), legend.position = &quot;top&quot;) Calculating Bioactivity-Exposure Ratios (BERs) The bioactivity-exposure ratio (BER) is simply the ratio of the lower-end equivalent dose (for the most-sensitive 5% of the population) divided by the upper-end estimated exposure (here, the upper bound on the inferred population median exposure). In the data frame hazard_exposure containing the hazard and exposure data, the lower-end equivalent dose is in column 95% (corresponding to the 95th-percentile \\(C_{ss}\\)-dose slope) and the upper-end exposure is in column up95. Calculate the BER, and assign the result to a new column in the hazard_exposure data frame called BER. hazard_exposure[[&quot;BER&quot;]] &lt;- hazard_exposure[[&quot;95%&quot;]]/hazard_exposure[[&quot;up95&quot;]] Prioritizing Chemicals by BER To prioritize chemicals according to potential risk, they can be sorted from lowest to highest BER. The lower the BER, the higher the priority. Sort the rows of the data.frame from lowest to highest BER. hazard_exposure &lt;- hazard_exposure[order(hazard_exposure$BER), ] head(hazard_exposure) ## DTXSID Compound.x CAS.x 5% 50% ## 1 DTXSID0020442 2,4-d 94-75-7 0.0000136 3.635e-06 ## 29 DTXSID5020607 Diethylhexyl phthalate (dehp) 117-81-7 0.0016980 8.292e-04 ## 5 DTXSID1021956 Di-n-octyl phthalate 117-84-0 0.0006022 3.029e-04 ## 19 DTXSID3022455 Dimethyl phthalate 131-11-3 0.0041230 1.457e-03 ## 13 DTXSID2024086 Ethion 563-12-2 0.0224200 4.747e-03 ## 25 DTXSID4022529 Methylparaben 99-76-3 9.1260000 1.970e+00 ## 95% Compound.y CAS.y Median low95 ## 1 6.410e-07 2,4-Dichlorophenoxyacetic acid 94-75-7 6.349713e-06 3.100467e-06 ## 29 4.290e-04 Di(2-ethylhexyl) phthalate 117-81-7 9.343466e-04 9.133541e-04 ## 5 1.552e-04 Dioctyl phthalate 117-84-0 8.039695e-05 7.674705e-05 ## 19 2.832e-04 Dimethyl phthalate 131-11-3 1.413887e-05 1.314067e-05 ## 13 1.002e-03 Ethion 563-12-2 9.440419e-08 1.456731e-10 ## 25 2.784e-01 Methylparaben 99-76-3 9.525392e-04 8.949158e-04 ## up95 BER ## 1 1.815981e-05 0.03529773 ## 29 9.546859e-04 0.44936245 ## 5 8.422537e-05 1.84267523 ## 19 1.520612e-05 18.62408093 ## 13 4.885048e-06 205.11569391 ## 25 1.013307e-03 274.74404382 The hazard-exposure plot above showed chemicals in alphabetical order. It can be revised to show chemicals in order of priority, from lowest to highest BER. First, create a categorical (factor) variable for the compound names, whose levels are in order of increasing BER. (Since we already sorted the data.frame in order of increasing BER, we can just take the compound names in the order that they appear.) hazard_exposure$Compound_factor &lt;- factor(hazard_exposure$Compound.x, levels = hazard_exposure$Compound.x) Now, make the same plot as before, but use Compound_factor as the x-axis variable instead of Compound. ggplot(hazard_exposure) + geom_crossbar(aes(x = Compound_factor, # Boxes for equivalent dose y = `50%`, ymax = `5%`, ymin = `95%`, color = &quot;Equiv. dose&quot;)) + geom_crossbar(aes( x= Compound_factor, # Boxes for exposure y = Median, ymax = up95, ymin = low95, color = &quot;Exposure&quot;)) + scale_color_manual(values = c(&quot;Equiv. dose&quot; = &quot;black&quot;, &quot;Exposure&quot; = &quot;Orange&quot;), name = NULL) + scale_x_discrete(label = function(x) str_trunc(x, 20) ) + # Truncate chemical names scale_y_log10() + ylab(&quot;Equiv. dose or Exposure, mg/kg/day&quot;) + annotation_logticks(sides = &quot;l&quot;) + theme_bw() + theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 6), axis.title.x = element_blank(), legend.position = &quot;top&quot;) Now, the chemicals are displayed in order of increasing BER. From left to right, you can visually see the distance increase between the lower bound of equivalent doses (the bottom of the black boxes) and the upper bound of exposure estimates (the top of the orange boxes). Since the y-axis is put on a log10 scale, the distance between the boxes corresponds to the BER. We can gather a lot of information from this plot! With this, we can answer Environmental Health Question 7: Based on httk modeling estimates, are chemicals with higher bioactivity-exposure ratios always less potent than chemicals with lower bioactivity-exposure ratios? Answer: No – some chemicals with high potency (low equivalent doses) demonstrate high BERs because they have relatively low human exposure estimates; and vice versa. With this, we can answer Environmental Health Question 8: Based on httk modeling estimates, do chemicals with higher bioactivity-exposure ratios always have lower estimated exposures than chemicals with lower bioactivity-exposure ratios? Answer: No – some chemicals with high estimated exposures have equivalent doses that are higher still, resulting in a high BER despite the higher estimated exposure. Likewise, some chemicals with low estimated exposures also have lower equivalent doses, resulting in a low BER despite the low estimated exposure. With this, we can answer Environmental Health Question 9: How are chemical prioritization results different when using only hazard information vs. only exposure information vs. bioactivity-exposure ratios? Answer: When chemicals are prioritized solely on the basis of hazard, more-potent chemicals will be highly prioritized. However, if humans are never exposed to these chemicals, or exposure is extremely low compared to potency, then despite the high potency, the potential risk may be low. Conversely, if chemicals are prioritized solely on the basis of exposure, then ubiquitous chemicals will be highly prioritized. However, if these chemicals are inert and do not produce adverse effects, then despite the high exposure, the potential risk may be low. For these reasons, risk-based chemical prioritization efforts consider both hazard (toxicity) and exposure, for instance through bioactivity-exposure ratios. Filling Hazard and Exposure Data Gaps to Prioritize More Chemicals To calculate a BER for a chemical, both bioactivity and exposure data are required, as well as sufficient TK data to perform reverse TK. In this training module, bioactivity data came from ToxCast AC50s; exposure data consisted of exposure inferences made from NHANES urinary biomonitoring data; and TK data consisted of parameter values measured in vitro and built into the httk R package. The intersections are illustrated in an Euler diagram below. BERs can only be calculated for chemicals in the triple intersection. fit &lt;- eulerr::euler(list(&#39;ToxCast AC50s&#39; = unique(toxcast$DTXSID), &#39;HTTK&#39; = unique(chems_3compss$DTXSID), &#39;NHANES inferred exposure&#39; = unique(exposure$DTXSID) ), shape = &quot;ellipse&quot;) plot(fit, legend = TRUE, quantities = TRUE ) Clearly, it would be useful to gather more data to allow calculation of BERs for more chemicals. With this, we can answer Environmental Health Question 10: Of the three data sets used in this training module – bioactivity from ToxCast, TK data from httk, and exposure inferred from NHANES urinary biomonitoring – which one most limits the number of chemicals that can be prioritized using BERs? Answer: The exposure data set includes the fewest chemicals and is therefore the most limiting. The exposure data set used in this training module is limited to chemicals for which NHANES did urinary biomonitoring for markers of exposure, which is a fairly small set of chemicals that were of interest to NHANES due to existing concerns about health effects of exposure, and/or other reasons. This data set was chosen because it is a convenient set of exposure estimates to use for demonstration purposes, but it could be expanded by including other sources of exposure data and exposure model predictions. Further discussion is beyond the scope of this training module, but as an example of this kind of high-throughput exposure modeling, see Ring et al., 2019. It would additionally be useful to gather TK data for additional chemicals. In vitro measurement efforts are ongoing. Additonally, in silico modeling can produce useful predictions of TK properties to facilitate chemical prioritization. Efforts are ongoing to develop computational models to predict TK parameters from chemical structure and properties. Concluding Remarks This training module provides an overview of toxicokinetic modeling using the httk R package, and its application to in vitro-in vivo extrapolation in the form of placing in vitro data in the context of exposure by calculating equivalent doses for in vitro bioactive concentrations. We would like to acknowledge the developers of the httk package, as detailed below via the CRAN website: This module also summarizes the use of the Bioactivity-Exposure Ratio (BER) for chemical prioritization, and provides examples of calculating the BER and ranking chemicals accordingly. Together, these approaches can be used to more efficiently identify chemicals present in the environment that pose a potential risk to human health. For additional case studies that leverage TK and/or httk modeling techniques, see the following publications that also address environmental health questions: Breen M, Ring CL, Kreutz A, Goldsmith MR, Wambaugh JF. High-throughput PBTK models for in vitro to in vivo extrapolation. Expert Opin Drug Metab Toxicol. 2021 Aug;17(8):903-921. PMID: 34056988. Klaren WD, Ring C, Harris MA, Thompson CM, Borghoff S, Sipes NS, Hsieh JH, Auerbach SS, Rager JE. Identifying Attributes That Influence In Vitro-to-In Vivo Concordance by Comparing In Vitro Tox21 Bioactivity Versus In Vivo DrugMatrix Transcriptomic Responses Across 130 Chemicals. Toxicol Sci. 2019 Jan 1;167(1):157-171. PMID: 30202884. Pearce RG, Setzer RW, Strope CL, Wambaugh JF, Sipes NS. httk: R Package for High-Throughput Toxicokinetics. J Stat Softw. 2017;79(4):1-26. PMID 30220889. Ring CL, Pearce RG, Setzer RW, Wetmore BA, Wambaugh JF. Identifying populations sensitive to environmental chemicals by simulating toxicokinetic variability. Environ Int. 2017 Sep;106:105-118. PMID: 28628784. Ring C, Sipes NS, Hsieh JH, Carberry C, Koval LE, Klaren WD, Harris MA, Auerbach SS, Rager JE. Predictive modeling of biological responses in the rat liver using in vitro Tox21 bioactivity: Benefits from high-throughput toxicokinetics. Comput Toxicol. 2021 May;18:100166. PMID: 34013136. Rotroff DM, Wetmore BA, Dix DJ, Ferguson SS, Clewell HJ, Houck KA, Lecluyse EL, Andersen ME, Judson RS, Smith CM, Sochaski MA, Kavlock RJ, Boellmann F, Martin MT, Reif DM, Wambaugh JF, Thomas RS. Incorporating human dosimetry and exposure into high-throughput in vitro toxicity screening. Toxicol Sci. 2010 Oct;117(2):348-58. PMID: 20639261. Wetmore BA, Wambaugh JF, Ferguson SS, Sochaski MA, Rotroff DM, Freeman K, Clewell HJ 3rd, Dix DJ, Andersen ME, Houck KA, Allen B, Judson RS, Singh R, Kavlock RJ, Richard AM, Thomas RS. Integration of dosimetry, exposure, and high-throughput screening data in chemical toxicity assessment. Toxicol Sci. 2012 Jan;125(1):157-74. PMID: 21948869. Wambaugh JF, Wetmore BA, Pearce R, Strope C, Goldsmith R, Sluka JP, Sedykh A, Tropsha A, Bosgra S, Shah I, Judson R, Thomas RS, Setzer RW. Toxicokinetic Triage for Environmental Chemicals. Toxicol Sci. 2015 Sep;147(1):55-67. PMID: 26085347. Wambaugh JF, Wetmore BA, Ring CL, Nicolas CI, Pearce RG, Honda GS, Dinallo R, Angus D, Gilbert J, Sierra T, Badrinarayanan A, Snodgrass B, Brockman A, Strock C, Setzer RW, Thomas RS. Assessing Toxicokinetic Uncertainty and Variability in Risk Prioritization. Toxicol Sci. 2019 Dec 1;172(2):235-251. doi: 10.1093/toxsci/kfz205. PMID: 31532498. "],["read-across-toxicity-predictions.html", "2.6 Read-Across Toxicity Predictions Introduction to Training Module Read-Across Example Analysis Calculating Chemical Similarities Chemical Read-Across to Predict Acute Toxicity Concluding Remarks", " 2.6 Read-Across Toxicity Predictions This training module was developed by Dr. Grace Patlewicz and Dr. Julia E. Rager Fall 2021 Disclaimer: The views expressed in this document are those of the author and do not necessarily reflect the views or policies of the U.S. EPA. Introduction to Chemical Read-Across The method of read-across represents one type of computational approach that is commonly used to predict a chemical’s toxicological effects using its properties. Other types of approaches that you will hear commonly used in this field include SAR and QSAR analyses. A high-level overview of each of these definitions and simple illustrative examples of these three computational modeling approaches is provided in the following schematic: Focusing more on read-across, this computational approach represents the method of filling a data gap whereby a chemical with existing data values is used to make a prediction for a ‘similar’ chemical, typically one which is structurally similar. Thus, information from chemicals with data is read across to chemical(s) without data. In a typical read-across workflow, the first step is to determine the problem definition - what question are we trying to address. The second step starts the process of identifying chemical analogues that have information that can be used to inform this question, imparting information towards a chemical of interest that is lacking data. A specific type of read-across that is commonly employed is termed ‘Generalized Read-Across’ or GenRA, which is based upon similarity-weighted activity predictions. This type of read-across approach will be used here when conducting the example chemical read-across training module. This approach has been previously described and published: Shah I, Liu J, Judson RS, Thomas RS, Patlewicz G. Systematically evaluating read-across prediction and performance using a local validity approach characterized by chemical structure and bioactivity information. Regul Toxicol Pharmacol. 2016 79:12-24. PMID: 27174420 Introduction to Training Module In this activity we are going to consider a chemical of interest (which we call the target chemical) that is lacking acute oral toxicity information. Specifically, we would like to obtain estimates of the dose that causes lethality after acute (meaning, short-term) exposure conditions. These dose values are typically presented as LD50 values, and are usually collected through animal testing. There is huge interest surrounding the reduced reliance upon animal testing, and we would like to avoid further animal testing as much as possible. With this goal in mind, this activity aims to estimate an LD50 value for the target chemical using completely computational approaches, leveraging existing data as best we can. To achieve this aim, we explore ways in which we can search for structurally similar chemicals that have acute toxicity data already available. Data on these structurally similar chemicals, termed ‘source analogues’, are then used to predict acute toxicity for the target chemical of interest using the GenRA approach. The dataset used for this training module were previously compiled and published in the following manuscript: Helman G, Shah I, Patlewicz G. Transitioning the Generalised Read-Across approach (GenRA) to quantitative predictions: A case study using acute oral toxicity data. Comput Toxicol. 2019 Nov 1;12(November 2019):10.1016/j.comtox.2019.100097. doi: 10.1016/j.comtox.2019.100097. PMID: 33623834; PMCID: PMC7898163. With associated data available at: https://github.com/USEPA/CompTox-GenRA-acutetox-comptoxicol/tree/master/input This exercise will specifically predict LD50 values for the chemical, 1-chloro-4-nitrobenzene (DTXSID5020281). This chemical is an organic compound with the formula ClC6H4NO2, and is a common intermediate in the production of a number of industrial compounds, including common antioxidants found in rubber. Training Module’s Environmental Health Questions This training module was specifically developed to answer the following environmental health questions: How many chemicals with acute toxicity data are structurally similar to 1-chloro-4-nitrobenzene? What is the predicted LD50 for 1-chloro-4-nitrobenzene, using the GenRA approach? How different is the predicted vs. experimentally observed LD50 for 1-chloro-4-nitrobenzene? Script Preparations Cleaning the global environment rm(list=ls()) Installing required R packages If you already have these packages installed, you can skip this step, or you can run the below code which checks installation status for you: if (!requireNamespace(&quot;tidyverse&quot;)) install.packages(&quot;tidyverse&quot;); if (!requireNamespace(&quot;fingerprint&quot;)) install.packages(&quot;fingerprint&quot;); if (!requireNamespace(&quot;rcdk&quot;)) install.packages(&quot;rcdk&quot;); Loading R packages required for this session library(tidyverse) #all tidyverse packages, including dplyr and ggplot2 library(fingerprint) # a package that supports operations on molecular fingerprint data library(rcdk) # a package that interfaces with the &#39;CDK&#39;, a Java framework for chemoinformatics libraries packaged for R Set your working directory setwd(&quot;/filepath to where your input files are&quot;) Read-Across Example Analysis Loading Example Datasets Let’s start by loading the datasets needed for this training module. We are going to use a dataset of substances that have chemical identification information ready in the form of SMILES, as well as acute toxicity data, in the form of LD50 values. The first file to upload is named ‘substances.csv’, and contains the list of substances and their structural information, in the form of SMILES nomenclature. SMILES stands for Simplified molecular-input line-entry system, a form of line notation to describe the structure of a chemical. The second file to upload is named ‘acute_tox_data.csv’, and contains the substances and their acute toxicity information. substances &lt;- read.csv(&quot;Module2_6/Module2_6_Substances.csv&quot;) acute_data &lt;- read.csv(&quot;Module2_6/Module2_6_AcuteToxData.csv&quot;) Data Viewing Let’s first view the substances dataset: dim(substances) ## [1] 6955 4 colnames(substances) ## [1] &quot;DTXSID&quot; &quot;PREFERRED_NAME&quot; &quot;SMILES&quot; ## [4] &quot;QSAR_READY_SMILES&quot; head(substances) ## DTXSID PREFERRED_NAME ## 1 DTXSID00142939 (Acetyloxy)acetonitrile ## 2 DTXSID00143108 Acrylic acid, 2-(hydroxymethyl)-, ethyl ester ## 3 DTXSID00143880 4-Heptyl-2,6-dimethylphenol ## 4 DTXSID00144796 Pyrimidine, 2-amino-4-(2-dimethylaminoethoxy)- ## 5 DTXSID00144933 O,O,O-Tris(2-chloroethyl) phosphorothioate ## 6 DTXSID00146356 Citenamide ## SMILES QSAR_READY_SMILES ## 1 CC(=O)OCC#N CC(=O)OCC#N ## 2 CCOC(=O)C(=C)CO CCOC(=O)C(=C)CO ## 3 CCCCCCCC1=CC(C)=C(O)C(C)=C1 CCCCCCCC1=CC(C)=C(O)C(C)=C1 ## 4 CN(C)CCOC1=NC(N)=NC=C1 CN(C)CCOC1=NC(N)=NC=C1 ## 5 ClCCOP(=S)(OCCCl)OCCCl ClCCOP(=S)(OCCCl)OCCCl ## 6 NC(=O)C1C2=CC=CC=C2C=CC2=CC=CC=C12 NC(=O)C1C2=CC=CC=C2C=CC2=CC=CC=C12 We can see that this dataset contains information on 6955 chemicals (rows), that are represented by DTXSIDs, a substance identifier provided through the U.S. EPA’s Computational Toxicology Dashboard: https://comptox.epa.gov/dashboard. Chemical identifiers are also presented as SMILES and QSAR_READY_SMILES. The QSAR_READY_SMILES values are what we will specifically need in a later step, to construct chemical fingerprints from. QSAR_READY_SMILES refers to SMILES that have been standardized related to salts, tautomers, inorganics, aromaticity, and stereochemistry (among other factors) prior to any QSAR modeling or prediction. Let’s make sure that these values are recognized as character format and placed in its own vector, to ensure proper execution of functions throughout this script: all_smiles &lt;- as.character(substances$QSAR_READY_SMILES) Now let’s view the acute toxicity dataset: dim(acute_data) ## [1] 6955 9 colnames(acute_data) ## [1] &quot;DTXSID&quot; &quot;very_toxic&quot; &quot;nontoxic&quot; &quot;LD50_mgkg&quot; &quot;EPA_category&quot; ## [6] &quot;GHS_category&quot; &quot;casrn&quot; &quot;mol_weight&quot; &quot;LD50_LM&quot; head(acute_data) ## DTXSID very_toxic nontoxic LD50_mgkg EPA_category GHS_category ## 1 DTXSID00142939 TRUE FALSE 32 1 2 ## 2 DTXSID00143108 FALSE FALSE 620 3 4 ## 3 DTXSID00143880 FALSE FALSE 1600 3 4 ## 4 DTXSID00144796 FALSE FALSE 1500 3 4 ## 5 DTXSID00144933 FALSE FALSE 820 3 4 ## 6 DTXSID00146356 FALSE FALSE 1800 3 4 ## casrn mol_weight LD50_LM ## 1 1001-55-4 99.089 0.4908755 ## 2 10029-04-6 130.143 -0.6779709 ## 3 10138-19-9 220.356 -0.8609951 ## 4 102207-77-2 182.227 -0.9154785 ## 5 10235-09-3 301.540 -0.4344689 ## 6 10423-37-7 235.286 -0.8836764 We can see that this dataset contains information on 6955 chemicals (rows), that are again represented by DTXSIDs. In this file, we will use data within the ‘LD50_LM’ column, which represents the -log10 of the millimolar LD50. LD stands for ‘Lethal Dose’. The LD50 value is the dose of substance given all at once which causes the death of 50% of a group of test animals. The lower the LD50 in mg/kg, the more toxic that substance is. Important Notes on Units In modeling studies, the convention is to convert toxicity values expressed as mg per unit into their molar or millimolar values and then to convert these to the base 10 logarithm. To increase clarity when plotting, such that higher toxicities would be expressed by higher values, the negative logarithm is then taken. For example, substance DTXSID00142939 has a molecular weight of 99.089 (grams per mole) and a LD50 of 32 mg/kg. This would be converted to a toxicity value of 32/99.089 = 0.322942 mmol/kg. The logarithm of that would be -0.4908755. By convention, the negative logarithm of the millimolar concentration would then be used i.e. -log[mmol/kg]. This conversion has been used to create the LD50_LM values in the acute toxicity dataset. Let’s check to see whether the same chemicals are present in both datasets: # First need to make sure that both dataframes are sorted by the identifier, DTXSID substances &lt;- substances[order(substances$DTXSID),] acute_data &lt;- acute_data[order(acute_data$DTXSID),] # Then test to see whether data in these columns are equal unique(substances$DTXSID == acute_data$DTXSID) ## [1] TRUE All accounts are true, meaning they are all equal (the same chemical) Data Visualizations of Acute Toxicity Values Let’s create a plot to show the distribution of the LD50 values in the dataset. ggplot(data = acute_data, aes(LD50_mgkg)) + stat_ecdf(geom = &quot;point&quot;) ggplot(data = acute_data, aes(LD50_LM)) + stat_ecdf(geom = &quot;point&quot;) Can you see a difference between these two plots? Yes, if the LD50 mg/kg values are converted into -log[mmol/kg] scale (LD50_LM), then the distribution resembles a normal cumulative distribution curve. Selecting the ‘Target’ Chemical of Interest for Read-Across Analysis For this exercise, we will select a ‘target’ substance of interest from our dataset, and assume that we have no acute toxicity data for it, and we will perform read-across for this target chemical. Note that this module’s example dataset actually has full data coverage (meaning all chemicals have acute toxicity data), but this exercise is beneficial, because we can make toxicity predictions, and then check to see how close we are by viewing the experimentally observed values. Our target substance for this exercise is going to be DTXSID5020281, which is 1-chloro-4-nitrobenzene. This chemical is an organic compound with the formula ClC6H4NO2, and is a common intermediate in the production of a number of industrially useful compounds, including common antioxidants found in rubber. Here is an image of the chemical structure (https://comptox.epa.gov/dashboard/dsstoxdb/results?search=DTXSID5020281): Filtering the dataframes for only data on this target substance: target_substance &lt;-filter(substances, DTXSID == &#39;DTXSID5020281&#39;) target_acute_data &lt;- filter(acute_data, DTXSID == &#39;DTXSID5020281&#39;) Calculating Structural Similarities between Substances To eventually identify chemical analogues with information that can be ‘read-across’ to our target chemical (1-chloro-4-nitrobenzene), we first need to evaluate how similar each chemical is to one another. In this example, we will base our search for similar substances upon similarities between chemical structure fingerprint representations. Once these chemical structure fingerprints are derived, they will be used to calculate the degree to which each possible pair of chemicals is similar, leveraging the Tanimoto metric. These findings will yield a similarity matrix of all possible pairwise similarity scores. Converting Chemical Identifiers into Molecular Objects (MOL) To derive structure fingerprints across all evaluated substances, we need to first convert the chemical identifiers originally provided as ‘QSAR_READY_SMILES’ into molecular objects. The standard exchange format for molecular information is a MOL file. This is a chemical file format that contains plain text information and stores information about atoms, bonds and their connections. We can carry out these identifier conversions using the ‘parse.smiles’ function within the rcdk package. Here we do this for the target chemical of interest, as well as all substances in the dataset. target_mol &lt;- parse.smiles(as.character(target_substance$QSAR_READY_SMILES)) all_mols &lt;-parse.smiles(all_smiles) Computing chemical fingerprints With these mol data, we can now compute the fingerprints for our target substance, as well as all the substances in the dataset. We can compute fingerprints leveraging the ‘get.fingerprint’ function. Let’s first run it on the target chemical: target.fp &lt;- get.fingerprint(target_mol[[1]], type = &#39;standard&#39;) target.fp # View fingerprint ## Fingerprint object ## name = ## length = 1024 ## folded = FALSE ## source = CDK ## bits on = 13 18 96 162 165 174 183 203 214 224 235 254 305 313 400 513 575 602 619 638 662 723 742 743 744 770 771 787 839 844 845 884 932 958 978 989 We can run the same function over the entire ‘all_mols’ dataset, leveraging the lapply function: all.fp &lt;- lapply(all_mols, get.fingerprint, type=&#39;standard&#39;) Calculating Chemical Similarities Using these molecular fingerprint data, we can now calculate the degree to which each chemical is similar to another chemical, based on structural similarity. The method employed in this example is the Tanimoto method. The Tanimoto similarity metric is a unitless number between zero and one that measures how similar two sets (in this case 2 chemicals) are from one another. A Tanimoto index of 1 means the 2 chemicals are identical whereas a index of 0 means that the chemicals share nothing in common. In the context of the fingerprints, a Tanimoto index of 0.5 means that half of the fingerprint matches between two chemicals whilst the other half does not match. Once these Tanimoto similarity indices are calculated between every possible chemical pair, the similarity results can be viewed in the form of a similarity matrix. In this matrix, all substances are listed across the rows and columns, and the degree to which every possible chemical pair is similar is summarized through values contained within the matrix. Further information about chemical similarity can be found here: https://en.wikipedia.org/wiki/Chemical_similarity Steps to generate this similarity matrix are detailed here: all.fp.sim &lt;- fingerprint::fp.sim.matrix(all.fp, method = &#39;tanimoto&#39;) all.fp.sim &lt;- as.data.frame(all.fp.sim) # Convert the outputted matrix to a dataframe colnames(all.fp.sim) = substances$DTXSID # Placing chemical identifiers back as column headers row.names(all.fp.sim) = substances$DTXSID # Placing chemical identifiers back as row names Since we are querying a large number of chemicals, it is difficult to view the entire resulting similarity matrix. Let’s, instead view portions of these results: all.fp.sim[1:5,1:5] # Viewing the first five rows and columns of data ## DTXSID00142939 DTXSID00143108 DTXSID00143880 DTXSID00144796 ## DTXSID00142939 1.00000000 0.38235294 0.09090909 0.10389610 ## DTXSID00143108 0.38235294 1.00000000 0.09523810 0.09302326 ## DTXSID00143880 0.09090909 0.09523810 1.00000000 0.09183673 ## DTXSID00144796 0.10389610 0.09302326 0.09183673 1.00000000 ## DTXSID00144933 0.17948718 0.14583333 0.09677419 0.06896552 ## DTXSID00144933 ## DTXSID00142939 0.17948718 ## DTXSID00143108 0.14583333 ## DTXSID00143880 0.09677419 ## DTXSID00144796 0.06896552 ## DTXSID00144933 1.00000000 all.fp.sim[6:10,6:10] # Viewing the next five rows and columns of data ## DTXSID00146356 DTXSID00147863 DTXSID00148532 DTXSID00148976 ## DTXSID00146356 1.00000000 0.05128205 0.12574850 0.10077519 ## DTXSID00147863 0.05128205 1.00000000 0.04166667 0.05050505 ## DTXSID00148532 0.12574850 0.04166667 1.00000000 0.08808290 ## DTXSID00148976 0.10077519 0.05050505 0.08808290 1.00000000 ## DTXSID00149721 0.35294118 0.07526882 0.15083799 0.08843537 ## DTXSID00149721 ## DTXSID00146356 0.35294118 ## DTXSID00147863 0.07526882 ## DTXSID00148532 0.15083799 ## DTXSID00148976 0.08843537 ## DTXSID00149721 1.00000000 You can see that there is an identity line within this similarity matrix, where instances when a chemical’s structure is being compared to itself, the similarity values are 1.00000. All other possible chemical pairings show variable similarity scores, ranging from: min(all.fp.sim) ## [1] 0 a minimum of zero, indicating no similarities between chemical structures. max(all.fp.sim) ## [1] 1 a maximum of 1, indicating the identical chemical structure (which occurs when comparing a chemical to itself). Identifying Chemical Analogues This step will find substances that are structurally similar to the target chemical, 1-chloro-4-nitrobenzene (with DTXSID5020281). Structurally similar chemicals are referred to as ‘source analogues’, with information that will be carried forward in this read-across analysis. The first step to identifying chemical analogues is to subset the full similarity matrix to focus just on our target chemical. target.sim &lt;- all.fp.sim %&gt;% filter(row.names(all.fp.sim) == &#39;DTXSID5020281&#39;) Then we’ll extract the substances that exceed a similarity threshold of 0.75 by selecting to keep columns which are &gt; 0.75. target.sim &lt;- target.sim %&gt;% select_if(function(x) any(x &gt; 0.75)) dim(target.sim) # Show dimensions of subsetted matrix ## [1] 1 12 This gives us our analogues list! Specifically, we selected 12 columns of data, representing our target chemical plus 11 structurally similar chemicals. Let’s create a dataframe of these substance identifiers to carry forward in the read-across analysis: source_analogues &lt;- t(target.sim) # Transposing the filtered similarity matrix results DTXSID &lt;-rownames(source_analogues) # Temporarily grabbing the dtxsid identifiers from this matrix source_analogues &lt;- cbind(DTXSID, source_analogues) # Adding these identifiers as a column rownames(source_analogues) &lt;- NULL # Removing the rownames from this dataframe, to land on a cleaned dataframe colnames(source_analogues) &lt;- c(&#39;DTXSID&#39;, &#39;Target_TanimotoSim&#39;) # Renaming column headers source_analogues[1:12,1:2] # Viewing the cleaned dataframe of analogues ## DTXSID Target_TanimotoSim ## [1,] &quot;DTXSID0020280&quot; &quot;0.846153846153846&quot; ## [2,] &quot;DTXSID2021105&quot; &quot;0.8&quot; ## [3,] &quot;DTXSID3024998&quot; &quot;0.878048780487805&quot; ## [4,] &quot;DTXSID4021971&quot; &quot;0.825&quot; ## [5,] &quot;DTXSID4030384&quot; &quot;0.80952380952381&quot; ## [6,] &quot;DTXSID5020281&quot; &quot;1&quot; ## [7,] &quot;DTXSID6020278&quot; &quot;0.782608695652174&quot; ## [8,] &quot;DTXSID6038827&quot; &quot;0.878048780487805&quot; ## [9,] &quot;DTXSID7052319&quot; &quot;0.790697674418605&quot; ## [10,] &quot;DTXSID7060596&quot; &quot;0.765957446808511&quot; ## [11,] &quot;DTXSID8024997&quot; &quot;0.767441860465116&quot; ## [12,] &quot;DTXSID8024999&quot; &quot;0.9&quot; With this, we can answer Environmental Health Question 1: How many chemicals with acute toxicity data are structurally similar to 1-chloro-4-nitrobenzene? Answer: In this dataset, 11 chemicals are structurally similar to the target chemical, based on a Tanimoto similiary score of &gt; 0.75. Chemical Read-Across to Predict Acute Toxicity Acute toxicity data from these chemical analogues can now be extracted and read across to the target chemical (1-chloro-4-nitrobenzene) to make predictions about its toxicity. Let’s first merge the acute data for these analogues into our working dataframe: source_analogues &lt;- merge(source_analogues, acute_data, by.x = &#39;DTXSID&#39;, by.y = &#39;DTXSID&#39;) Then, let’s remove the target chemical of interest and create a new dataframe of just the source analogues: source_analogues_only &lt;- source_analogues %&gt;% filter(Target_TanimotoSim != 1) # Removing the row of data with the target chemical, identified as the chemical with a similarity of 1 to itself Read-across Calculations using GenRA The final generalized read-across (GenRA) prediction is based on a similarity-weighted activity score. This score is specifically calculated as the following weighted average: (pairwise similarity between the target and source analogue) * (the toxicity of the source analogue), summed across each individual analogue; and then this value is divided by the sum of all pairwise similarities. For further details surrounding this algorithm and its spelled out formulation, see Shah et al.. Here are the underlying calculations needed to derive the similarity weighted activity score for this current exercise: source_analogues_only$wt_tox_calc &lt;- as.numeric(source_analogues_only$Target_TanimotoSim) * source_analogues_only$LD50_LM # Calculating (pairwise similarity between the target and source analogue) * (the toxicity of the source analogue) # for each analogy, and saving it as a new column titled &#39;wt_tox_calc&#39; sum_tox &lt;- sum(source_analogues_only$wt_tox_calc) #Summing this wt_tox_calc value across all analogues sum_sims &lt;- sum(as.numeric(source_analogues_only$Target_TanimotoSim)) # Summing all of the pairwise Tanimoto similarity scores ReadAcross_Pred &lt;- sum_tox/sum_sims # Final calculation for the weighted activity score (i.e., read-across prediction) Converting LD50 Units Right now, these results are in units of -log10 millimolar. So we still need to convert them into mg/kg equivalent, by converting out of -log10 and multiplying by the molecular weight of 1-chloro-4-nitrobenzene (g/mol): ReadAcross_Pred &lt;- (10^(-ReadAcross_Pred))*157.55 ReadAcross_Pred ## [1] 471.2042 With this, we can answer Environmental Health Question 2: What is the predicted LD50 for 1-chloro-4-nitrobenzene, using the GenRA approach? Answer: 1-chloro-4-nitrobenzene has a predicted LD50 (mg/kg) of 471 mg/kg. Visual Representation of this Read-Across Approach Here is a schematic summarizing the steps we employed in this analysis: Comparing Read-Across Predictions to Experimental Observations Let’s now compare how close this computationally-based prediction is to the experimentally observed LD50 value target_acute_data$LD50_mgkg ## [1] 460 We can see that the experimentally observed LD50 values for this chemical is 460 mg/kg With this, we can answer Environmental Health Question 3: How different is the predicted vs. experimentally observed LD50 for 1-chloro-4-nitrobenzene? Answer: The predicted LD50 is 471 mg/kg, and the experimentally observed LD50 is 460 mg/kg. Therefore, these values are very close! Concluding Remarks In conclusion, this training module leverages a dataset of substances with structural representations and toxicity data to create chemical fingerprint representations. We have selected a chemical of interest (target) and used the most similar analogues based on a similarity threshold to predict the acute toxicity of that target using the generalized read-across formula of weighted activity by similarity. We have seen that the prediction is in close agreement with that already reported for the target chemical in the dataset. Similar methods can be used to predict other toxicity endpoints, based on other datasets of chemicals. Further information on the GenRA approach as implemented in the EPA CompTox Chemicals Dashboard is described in the following manuscript: Helman G, Shah I, Williams AJ, Edwards J, Dunne J, Patlewicz G. Generalized Read-Across (GenRA): A workflow implemented into the EPA CompTox Chemicals Dashboard. ALTEX. 2019;36(3):462-465. PMID: 30741315. GenRA has also been implemented as a standalone python package. "],["comparative-toxicogenomics-database.html", "3.1 Comparative Toxicogenomics Database Introduction to Training Module CTD Data in R Identifying Genes under Epigenetic Control Concluding Remarks", " 3.1 Comparative Toxicogenomics Database This training module was developed by Lauren E. Koval, Dr. Kyle R. Roell, and Dr. Julia E. Rager Fall 2021 Background on Training Module Introduction to Comparative Toxicogenomics Database (CTD) CTD is a publicly available, online database that provides manually curated information about chemical-gene/protein interactions, chemical-disease and gene-disease relationships. CTD also recently incorporated curation of exposure data and chemical-phenotype relationships. CTD is located at: http://ctdbase.org/. Here is a screenshot of the CTD homepage (as of August 5, 2021): Introduction to Training Module In this activity, we will be using CTD to access and download data to perform data organization and analysis as an applications-based example towards environmental health research. This activity represents a demonstration of basic data manipulation, filtering, and organization steps in R, while highlighting the utility of CTD to identify novel genomic/epigenomic relationships to environmental exposures. Example visualizations are also included in this training module’s script, providing visualizations of gene list comparison results. Training Module’s Environmental Health Questions This training module was specifically developed to answer the following environmental health questions: Which genes show altered expression in response to arsenic exposure? Of the genes showing altered expression, which may be under epigenetic control? Script Preparations Cleaning the global environment rm(list=ls()) Installing required R packages If you already have these packages installed, you can skip this step, or you can run the below code which checks installation status for you. if (!requireNamespace(&quot;tidyverse&quot;)) install.packages(&quot;tidyverse&quot;) if (!requireNamespace(&quot;VennDiagram&quot;)) install.packages(&quot;VennDiagram&quot;) if (!requireNamespace(&quot;grid&quot;)) install.packages(&quot;grid&quot;) Loading R packages required for this session library(tidyverse) library(VennDiagram) library(grid) Set your working directory setwd(&quot;/filepath to where your input files are&quot;) CTD Data in R Organizing Example Dataset from CTD CTD requires manual querying of its database, outside of the R scripting environment. Because of this, let’s first manually pull the data we need for this example analysis. We can answer both of the example questions by pulling all chemical-gene relationship data for arsenic, which we can do by following the below steps: Navigate to the main CTD website: http://ctdbase.org/. Select at the top, ‘Search’ -&gt; ‘Chemical-Gene Interactions’. Select to query all chemical-gene interaction data for arsenic. Note that there are lots of results, represented by many many rows of data! Scroll to the bottom of the webpage and select to download as ‘CSV’. This is the file that we can now use to import into the R environment and analyze! Note that the data pulled here represent data available on August 1, 2021 Loading the Example CTD Dataset into R Read in the csv file of the results from CTD query ctd = read_csv(&quot;Module3_1/Module3_1_CTDOutput_ArsenicGene_Interactions.csv&quot;) Data Viewing Let’s first see how many rows and columns of data this file contains: dim(ctd) ## [1] 6280 9 This dataset includes 6280 observations (represented by rows) linking arsenic exposure to gene-level alterations With information spanning across 9 columns Let’s also see what kind of data are organized within the columns: colnames(ctd) ## [1] &quot;Chemical Name&quot; &quot;Chemical ID&quot; &quot;CAS RN&quot; ## [4] &quot;Gene Symbol&quot; &quot;Gene ID&quot; &quot;Interaction&quot; ## [7] &quot;Interaction Actions&quot; &quot;Reference Count&quot; &quot;Organism Count&quot; # Viewing the first five rows of data, across all 9 columns ctd[1:9,1:5] ## # A tibble: 9 × 5 ## `Chemical Name` `Chemical ID` `CAS RN` `Gene Symbol` `Gene ID` ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Arsenic D001151 7440-38-2 AACSP1 729522 ## 2 Arsenic D001151 7440-38-2 AADACL2 344752 ## 3 Arsenic D001151 7440-38-2 AAGAB 79719 ## 4 Arsenic D001151 7440-38-2 AAK1 22848 ## 5 Arsenic D001151 7440-38-2 AAMDC 28971 ## 6 Arsenic D001151 7440-38-2 AAR2 25980 ## 7 Arsenic D001151 7440-38-2 AASS 10157 ## 8 Arsenic D001151 7440-38-2 ABCA1 19 ## 9 Arsenic D001151 7440-38-2 ABCA12 26154 Filtering Data for Genes with Altered Expression To identify genes with altered expression in association with arsenic, we can leverage the results of our CTD query and filter this dataset to include only the rows that contain the term “expression” in the “Interaction Actions” column. exp_filt = ctd %&gt;% filter(grepl(&quot;expression&quot;, `Interaction Actions`)) We now have 2586 observations, representing instances of arsenic exposure causing a changes in a target gene’s expression levels. dim(exp_filt) ## [1] 2586 9 Let’s see how many unique genes this represents: length(unique(exp_filt$`Gene Symbol`)) ## [1] 1878 This reflects 1878 unique genes that show altered expression in association with arsenic. Let’s make a separate dataframe that includes only the unique genes, based on the “Gene Symbol” column. exp_genes = exp_filt %&gt;% distinct(`Gene Symbol`, .keep_all=TRUE) # Removing columns besides gene identifier exp_genes = exp_genes[,4] # Viewing the first 10 genes listed exp_genes[1:10,] ## # A tibble: 10 × 1 ## `Gene Symbol` ## &lt;chr&gt; ## 1 AADACL2 ## 2 AAK1 ## 3 AASS ## 4 ABCA12 ## 5 ABCC1 ## 6 ABCC2 ## 7 ABCC3 ## 8 ABCC4 ## 9 ABCG4 ## 10 ABHD12B This now provides us a list of 1878 genes showing altered expression in association with arsenic. Technical notes on running the distinct function within tidyverse: By default, the distinct function keeps the first instance of a duplicated value. This does have implications if the rest of the values in the rows differ. You will only retain the data associated with the first instance of the duplicated value (which is why we just retained the gene column here). It may be useful to first find the rows with the duplicate value and verify that results are as you would expect before removing observations. For example, in this dataset, expression levels can increase or decrease. If you were looking for just increases in expression, and there were genes that showed increased and decreased expression across different samples, using the distinct function just on “Gene Symbol” would not give you the results you wanted. If the first instance of the gene symbol noted decreased expression, that gene would not be returned in the results even though it might be one you would want. For this example case, we only care about expression change, regardless of direction, so this is not an issue. The distinct function can also take multiple columns to consider jointly as the value to check for duplicates if you are concerned about this. With this, we can answer Environmental Health Question 1: Which genes show altered expression in response to arsenic exposure? Answer: This list of 1878 genes have published evidence supporting their altered expression levels associated with arsenic exposure. Identifying Genes under Epigenetic Control For this dataset, let’s focus on gene-level methylation as a marker of epigenetic regulation. Let’s return to our main dataframe, representing the results of the CTD query, and filter these results for only the rows that contain the term “methylation” in the “Interaction Actions” column. met_filt = ctd %&gt;% filter(grepl(&quot;methylation&quot;,`Interaction Actions`)) We now have 3211 observations, representing instances of arsenic exposure causing a changes in a target gene’s methylation levels. dim(met_filt) ## [1] 3211 9 Let’s see how many unique genes this represents. length(unique(met_filt$`Gene Symbol`)) ## [1] 3142 This reflects 3142 unique genes that show altered methylation in association with arsenic Let’s make a separate dataframe that includes only the unique genes, based on the “Gene Symbol” column. met_genes = met_filt %&gt;% distinct(`Gene Symbol`, .keep_all=TRUE) # Removing columns besides gene identifier met_genes = met_genes[,4] This now provides us a list of 3142 genes showing altered methylation in association with arsenic. With this list of genes with altered methylation, we can now compare it to previous list of genes with altered expression to yeild our final list of genes of interest. To achieve this last step, we present two different methods to carry out list comparisons below. Method 1 for list comparisons: Merging Merge the expression results with the methylation resuts on the Gene Symbol column found in both datasets. merge_df = merge(exp_genes, met_genes, by = &quot;Gene Symbol&quot;) We end up with 315 rows reflecting the 315 genes that show altered expression and altered methylation Let’s view these genes: merge_df[1:315,] ## [1] &quot;ABCC4&quot; &quot;ABHD17A&quot; &quot;ABLIM2&quot; &quot;ACAD9&quot; &quot;ACKR2&quot; &quot;ACP3&quot; ## [7] &quot;ADAMTS1&quot; &quot;AFF1&quot; &quot;AGO2&quot; &quot;ALDH3B2&quot; &quot;ANPEP&quot; &quot;AOPEP&quot; ## [13] &quot;AP3D1&quot; &quot;APBB3&quot; &quot;APP&quot; &quot;AQP1&quot; &quot;ARF1&quot; &quot;ARID5B&quot; ## [19] &quot;AS3MT&quot; &quot;ASAP1&quot; &quot;ATF2&quot; &quot;ATG7&quot; &quot;ATP6V1C2&quot; &quot;ATXN1&quot; ## [25] &quot;ATXN7&quot; &quot;BACH1&quot; &quot;BCAR1&quot; &quot;BCL2&quot; &quot;BCL6&quot; &quot;BDNF&quot; ## [31] &quot;BECN1&quot; &quot;BMI1&quot; &quot;BMPR1A&quot; &quot;C1GALT1C1&quot; &quot;C1S&quot; &quot;C2CD3&quot; ## [37] &quot;CAMP&quot; &quot;CARD18&quot; &quot;CASP8&quot; &quot;CASTOR1&quot; &quot;CBR4&quot; &quot;CBS&quot; ## [43] &quot;CCDC68&quot; &quot;CCL14&quot; &quot;CCL20&quot; &quot;CCL24&quot; &quot;CCR2&quot; &quot;CD2&quot; ## [49] &quot;CD27&quot; &quot;CD40&quot; &quot;CDC42&quot; &quot;CDH1&quot; &quot;CDK2&quot; &quot;CDK4&quot; ## [55] &quot;CDK5&quot; &quot;CDK6&quot; &quot;CDKN1B&quot; &quot;CDKN2A&quot; &quot;CELF1&quot; &quot;CENPM&quot; ## [61] &quot;CEP72&quot; &quot;CERK&quot; &quot;CES4A&quot; &quot;CFAP300&quot; &quot;CHORDC1&quot; &quot;CLEC4D&quot; ## [67] &quot;CLIC5&quot; &quot;CMBL&quot; &quot;CNTNAP2&quot; &quot;CRCP&quot; &quot;CREBBP&quot; &quot;CUX2&quot; ## [73] &quot;CYP1B1&quot; &quot;CYP26B1&quot; &quot;CYP2U1&quot; &quot;DAPK1&quot; &quot;DAXX&quot; &quot;DCAF7&quot; ## [79] &quot;DDB2&quot; &quot;DHX32&quot; &quot;DLK1&quot; &quot;DNMT1&quot; &quot;DSG1&quot; &quot;DYNC2I2&quot; ## [85] &quot;ECHS1&quot; &quot;EDAR&quot; &quot;EFCAB2&quot; &quot;EHMT2&quot; &quot;EML2&quot; &quot;EPHA1&quot; ## [91] &quot;EPHA2&quot; &quot;EPM2AIP1&quot; &quot;ERBB4&quot; &quot;ERCC2&quot; &quot;ERN2&quot; &quot;ESR1&quot; ## [97] &quot;ETFB&quot; &quot;ETFDH&quot; &quot;F3&quot; &quot;FAM25A&quot; &quot;FAM43A&quot; &quot;FAM50B&quot; ## [103] &quot;FAM53C&quot; &quot;FAS&quot; &quot;FBLN2&quot; &quot;FBXO32&quot; &quot;FGF2&quot; &quot;FGFR3&quot; ## [109] &quot;FGGY&quot; &quot;FOSB&quot; &quot;FPR2&quot; &quot;FTH1P3&quot; &quot;FTL&quot; &quot;GAK&quot; ## [115] &quot;GAS1&quot; &quot;GFRA1&quot; &quot;GGACT&quot; &quot;GLI2&quot; &quot;GLI3&quot; &quot;GNPDA1&quot; ## [121] &quot;GOLGA4&quot; &quot;GSTM3&quot; &quot;GTSE1&quot; &quot;H2AC6&quot; &quot;H6PD&quot; &quot;HAPLN2&quot; ## [127] &quot;HCRT&quot; &quot;HDAC4&quot; &quot;HGF&quot; &quot;HLA-DQA1&quot; &quot;HOTAIR&quot; &quot;HSD17B2&quot; ## [133] &quot;HSPA1B&quot; &quot;HSPA1L&quot; &quot;HYAL1&quot; &quot;IER3&quot; &quot;IFNAR2&quot; &quot;IFNG&quot; ## [139] &quot;IGF1&quot; &quot;IKBKB&quot; &quot;IL10&quot; &quot;IL16&quot; &quot;IL1R1&quot; &quot;IL1RAP&quot; ## [145] &quot;IL20RA&quot; &quot;INPP4B&quot; &quot;IRF1&quot; &quot;ITGA8&quot; &quot;ITGAM&quot; &quot;ITGB1&quot; ## [151] &quot;JMJD6&quot; &quot;JUP&quot; &quot;KCNQ1&quot; &quot;KEAP1&quot; &quot;KLC1&quot; &quot;KLHL21&quot; ## [157] &quot;KRT1&quot; &quot;KRT18&quot; &quot;KRT27&quot; &quot;LAMB1&quot; &quot;LCE2B&quot; &quot;LEPR&quot; ## [163] &quot;LGALS7&quot; &quot;LMF1&quot; &quot;LMNA&quot; &quot;LRP8&quot; &quot;LRRC20&quot; &quot;MALAT1&quot; ## [169] &quot;MAOA&quot; &quot;MAP2&quot; &quot;MAP2K6&quot; &quot;MAP3K8&quot; &quot;MAPT&quot; &quot;MARVELD3&quot; ## [175] &quot;MBNL2&quot; &quot;MEF2C&quot; &quot;MEG3&quot; &quot;MGMT&quot; &quot;MICB&quot; &quot;MLC1&quot; ## [181] &quot;MLH1&quot; &quot;MMP19&quot; &quot;MOSMO&quot; &quot;MPG&quot; &quot;MRAP&quot; &quot;MSH2&quot; ## [187] &quot;MSI2&quot; &quot;MT1M&quot; &quot;MTOR&quot; &quot;MUC1&quot; &quot;MYH14&quot; &quot;MYL9&quot; ## [193] &quot;MYRIP&quot; &quot;NCL&quot; &quot;NEBL&quot; &quot;NEDD4&quot; &quot;NES&quot; &quot;NEU1&quot; ## [199] &quot;NFE2L2&quot; &quot;NLRP3&quot; &quot;NOS2&quot; &quot;NPM1&quot; &quot;NRF1&quot; &quot;NRG1&quot; ## [205] &quot;NRP2&quot; &quot;NTM&quot; &quot;NUAK2&quot; &quot;NUP62CL&quot; &quot;OASL&quot; &quot;OCLN&quot; ## [211] &quot;OSBPL5&quot; &quot;PALS1&quot; &quot;PCSK6&quot; &quot;PDZD2&quot; &quot;PECAM1&quot; &quot;PFKFB3&quot; ## [217] &quot;PGAP2&quot; &quot;PGK1&quot; &quot;PIAS1&quot; &quot;PLA2G4D&quot; &quot;PLCD1&quot; &quot;PLEC&quot; ## [223] &quot;PLEKHA6&quot; &quot;PLEKHG3&quot; &quot;PPFIA4&quot; &quot;PPFIBP2&quot; &quot;PPTC7&quot; &quot;PRDX1&quot; ## [229] &quot;PRKCQ&quot; &quot;PRMT6&quot; &quot;PRR5L&quot; &quot;PRSS3&quot; &quot;PTGS2&quot; &quot;PTPRE&quot; ## [235] &quot;PVT1&quot; &quot;PYROXD2&quot; &quot;RAB11FIP3&quot; &quot;RAMP1&quot; &quot;RAP1GAP2&quot; &quot;RAPGEF1&quot; ## [241] &quot;RASAL2&quot; &quot;RELCH&quot; &quot;RGMA&quot; &quot;RHEBL1&quot; &quot;RHOH&quot; &quot;RIPOR1&quot; ## [247] &quot;RNF213&quot; &quot;RNF216&quot; &quot;ROBO1&quot; &quot;S100P&quot; &quot;S1PR1&quot; &quot;SBF1&quot; ## [253] &quot;SBNO2&quot; &quot;SCGB3A1&quot; &quot;SCHIP1&quot; &quot;SELENOW&quot; &quot;SEMA5B&quot; &quot;SGMS1&quot; ## [259] &quot;SH2B2&quot; &quot;SKP2&quot; &quot;SLC22A5&quot; &quot;SLC44A2&quot; &quot;SLC6A6&quot; &quot;SNCA&quot; ## [265] &quot;SNHG32&quot; &quot;SNX1&quot; &quot;SORL1&quot; &quot;SPHK1&quot; &quot;SPINK1&quot; &quot;SPSB1&quot; ## [271] &quot;SPTBN1&quot; &quot;SQSTM1&quot; &quot;SRGAP1&quot; &quot;SSU72&quot; &quot;STAT3&quot; &quot;STK17B&quot; ## [277] &quot;STX1A&quot; &quot;STX3&quot; &quot;SULT2B1&quot; &quot;TCEA3&quot; &quot;TERT&quot; &quot;TGFB1&quot; ## [283] &quot;TGFB3&quot; &quot;TGFBR2&quot; &quot;THNSL2&quot; &quot;TIMP2&quot; &quot;TLR10&quot; &quot;TMEM86A&quot; ## [289] &quot;TNFRSF10B&quot; &quot;TNFRSF10D&quot; &quot;TNFRSF1B&quot; &quot;TNFSF10&quot; &quot;TNNC2&quot; &quot;TP53&quot; ## [295] &quot;TRIB1&quot; &quot;TRNP1&quot; &quot;TSC22D3&quot; &quot;TSLP&quot; &quot;TXNRD1&quot; &quot;UAP1&quot; ## [301] &quot;UBE2J2&quot; &quot;ULK1&quot; &quot;USP36&quot; &quot;VAV3&quot; &quot;VWF&quot; &quot;WDR26&quot; ## [307] &quot;WDR55&quot; &quot;WNK1&quot; &quot;WWTR1&quot; &quot;XDH&quot; &quot;ZBTB25&quot; &quot;ZEB1&quot; ## [313] &quot;ZNF200&quot; &quot;ZNF267&quot; &quot;ZNF696&quot; With this, we can answer Environmental Health Question 2: Of the genes showing altered expression, which may be under epigenetic control? Answer: We identified 315 genes with altered expression resulting from arsenic exposure, that also demonstrate epigenetic modifications from arsenic. These genes include many high interest molecules involved in regulating cell health, including several cyclin dependent kinases (e.g., CDK2, CDK4, CDK5, CDK6), molecules involved in oxidative stress (e.g., FOSB, NOS2), and cytokines involved in inflammatory response pathways (e.g., IFNG, IL10, IL16, IL1R1, IR1RAP, TGFB1, TGFB3). Method 2 for list comparisons: Intersection For further training, shown here is another method for pulling this list of interest, through the use of the ‘intersection’ function. Obtain a list of the overlapping genes in the overall expression results and the methylation results. inxn = intersect(exp_filt$`Gene Symbol`,met_filt$`Gene Symbol`) Again, we end up with a list of 315 unique genes that show altered expression and altered methylation. This list can be viewed on its own or converted to a dataframe (df). inxn_df = data.frame(genes=inxn) This list can also be conveniently used to filter the original query results. inxn_df_all_data = ctd %&gt;% filter(`Gene Symbol` %in% inxn) Note that in this last case, the same 315 genes are present, but this time the results contain all records from the original query results, hence the 875 rows (875 records observations reflecting the 315 genes). summary(unique(sort(inxn_df_all_data$`Gene Symbol`))==sort(merge_df$`Gene Symbol`)) ## Mode TRUE ## logical 315 dim(inxn_df_all_data) ## [1] 875 9 Visually we can represent this as a Venn diagram. Here, we use the “VennDiagram” R package. # Use the data we previously used for intersection in the venn diagram function venn.plt = venn.diagram( x = list(exp_filt$`Gene Symbol`, met_filt$`Gene Symbol`), category.names = c(&quot;Altered Expression&quot; , &quot;Altered Methylation&quot;), filename = &quot;venn_diagram.png&quot;, # Change font size, type, and position cat.cex = 1.15, cat.fontface = &quot;bold&quot;, cat.default.pos = &quot;outer&quot;, cat.pos = c(-27, 27), cat.dist = c(0.055, 0.055), # Change color of ovals col=c(&quot;#440154ff&quot;, &#39;#21908dff&#39;), fill = c(alpha(&quot;#440154ff&quot;,0.3), alpha(&#39;#21908dff&#39;,0.3)), ) Concluding Remarks In conclusion, we identified 315 genes that show altered expression in response to arsenic exposure that may be under epigenetic control. These genes represent critical mediators of oxidative stress and inflammation, among other important cellular processes. Results yielded an important list of genes representing potential targets for further evaluation, to better understand mechanism of environmental exposure-induced disease. Together, this example highlights the utility of CTD to address environmental health research questions. For more information, see the recently updated primary CTD publication: Davis AP, Grondin CJ, Johnson RJ, Sciaky D, Wiegers J, Wiegers TC, Mattingly CJ. Comparative Toxicogenomics Database (CTD): update 2021. Nucleic Acids Res. 2021 Jan 8;49(D1):D1138-D1143. PMID: 33068428. Additional case studies relevant to environmental health research include the following: An example publication leveraging CTD findings to identify mechanisms of metals-induced birth defects: Ahir BK, Sanders AP, Rager JE, Fry RC. Systems biology and birth defects prevention: blockade of the glucocorticoid receptor prevents arsenic-induced birth defects. Environ Health Perspect. 2013 Mar;121(3):332-8. PMID: 23458687. An example publication leveraging CTD to help fill data gaps on data poor chemicals, in combination with ToxCast/Tox21 data streams, to elucidate environmental influences on disease pathways: Kosnik MB, Planchart A, Marvel SW, Reif DM, Mattingly CJ. Integration of curated and high-throughput screening data to elucidate environmental influences on disease pathways. Comput Toxicol. 2019 Nov;12:100094. PMID: 31453412. An example publication leveraging CTD to extract chemical-disease relationships used to derive new chemical risk values, with the goal of prioritizing connections between environmental factors, genetic variants, and human diseases: Kosnik MB, Reif DM. Determination of chemical-disease risk values to prioritize connections between environmental factors, genetic variants, and human diseases. Toxicol Appl Pharmacol. 2019 Sep 15;379:114674. PMID: 31323264. "],["gene-expression-omnibus.html", "3.2 Gene Expression Omnibus Introduction to Training Module GEO Data in R Visualizing Data Statistical Analyses Concluding Remarks", " 3.2 Gene Expression Omnibus This training module was developed by Dr. Kyle R. Roell and Dr. Julia E. Rager Fall 2021 Background on Training Module Introduction to the Environmental Health Database, Gene Expression Omnibus (GEO) GEO is a publicly available database repository of high-throughput gene expression data and hybridization arrays, chips, and microarrays that span genome-wide endpoints of genomics, transcriptomics, and epigenomics. The repository is organized and managed by the The National Center for Biotechnology Information (NCBI), which seeks to advance science and health by providing access to biomedical and genomic information. The three overall goals of GEO are to: (1) Provide a robust, versatile database in which to efficiently store high-throughput functional genomic data, (2) Offer simple submission procedures and formats that support complete and well-annotated data deposits from the research community, and (3) Provide user-friendly mechanisms that allow users to query, locate, review and download studies and gene expression profiles of interest. Of high relevance to environmental health, data organized within GEO can be pulled and analyzed to address new environmental health questions, leveraging previously generated data. For example, we have pulled gene expression data from acute myeloid leukemia patients and re-analyzed these data to elucidate new mechanisms of epigenetically-regulated networks involved in cancer, that in turn, may be modified by environmental insults, as previously published in Rager et al. 2012. We have also pulled and analyzed gene expression data from published studies evaluating toxicity resulting from hexavalent chromium exposure, to further substantiate the role of epigenetic mediators in hexavelent chromium-induced carcinogenesis (see Rager et al. 2019). This training exercise leverages an additional dataset that we published and deposited through GEO to evaluate the effects of formaldehyde inhalation exposure, as detailed below. Introduction to Training Module This training module provides an overview on pulling and analyzing data deposited in GEO. As an example, data are pulled from the published GEO dataset recorded through the online series GSE42394. This series represents Affymetrix rat genome-wide microarray data generated from our previous study, aimed at evaluating the transcriptomic effects of formaldehyde across three tissues: the nose, blood, and bone marrow. For the purposes of this training module, we will focus on evaluating gene expression profiles from nasal samples after 7 days of exposure, collected from rats exposed to 2 ppm formaldehyde via inhalation. These findings, in addition to other epigenomic endpoint measures, have been previously published (see Rager et al. 2014). This training module specifically guides trainees through the loading of required packages and data, including the manual upload of GEO data as well as the upload/organization of data leveraging the GEOquery package. Data are then further organized and combined with gene annotation information through the merging of platform annotation files. Example visualizations are then produced, including boxplots to evaluate the overall distribution of expression data across samples, as well as heat map visualizations that compare unscaled versus scaled gene expression values. Statistical analyses are then included to identify which genes are significantly altered in expression upon exposure to formaldehyde. Together, this training module serves as a simple example showing methods to access and download GEO data and to perform data organization, analysis, and visualization tasks through applications-based questions. Training Module’s Environmental Health Questions This training module was specifically developed to answer the following environmental health questions: What kind of molecular identifiers are commonly used in microarray-based -omics technologies? How can we convert platform-specific molecular identifiers used in -omics study designs to gene-level information? Why do we often scale gene expression signatures prior to heat map visualizations? What genes are altered in expression by formaldehyde inhalation exposure? What are the potential biological consequences of these gene-level perturbations? Script Preparations Cleaning the global environment rm(list=ls()) Installing required R packages If you already have these packages installed, you can skip this step, or you can run the below code which checks installation status for you if (!requireNamespace(&quot;tidyverse&quot;)) install.packages(&quot;tidyverse&quot;) if (!requireNamespace(&quot;reshape2&quot;)) install.packages(&quot;reshape2&quot;) # GEOquery, this will install BiocManager if you don&#39;t have it installed if (!requireNamespace(&quot;BiocManager&quot;)) install.packages(&quot;BiocManager&quot;) BiocManager::install(&quot;GEOquery&quot;) ## Warning: package(s) not installed when version(s) same as current; use `force = TRUE` to ## re-install: &#39;GEOquery&#39; Loading R packages required for this session library(tidyverse) library(reshape2) library(GEOquery) For more information on the tidyverse package, see its associated CRAN webpage, primary webpage, and peer-reviewed article released in 2018. For more information on the reshape2 package, see its associated CRAN webpage, R Documentation, and helpful website providing an introduction to the reshape2 package. For more information on the GEOquery package, see its associated Bioconductor website and R Documentation file. Set your working directory setwd(&quot;/filepath to where your input files are&quot;) Loading and Organizing the Example Dataset Let’s start by loading the GEO dataset needed for this training module. As explained in the introduction, this module walks through two methods of uploading GEO data: manual option vs automatic option using the GEOquery package. These two methods are detailed below. GEO Data in R 1. Manually Downloading and Uploading GEO Files In this first method, we will navigate to the datasets within the GEO website, manually download its associated text data file, save it in our working directory, and then upload it into our global environment in R. For the purposes of this training exercise, we manually downloaded the GEO series matrix file from the GEO series webpage, located at: https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE42394. The specific file that was downloaded was noted as GSE42394_series_matrix.txt, pulled by clicking on the link indicated by the red arrow from the GEO series webpage: For simplicity, we also have already pre-filtered this file for the samples we are interested in, focusing on the rat nasal gene expression data after 7 days of exposure to gaseous formaldehyde. This filtered file was saved as: GSE42394_series_matrix_filtered.txt At this point, we can simply read in this pre-filtered text file for the purposes of this training module geodata_manual = read.table(file=&quot;Module3_2/Module3_2_GSE42394_series_matrix_filtered.txt&quot;, header=T) Because this is a manual approach, we have to also manually define the treated and untreated samples (based on manually opening the surrounding metadata from the GEO webpage) Manually defining treated and untreated for these samples of interest: exposed_manual = c(&quot;GSM1150940&quot;, &quot;GSM1150941&quot;, &quot;GSM1150942&quot;) unexposed_manual = c(&quot;GSM1150937&quot;, &quot;GSM1150938&quot;, &quot;GSM1150939&quot;) 2. Uploading and Organizing GEO Files through the GEOquery Package In this second method, we will leverage the GEOquery package, which allows for easier downloading and reading in of data from GEO without having to manually download raw text files, and manually assign sample attributes (e.g., exposed vs unexposed). This package is set-up to automatically merge sample information from GEO metadata files with raw genome-wide datasets. Let’s first use the getGEO function (from the GEOquery package) to load data from our series matrix. Note that this line of code may take a couple of minutes geo.getGEO.data = getGEO(filename=&#39;Module3_2/Module3_2_GSE42394_series_matrix.txt&#39;) One of the reasons the getGEO package is so helpful is that we can automatically link a dataset with nicely organized sample information using the pData() function. sampleInfo = pData(geo.getGEO.data) Let’s view this sample information / metadata file, first by viewing what the column headers are. colnames(sampleInfo) ## [1] &quot;title&quot; &quot;geo_accession&quot; ## [3] &quot;status&quot; &quot;submission_date&quot; ## [5] &quot;last_update_date&quot; &quot;type&quot; ## [7] &quot;channel_count&quot; &quot;source_name_ch1&quot; ## [9] &quot;organism_ch1&quot; &quot;characteristics_ch1&quot; ## [11] &quot;characteristics_ch1.1&quot; &quot;characteristics_ch1.2&quot; ## [13] &quot;characteristics_ch1.3&quot; &quot;characteristics_ch1.4&quot; ## [15] &quot;characteristics_ch1.5&quot; &quot;treatment_protocol_ch1&quot; ## [17] &quot;growth_protocol_ch1&quot; &quot;molecule_ch1&quot; ## [19] &quot;extract_protocol_ch1&quot; &quot;label_ch1&quot; ## [21] &quot;label_protocol_ch1&quot; &quot;taxid_ch1&quot; ## [23] &quot;hyb_protocol&quot; &quot;scan_protocol&quot; ## [25] &quot;description&quot; &quot;data_processing&quot; ## [27] &quot;platform_id&quot; &quot;contact_name&quot; ## [29] &quot;contact_email&quot; &quot;contact_department&quot; ## [31] &quot;contact_institute&quot; &quot;contact_address&quot; ## [33] &quot;contact_city&quot; &quot;contact_zip/postal_code&quot; ## [35] &quot;contact_country&quot; &quot;supplementary_file&quot; ## [37] &quot;data_row_count&quot; &quot;age:ch1&quot; ## [39] &quot;cell type:ch1&quot; &quot;gender:ch1&quot; ## [41] &quot;strain:ch1&quot; &quot;time:ch1&quot; ## [43] &quot;treatment:ch1&quot; Then viewing the first five columns. sampleInfo[1:10,1:5] ## title geo_accession ## GSM1150937 Nose_7DayControl_Rep1 [Affymetrix] GSM1150937 ## GSM1150938 Nose_7DayControl_Rep2 [Affymetrix] GSM1150938 ## GSM1150939 Nose_7DayControl_Rep3 [Affymetrix] GSM1150939 ## GSM1150940 Nose_7DayExposed_Rep1 [Affymetrix] GSM1150940 ## GSM1150941 Nose_7DayExposed_Rep2 [Affymetrix] GSM1150941 ## GSM1150942 Nose_7DayExposed_Rep3 [Affymetrix] GSM1150942 ## GSM1150943 WhiteBloodCells_7DayControl_Rep1 [Affymetrix] GSM1150943 ## GSM1150944 WhiteBloodCells_7DayControl_Rep2 [Affymetrix] GSM1150944 ## GSM1150945 WhiteBloodCells_7DayControl_Rep3 [Affymetrix] GSM1150945 ## GSM1150946 WhiteBloodCells_7DayExposed_Rep1 [Affymetrix] GSM1150946 ## status submission_date last_update_date ## GSM1150937 Public on Jan 07 2014 May 29 2013 Jan 07 2014 ## GSM1150938 Public on Jan 07 2014 May 29 2013 Jan 07 2014 ## GSM1150939 Public on Jan 07 2014 May 29 2013 Jan 07 2014 ## GSM1150940 Public on Jan 07 2014 May 29 2013 Jan 07 2014 ## GSM1150941 Public on Jan 07 2014 May 29 2013 Jan 07 2014 ## GSM1150942 Public on Jan 07 2014 May 29 2013 Jan 07 2014 ## GSM1150943 Public on Jan 07 2014 May 29 2013 Jan 07 2014 ## GSM1150944 Public on Jan 07 2014 May 29 2013 Jan 07 2014 ## GSM1150945 Public on Jan 07 2014 May 29 2013 Jan 07 2014 ## GSM1150946 Public on Jan 07 2014 May 29 2013 Jan 07 2014 This shows that each sample is provided with a unique number starting with “GSM”, and these are described by information summarized in the “title” column. We can also see that these data were made public on Jan 7, 2014. Let’s view the next five columns. sampleInfo[1:10,6:10] ## type channel_count source_name_ch1 ## GSM1150937 RNA 1 Nasal epithelial cells, 7 day, unexposed ## GSM1150938 RNA 1 Nasal epithelial cells, 7 day, unexposed ## GSM1150939 RNA 1 Nasal epithelial cells, 7 day, unexposed ## GSM1150940 RNA 1 Nasal epithelial cells, 7 day, exposed ## GSM1150941 RNA 1 Nasal epithelial cells, 7 day, exposed ## GSM1150942 RNA 1 Nasal epithelial cells, 7 day, exposed ## GSM1150943 RNA 1 Circulating white blood cells, 7 day, unexposed ## GSM1150944 RNA 1 Circulating white blood cells, 7 day, unexposed ## GSM1150945 RNA 1 Circulating white blood cells, 7 day, unexposed ## GSM1150946 RNA 1 Circulating white blood cells, 7 day, exposed ## organism_ch1 characteristics_ch1 ## GSM1150937 Rattus norvegicus gender: male ## GSM1150938 Rattus norvegicus gender: male ## GSM1150939 Rattus norvegicus gender: male ## GSM1150940 Rattus norvegicus gender: male ## GSM1150941 Rattus norvegicus gender: male ## GSM1150942 Rattus norvegicus gender: male ## GSM1150943 Rattus norvegicus gender: male ## GSM1150944 Rattus norvegicus gender: male ## GSM1150945 Rattus norvegicus gender: male ## GSM1150946 Rattus norvegicus gender: male We can see that information is provided here surrounding the type of sample that was analyzed (i.e., RNA), more information on the collected samples within the column ‘source_name_ch1’, and the organism (rat) is provided in the ‘organism_ch1’ column. More detailed metadata information is provided throughout this file, as seen when viewing the column headers above. Defining Samples Now, we can use this information to define the samples we want to analyze. Note that this is the same step we did manually above. In this training exercise, we are focusing on responses in the nose, so we can easily filter for cell type = Nasal epithelial cells (specifically in the “cell type:ch1” variable). We are also focusing on responses collected after 7 days of exposure, which we can filter for using time = 7 day (specifically in the “time:ch1” variable). We will also define exposed and unexposed samples using the variable “treatment:ch1”. First, let’s subset the sampleInfo dataframe to just keep the samples we’re interested in # Define a vector variable (here we call it &#39;keep&#39;) that will store rows we want to keep keep = rownames(sampleInfo[which(sampleInfo$`cell type:ch1`==&quot;Nasal epithelial cells&quot; &amp; sampleInfo$`time:ch1`==&quot;7 day&quot;),]) # Then subset the sample info for just those samples we defined in keep variable sampleInfo = sampleInfo[keep,] Next, we can pull the exposed and unexposed animal IDs. Let’s first see how these are labeled within the “treatment:ch1” variable. unique(sampleInfo$`treatment:ch1`) ## [1] &quot;unexposed&quot; &quot;2 ppm formaldehyde&quot; And then search for the rows of data, pulling the sample animal IDs (which are in the variable ‘geo_accession’). exposedIDs = sampleInfo[which(sampleInfo$`treatment:ch1`==&quot;2 ppm formaldehyde&quot;), &quot;geo_accession&quot;] unexposedIDs = sampleInfo[which(sampleInfo$`treatment:ch1`==&quot;unexposed&quot;), &quot;geo_accession&quot;] The next step is to pull the expression data we want to use in our analyses. The GEOquery function, exprs(), allows us to easily pull these data. Here, we can pull the data we’re interested in using the exprs() function, while defining the data we want to pull based off our previously generated ‘keep’ vector. # As a reminder, this is what the &#39;keep&#39; vector includes # (i.e., animal IDs that we&#39;re interested in) keep ## [1] &quot;GSM1150937&quot; &quot;GSM1150938&quot; &quot;GSM1150939&quot; &quot;GSM1150940&quot; &quot;GSM1150941&quot; ## [6] &quot;GSM1150942&quot; # Using the exprs() function geodata = exprs(geo.getGEO.data[,keep]) Let’s view the full dataset as is now: head(geodata) ## GSM1150937 GSM1150938 GSM1150939 GSM1150940 GSM1150941 GSM1150942 ## 10700001 5786.60 5830.08 5637.34 5313.33 5557.04 5469.90 ## 10700002 192.92 206.86 220.83 183.12 177.16 198.64 ## 10700003 1820.98 1795.79 1735.70 1578.02 1681.58 1632.20 ## 10700004 66.95 65.61 64.41 60.19 60.41 60.67 ## 10700005 770.07 753.41 731.20 684.53 657.25 667.66 ## 10700006 5.80 5.35 5.48 5.58 5.39 5.35 This now represents a matrix of data, with animal IDs as column headers and expression levels within the matrix. Simplifying column names These column names are not the easiest to interpret, so let’s rename these columns to indicate which animals were from the exposed vs. unexposed groups. We need to first convert our expression dataset to a dataframe so we can edit columns names, and continue with downstream data manipulations that require dataframe formats. geodata = data.frame(geodata) Let’s remind ourselves what the column names are: colnames(geodata) ## [1] &quot;GSM1150937&quot; &quot;GSM1150938&quot; &quot;GSM1150939&quot; &quot;GSM1150940&quot; &quot;GSM1150941&quot; ## [6] &quot;GSM1150942&quot; Which ones of these are exposed vs unexposed animals can be determined by viewing our previously defined vectors. exposedIDs ## [1] &quot;GSM1150940&quot; &quot;GSM1150941&quot; &quot;GSM1150942&quot; unexposedIDs ## [1] &quot;GSM1150937&quot; &quot;GSM1150938&quot; &quot;GSM1150939&quot; With this we can tell that the first three listed IDs are from unexposed animals, and the last three IDs are from exposed animals. Let’s simplify the names of these columns to indicate exposure status and replicate number. colnames(geodata) = c(&quot;Control_1&quot;, &quot;Control_2&quot;, &quot;Control_3&quot;, &quot;Exposed_1&quot;, &quot;Exposed_2&quot;, &quot;Exposed_3&quot;) And we’ll now need to re-define our ‘exposed’ vs ‘unexposed’ vectors for downstream script. exposedIDs = c(&quot;Exposed_1&quot;, &quot;Exposed_2&quot;, &quot;Exposed_3&quot;) unexposedIDs = c(&quot;Control_1&quot;, &quot;Control_2&quot;, &quot;Control_3&quot;) Viewing the data again: head(geodata) ## Control_1 Control_2 Control_3 Exposed_1 Exposed_2 Exposed_3 ## 10700001 5786.60 5830.08 5637.34 5313.33 5557.04 5469.90 ## 10700002 192.92 206.86 220.83 183.12 177.16 198.64 ## 10700003 1820.98 1795.79 1735.70 1578.02 1681.58 1632.20 ## 10700004 66.95 65.61 64.41 60.19 60.41 60.67 ## 10700005 770.07 753.41 731.20 684.53 657.25 667.66 ## 10700006 5.80 5.35 5.48 5.58 5.39 5.35 These data are now looking easier to interpret/analyze. Still, the row identifiers include 8 digit numbers starting with “107…”. We know that this dataset is a gene expression dataset, but these identifiers, in themselves, don’t tell us much about what genes these are referring to. These numeric IDs specifically represent microarray probesetIDs, that were produced by the Affymetrix platform used in the original study. But how can we tell which genes are represented by these data?! Adding Gene Symbol Information Each -omics dataset contained within GEO points to a specific platform that was used to obtain measurements. In instances where we want more information surrounding the molecular identifiers, we can merge the platform-specific annotation file with the molecular IDs given in the full dataset. For example, let’s pull the platform-specific annotation file for this experiment. Let’s revisit the website that contained the original dataset on GEO Scroll down to where it lists “Platforms”, and there is a hyperlinked platform number “GPL6247” (see arrow below) Click on this, and you will be navigated to a different GEO website describing the Affymetrix rat array platform that was used in this analysis Note that this website also includes information on when this array became available, links to other experiments that have used this platform within GEO, and much more Here, we’re interested in pulling the corresponding gene symbol information for the probeset IDs To do so, scroll to the bottom, and click “Annotation SOFT table…” and download the corresponding .gz file within your working directory Unzip this, and you will find the master annotation file: “GPL6247.annot” In this exercise, we’ve already done these steps and unzipped the file in our working directory. So at this point, we can simply read in this annotation dataset, still using the GEOquery function to help automate. geo.annot = GEOquery::getGEO(filename=&quot;Module3_2/Module3_2_GPL6247.annot&quot;) Now we can use the Table function from GEOquery to pull data from the annotation dataset. id.gene.table = GEOquery::Table(geo.annot)[,c(&quot;ID&quot;, &quot;Gene symbol&quot;)] id.gene.table[1:10,1:2] ## ID Gene symbol ## 1 10701620 Vom2r67///Vom2r5///Vom2r6///Vom2r4 ## 2 10701630 ## 3 10701632 ## 4 10701636 ## 5 10701643 ## 6 10701648 Vom2r5 ## 7 10701654 Vom2r6 ## 8 10701663 ## 9 10701666 ## 10 10701668 Vom2r65///Vom2r1 With these two columns of data, we now have the needed IDs and gene symbols to match with our dataset. Within the full dataset, we need to add a new column for the probeset ID, taken from the rownames, in preparation for the merging step. geodata$ID = rownames(geodata) We can now merge the gene symbol information by ID with our expression data. geodata_genes = merge(geodata, id.gene.table, by=&quot;ID&quot;) head(geodata_genes) ## ID Control_1 Control_2 Control_3 Exposed_1 Exposed_2 Exposed_3 ## 1 10700001 5786.60 5830.08 5637.34 5313.33 5557.04 5469.90 ## 2 10700002 192.92 206.86 220.83 183.12 177.16 198.64 ## 3 10700003 1820.98 1795.79 1735.70 1578.02 1681.58 1632.20 ## 4 10700004 66.95 65.61 64.41 60.19 60.41 60.67 ## 5 10700005 770.07 753.41 731.20 684.53 657.25 667.66 ## 6 10700006 5.80 5.35 5.48 5.58 5.39 5.35 ## Gene symbol ## 1 ## 2 ## 3 ## 4 ## 5 ## 6 Note that many of the probeset IDs do not map to full gene symbols, which is shown here by viewing the top few rows - this is expected in genome-wide analyses based on microarray platforms. Let’s look at the first 25 unique genes in these data: UniqueGenes = unique(geodata_genes$`Gene symbol`) UniqueGenes[1:25] ## [1] &quot;&quot; ## [2] &quot;Vom2r67///Vom2r5///Vom2r6///Vom2r4&quot; ## [3] &quot;Vom2r5&quot; ## [4] &quot;Vom2r6&quot; ## [5] &quot;Vom2r65///Vom2r1&quot; ## [6] &quot;Vom2r65///Vom2r5///Vom2r1///Vom2r6///Vom2r4&quot; ## [7] &quot;Vom2r65///Vom2r5///Vom2r6///Vom2r4&quot; ## [8] &quot;Raet1e&quot; ## [9] &quot;Lrp11&quot; ## [10] &quot;Katna1&quot; ## [11] &quot;Ppil4&quot; ## [12] &quot;Zc3h12d&quot; ## [13] &quot;Shprh&quot; ## [14] &quot;Fbxo30&quot; ## [15] &quot;Epm2a&quot; ## [16] &quot;Sf3b5&quot; ## [17] &quot;Plagl1&quot; ## [18] &quot;Fuca2&quot; ## [19] &quot;Adat2&quot; ## [20] &quot;Hivep2&quot; ## [21] &quot;Nmbr&quot; ## [22] &quot;Cited2&quot; ## [23] &quot;Txlnb&quot; ## [24] &quot;Reps1&quot; ## [25] &quot;Perp&quot; Again, you can see that the first value listed is blank, representing probesetIDs that do not match to fully annotated gene symbols. Though the rest pertain for gene symbols annotated to the rat genome. You can also see that some gene symbols have multiple entries, separated by “///” To simplify identifiers, we can pull just the first gene symbol, and remove the rest by using gsub(). geodata_genes$`Gene symbol` = gsub(&quot;///.*&quot;, &quot;&quot;, geodata_genes$`Gene symbol`) Let’s alphabetize by main expression dataframe by gene symbol. geodata_genes = geodata_genes[order(geodata_genes$`Gene symbol`),] And then re-view these data: geodata_genes[1:5,] ## ID Control_1 Control_2 Control_3 Exposed_1 Exposed_2 Exposed_3 ## 1 10700001 5786.60 5830.08 5637.34 5313.33 5557.04 5469.90 ## 2 10700002 192.92 206.86 220.83 183.12 177.16 198.64 ## 3 10700003 1820.98 1795.79 1735.70 1578.02 1681.58 1632.20 ## 4 10700004 66.95 65.61 64.41 60.19 60.41 60.67 ## 5 10700005 770.07 753.41 731.20 684.53 657.25 667.66 ## Gene symbol ## 1 ## 2 ## 3 ## 4 ## 5 In preparation for the visualization steps below, let’s reset the probeset IDs to rownames. rownames(geodata_genes) = geodata_genes$ID # Can then remove this column within the dataframe geodata_genes$ID = NULL Finally let’s rearrange this dataset to include gene symbols as the first column, right after rownames (probeset IDs). geodata_genes = geodata_genes[,c(ncol(geodata_genes),1:(ncol(geodata_genes)-1))] geodata_genes[1:5,] ## Gene symbol Control_1 Control_2 Control_3 Exposed_1 Exposed_2 ## 10700001 5786.60 5830.08 5637.34 5313.33 5557.04 ## 10700002 192.92 206.86 220.83 183.12 177.16 ## 10700003 1820.98 1795.79 1735.70 1578.02 1681.58 ## 10700004 66.95 65.61 64.41 60.19 60.41 ## 10700005 770.07 753.41 731.20 684.53 657.25 ## Exposed_3 ## 10700001 5469.90 ## 10700002 198.64 ## 10700003 1632.20 ## 10700004 60.67 ## 10700005 667.66 dim(geodata_genes) ## [1] 29214 7 Note that this dataset includes expression measures across 29,214 probes, representing 14,019 unique genes. For simplicity in the final exercises, let’s just filter for rows representing mapped genes. geodata_genes = geodata_genes[!(geodata_genes$`Gene symbol` == &quot;&quot;), ] dim(geodata_genes) ## [1] 16024 7 Note that this dataset now includes 16,024 rows with mapped gene symbol identifiers. With this, we can now answer Environmental Health Question 1: What kind of molecular identifiers are commonly used in microarray-based -omics technologies? Answer: Platform-specific probeset IDs. We can also answer Environmental Health Question 2: How can we convert platform-specific molecular identifiers used in -omics study designs to gene-level information? Answer: We can merge platform-specific IDs with gene-level information using annotation files. Visualizing Data Visualizing Gene Expression Data using Boxplots and Heat Maps To visualize the -omics data, we can generate boxplots, heat maps, any many other types of visualizations Here, we provide an example to plot a boxplot, which can be used to visualize the variability amongst samples We also provide an example to plot a heat map, comparing unscaled vs scaled gene expression profiles These visualizations can be useful to both simply visualize the data as well as identify patterns across samples or genes Boxplot Visualizations For this example, let’s simply use R’s built in boxplot() function. We only want to use columns with our expression data (2 to 7), so let’s pull those columns when running the boxplot function. boxplot(geodata_genes[,2:7]) There seem to be a lot of variability within each sample’s range of expression levels, with many outliers. This makes sense given that we are analyzing the expression levels across the rat’s entire genome, where some genes won’t be expressed at all while others will be highly expressed due to biological and/or potential technical variability. To show plots without outliers, we can simply use outline=F. boxplot(geodata_genes[,2:7], outline=F) Heat Map Visualizations Heat maps are also useful when evaluating large datasets. There are many different packages you can use to generate heat maps. Here, we use the superheat package. It also takes awhile to plot all genes across the genome, so to save time for this training module, let’s randomly select 100 rows to plot. # To ensure that the same subset of genes are selected each time set.seed = 101 # Random selection of 100 rows row.sample = sample(1:nrow(geodata_genes),100) # Heat map code superheat::superheat(geodata_genes[row.sample,2:7], # Only want to plot non-id/gene symbol columns (2 to 7) pretty.order.rows = TRUE, pretty.order.cols = TRUE, col.dendrogram = T, row.dendrogram = T) This produces a heat map with sample IDs along the x-axis and probeset IDs along the y-axis. Here, the values being displayed represent normalized expression values. One way to improve our ability to distinguish differences between samples is to scale expression values across probes. Scaling data Z-score is a very common method of scaling that transforms data points to reflect the number of standard deviations they are from the overall mean. Z-score scaling data results in the overall transformation of a dataset to have an overall mean = 0 and standard deviation = 1. Let’s see what happens when we scale this gene expression dataset by z-score across each probe. This can be easily done using the scale function. This specific scale function works by centering and scaling across columns, but since we want to use it across probesets (organized as rows), we need to first transpose our dataset, then run the scale function. geodata_genes_scaled = scale(t(geodata_genes[,2:7]), center=T, scale=T) Now we can transpose it back to the original format (i.e., before it was transposed). geodata_genes_scaled = t(geodata_genes_scaled) And then view what the normalized and now scaled expression data look like for now a random subset of 100 probesets (representing genes). With these data now scaled, we can more easily visualize patterns between samples. We can also answer Environmental Health Question 3: Why do we often scale gene expression signatures prior to heat map visualizations? Answer: To better visualize patterns in expression signatures between samples. Now, with these data nicely organized, we can see how statistics can help find which genes show trends in expression associated with formaldehyde exposure. Statistical Analyses Statistical Analyses to Identify Genes altered by Formaldehyde A simple way to identify differences between formaldehyde-exposed and unexposed samples is to use a t-test. Because there are so many tests being performed, one for each gene, it is also important to carry out multiple test corrections through a p-value adjustment method. We need to run a t-test for each row of our dataset. This exercise demonstrates two different methods to run a t-test: Method 1: using a ‘for loop’ Method 2: using the apply function (more computationally efficient) Method 1 (m1): ‘For Loop’ Let’s first re-save the molecular probe IDs to a column within the dataframe, since we need those values in the loop function. geodata_genes$ID = rownames(geodata_genes) We also need to initially create an empty dataframe to eventually store p-values. pValue_m1 = matrix(0, nrow=nrow(geodata_genes), ncol=3) colnames(pValue_m1) = c(&quot;ID&quot;, &quot;pval&quot;, &quot;padj&quot;) head(pValue_m1) ## ID pval padj ## [1,] 0 0 0 ## [2,] 0 0 0 ## [3,] 0 0 0 ## [4,] 0 0 0 ## [5,] 0 0 0 ## [6,] 0 0 0 You can see the empty dataframe that was generated through this code. Then we can loop through the entire dataset to acquire p-values from t-test statistics, comparing n=3 exposed vs n=3 unexposed samples. for (i in 1:nrow(geodata_genes)) { #Get the ID ID.i = geodata_genes[i, &quot;ID&quot;]; #Run the t-test and get the p-value pval.i = t.test(geodata_genes[i,exposedIDs], geodata_genes[i,unexposedIDs])$p.value; #Store the data in the empty dataframe pValue_m1[i,&quot;ID&quot;] = ID.i; pValue_m1[i,&quot;pval&quot;] = pval.i } View the results: # Note that we&#39;re not pulling the last column (padj) since we haven&#39;t calculated these yet pValue_m1[1:5,1:2] ## ID pval ## [1,] &quot;10903987&quot; &quot;0.0812802229304083&quot; ## [2,] &quot;10714794&quot; &quot;0.757311314118124&quot; ## [3,] &quot;10858408&quot; &quot;0.390952310869689&quot; ## [4,] &quot;10872252&quot; &quot;0.0548937136005506&quot; ## [5,] &quot;10905819&quot; &quot;0.173539535577791&quot; Method 2 (m2): Apply Function For the second method, we can use the apply() function to calculate resulting t-test p-values more efficiently labeled. pValue_m2 = apply(geodata_genes[,2:7], 1, function(x) t.test(x[unexposedIDs], x[exposedIDs])$p.value) names(pValue_m2) = geodata_genes[,&quot;ID&quot;] We can convert the results into a dataframe to make it similar to m1 matrix we created above. pValue_m2 = data.frame(pValue_m2) # Now create an ID column pValue_m2$ID = rownames(pValue_m2) Then we can view at the two datasets to see they result in the same pvalues. head(pValue_m1) ## ID pval padj ## [1,] &quot;10903987&quot; &quot;0.0812802229304083&quot; &quot;0&quot; ## [2,] &quot;10714794&quot; &quot;0.757311314118124&quot; &quot;0&quot; ## [3,] &quot;10858408&quot; &quot;0.390952310869689&quot; &quot;0&quot; ## [4,] &quot;10872252&quot; &quot;0.0548937136005506&quot; &quot;0&quot; ## [5,] &quot;10905819&quot; &quot;0.173539535577791&quot; &quot;0&quot; ## [6,] &quot;10907585&quot; &quot;0.215200167867295&quot; &quot;0&quot; head(pValue_m2) ## pValue_m2 ID ## 10903987 0.08128022 10903987 ## 10714794 0.75731131 10714794 ## 10858408 0.39095231 10858408 ## 10872252 0.05489371 10872252 ## 10905819 0.17353954 10905819 ## 10907585 0.21520017 10907585 We can see from these results that both methods (m1 and m2) generate the same statistical p-values. Interpreting Results Let’s again merge these data with the gene symbols to tell which genes are significant. First, let’s convert to a dataframe and then merge as before, for one of the above methods as an example (m1). pValue_m1 = data.frame(pValue_m1) pValue_m1 = merge(pValue_m1, id.gene.table, by=&quot;ID&quot;) We can also add a multiple test correction by applying a false discovery rate-adjusted p-value; here, using the Benjamini Hochberg (BH) method. # Here fdr is an alias for B-H method pValue_m1[,&quot;padj&quot;] = p.adjust(pValue_m1[,&quot;pval&quot;], method=c(&quot;fdr&quot;)) Now, we can sort these statistical results by adjusted p-values. pValue_m1.sorted = pValue_m1[order(pValue_m1[,&#39;padj&#39;]),] head(pValue_m1.sorted) ## ID pval padj Gene symbol ## 9143 10837582 4.57288413593085e-07 0.00732759 Olr633 ## 5640 10783648 1.93688668590855e-06 0.01551834 Slc7a8 ## 8 10701699 0.0166773380386967 0.13089115 Lrp11 ## 17 10701817 0.0131845685452954 0.13089115 Fuca2 ## 19 10701830 0.00586885826460337 0.13089115 Hivep2 ## 23 10701880 0.00749149990409956 0.13089115 Reps1 Pulling just the significant genes using an adjusted p-value threshold of 0.05. adj.pval.sig = pValue_m1[which(pValue_m1[,&#39;padj&#39;] &lt; .05),] # Viewing these genes adj.pval.sig ## ID pval padj Gene symbol ## 5640 10783648 1.93688668590855e-06 0.01551834 Slc7a8 ## 9143 10837582 4.57288413593085e-07 0.00732759 Olr633 With this, we can answer Environmental Health Question 4: What genes are altered in expression by formaldehyde inhalation exposure? Answer: Olr633 and Slc7a8. Finally, let’s plot these using a mini heat map. Note that we can use probesetIDs, then gene symbols, in rownames to have them show in heat map labels. Note that this statistical filter is pretty strict when comparing only n=3 vs n=3 biological replicates. If we loosen the statistical criteria to p-value &lt; 0.05, this is what we can find: pval.sig = pValue_m1[which(pValue_m1[,&#39;pval&#39;] &lt; .05),] nrow(pval.sig) ## [1] 5327 5327 genes with significantly altered expression! Note that other filters are commonly applied to further focus these lists (e.g., background and fold change filters) prior to statistical evaluation, which can impact the final results. See Rager et al. 2013 for further statistical approaches and visualizations. With this, we can answer Environmental Health Question 5: What are the potential biological consequences of these gene-level perturbations? Answer: Olr633 stands for ‘olfactory receptor 633’. Olr633 is up-regulated in expression, meaning that formaldehyde inhalation exposure has a smell that resulted in ‘activated’ olfactory receptors in the nose of these exposed rats. Slc7a8 stands for ‘solute carrier family 7 member 8’. Slc7a8 is down-regulated in expression, and it plays a role in many biological processes, that when altered, can lead to changes in cellular homeostasis and disease. Concluding Remarks In conclusion, this training module provides an overview of pulling, organizing, visualizing, and analyzing -omics data from the online repository, Gene Expression Omnibus (GEO). Trainees are guided through the overall organization of an example high dimensional dataset, focusing on transcriptomic responses in the nasal epithelium of rats exposed to formaldehyde. Data are visualized and then analyzed using standard two-group comparisons. Findings are interpreted for biological relevance, yielding insight into the effects resulting from formaldehyde exposure. For additional case studies that leverage GEO, see the following publications that also address environmental health questions from our research group: Rager JE, Fry RC. The aryl hydrocarbon receptor pathway: a key component of the microRNA-mediated AML signalisome. Int J Environ Res Public Health. 2012 May;9(5):1939-53. doi: 10.3390/ijerph9051939. Epub 2012 May 18. PMID: 22754483; PMCID: PMC3386597. Rager JE, Suh M, Chappell GA, Thompson CM, Proctor DM. Review of transcriptomic responses to hexavalent chromium exposure in lung cells supports a role of epigenetic mediators in carcinogenesis. Toxicol Lett. 2019 May 1;305:40-50. PMID: 30690063. "],["database-integration-air-quality-mortality-and-environmental-justice-data.html", "3.3 Database Integration: Air Quality, Mortality, and Environmental Justice Data Introduction to Training Module Population-weighted vs Unweighted Exposures Regression Modeling Environmental Justice Considerations Concluding Remarks", " 3.3 Database Integration: Air Quality, Mortality, and Environmental Justice Data The development of this training module was led by Dr. Cavin Ward-Caviness. Fall 2021 Disclaimer: The views expressed in this document are those of the author and do not necessarily reflect the views or policies of the U.S. EPA. Introduction to Exposure and Health Databases used in this Module In this R example we will use publicly available exposure and health databases to examine associations between air quality and mortality across the entire U.S. Specific databases that we will query include the following: EPA Air Quality data: As an example air pollutant exposure dataset, 2016 annual average data from the EPA Air Quality System database will be analyzed, using data downloaded and organized from the following website: https://aqs.epa.gov/aqsweb/airdata/annual_conc_by_monitor_2016.zip CDC Health Outcome data: As an example health outcome dataset, the 2016 CDC Underlying Cause of Death dataset, from the WONDER (Wide-ranging ONline Data for Epidemiologic Research) website will be analyzed, using All-Cause Mortality Rates downloaded and organized from the following website: https://wonder.cdc.gov/ucd-icd10.html Human covariate data: The potential influence of covariates (e.g., race) and other confounders will be analyzed using data downloaded and organized from the following 2016 county-level resource: https://www.countyhealthrankings.org/explore-health-rankings/rankings-data-documentation/national-data-documentation-2010-2019 Introduction to Training Module This training module provides an example analysis based on the integration of data across multiple environmental health databases. This module specifically guides trainees through an explanation of how the data were downloaded and organized, and then details the loading of required packages and datasets. Then, this module provides code for visualizing county-level air pollution measures obtained through U.S. EPA monitoring stations throughout the U.S. Air pollution measures include PM2.5, NO2, and SO2, are visualized here as the yearly average. Air pollution concentrations are then evaluated for potential relationship to the health outcome, mortality. Specifically, age adjusted mortality rates are organized and statistically related to PM2.5 concentrations through linear regression modeling. Crude statistical models are first provided that do not take into account the influence of potential confounders. Then, statistical models are used that adjust for potential confounders, including adult smoking rates, obesity, food environment indicators, physical activity, employment status, rural vs urban living percentages, sex, ethnicity, and race. Results from these models point to the finding that areas with higher percentages of African-Americans may be experiencing higher impacts from PM2.5 on mortality. This relationship is of high interest, as it represents a potential Environmental Justice issue. Training Module’s Environmental Health Questions: This training module was specifically developed to answer the following environmental health questions: What areas of the U.S. are most heavily monitored for air quality? Is there an association between long-term, ambient PM2.5 concentrations and mortality at the county level? Stated another way we are asking: Do counties with higher annual average PM2.5 concentrations also have higher all-cause mortality rates? What is the difference when running crude statistical models vs. statistical models that adjust for potential confounding, when evaluating the relationship between PM2.5 and mortality? Do observed associations differ when comparing between counties with a higher vs. lower percentage of African-Americans which can indicate environmental justice concerns? Script Preparations Cleaning the global environment rm(list=ls()) Installing required R packages If you already have these packages installed, you can skip this step, or you can run the below code which checks installation status for you: if (!requireNamespace(&quot;sf&quot;)) install.packages(&quot;sf&quot;); if (!requireNamespace(&quot;dplyr&quot;)) install.packages(&quot;dplyr&quot;); if (!requireNamespace(&quot;tidyr&quot;)) install.packages(&quot;tidyr&quot;); if (!requireNamespace(&quot;ggplot2&quot;)) install.packages(&quot;ggplot2&quot;); if (!requireNamespace(&quot;ggthemes&quot;)) install.packages(&quot;ggthemes&quot;); Loading R packages required for this session library(sf) library(dplyr) library(tidyr) library(ggplot2) library(ggthemes) Set your working directory setwd(&quot;/filepath to where your input files are&quot;) Loading Example Datasets Let’s start by loading the datasets needed for this training module. As detailed in the introduction, these data were previously downloaded and organized, and specifically made available for this training excercise as a compiled RDataset, containing organized dataframes ready to analyze. We can now read in these organized data using the ‘load’ function: load(&quot;Module3_3/Module3_3_Data_AirQuality_Mortality_EJ.RData&quot;) Data Viewing &amp; Plotting The air pollution data has already been aggregated to the county level so let’s plot it and see what it looks like. First let’s take a look at the data, starting with the county-level shapefile: head(counties_shapefile) ## Simple feature collection with 6 features and 9 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: -102 ymin: 37.39 xmax: -84.8 ymax: 43.5 ## Geodetic CRS: NAD83 ## STATEFP COUNTYFP COUNTYNS AFFGEOID GEOID NAME LSAD ALAND ## 1 19 107 00465242 0500000US19107 19107 Keokuk 06 1.500e+09 ## 2 19 189 00465283 0500000US19189 19189 Winnebago 06 1.037e+09 ## 3 20 093 00485011 0500000US20093 20093 Kearny 06 2.255e+09 ## 4 20 123 00485026 0500000US20123 20123 Mitchell 06 1.818e+09 ## 5 20 187 00485055 0500000US20187 20187 Stanton 06 1.762e+09 ## 6 21 005 00516849 0500000US21005 21005 Anderson 06 5.227e+08 ## AWATER geometry ## 1 1929323 MULTIPOLYGON (((-92.41 41.5... ## 2 3182052 MULTIPOLYGON (((-93.97 43.5... ## 3 1133601 MULTIPOLYGON (((-101.5 37.9... ## 4 44979981 MULTIPOLYGON (((-98.49 39.2... ## 5 178555 MULTIPOLYGON (((-102 37.54,... ## 6 6311537 MULTIPOLYGON (((-85.17 38, ... This dataframe contains the location information for the counties which we will use for visualizations. Now let’s view the EPA Air Quality Survey (AQS) data collected from 2016: head(epa_ap_county) ## State.Code State.Name County.Code County.Name State_County_Code ## 1 1 Alabama 3 Baldwin 1003 ## 2 1 Alabama 27 Clay 1027 ## 3 1 Alabama 33 Colbert 1033 ## 4 1 Alabama 49 DeKalb 1049 ## 5 1 Alabama 55 Etowah 1055 ## 6 1 Alabama 69 Houston 1069 ## Parameter.Name Units.of.Measure County_Avg ## 1 PM25 Micrograms/cubic meter (LC) 7.226 ## 2 PM25 Micrograms/cubic meter (LC) 7.364 ## 3 PM25 Micrograms/cubic meter (LC) 7.492 ## 4 PM25 Micrograms/cubic meter (LC) 7.696 ## 5 PM25 Micrograms/cubic meter (LC) 8.196 ## 6 PM25 Micrograms/cubic meter (LC) 7.062 This dataframe represents county-level air quality measures, collected through the Air Quality Survey (2016), as detailed above. This dataframe is in melted (or long) format, meaning that different air quality measures are organized across rows, with variable measure indicators in the ‘Parameter.Name’, ‘Units.of.Measure’, and ‘County_Avg’ columns. These data can be restructured to view air quality measures as separate variables labeled across columns using: # transform from the &quot;long&quot; to &quot;wide&quot; format for the pollutants epa_ap_county &lt;- epa_ap_county %&gt;% select(-Units.of.Measure) %&gt;% unique() %&gt;% tidyr::spread(Parameter.Name, County_Avg) head(epa_ap_county) ## State.Code State.Name County.Code County.Name State_County_Code NO2 PM25 SO2 ## 1 1 Alabama 3 Baldwin 1003 NA 7.226 NA ## 2 1 Alabama 27 Clay 1027 NA 7.364 NA ## 3 1 Alabama 33 Colbert 1033 NA 7.492 NA ## 4 1 Alabama 49 DeKalb 1049 NA 7.696 NA ## 5 1 Alabama 55 Etowah 1055 NA 8.196 NA ## 6 1 Alabama 69 Houston 1069 NA 7.062 NA Note that we can now see the specific pollutant variables ‘NO2’, ‘PM25’, and ‘SO2’ on the far right. Population-weighted vs Unweighted Exposures Here we pause briefly to speak on population-weighted vs unweighted exposures. The analysis we will be undertaking is known as an “ecological” analysis where we are looking at associations by area, e.g. county. When studying environmental exposures by area a common practice is to try to weight the exposures by the population so that exposures better represent the “burden” faced by the population. Ideally for this you would want a systematic model or assessment of the exposure that corresponded with a fine-scale population estimate so that for each county you could weight exposures within different areas of the county by the population exposed. This sparse monitor data (we will examine the population covered later in the tutorial) is not population weighted, but should you see similar analyses with population weighting of exposures you should simply be aware that this better captures the “burden” of exposure experienced by the population within the area estimated, typically zip code or county. Now let’s view the CDC’s mortality dataset collected from 2016: head(cdc_mortality) ## Notes County County.Code Deaths Population Crude.Rate ## 1 NA Autauga County, AL 1001 520 55416 938.36 ## 2 NA Baldwin County, AL 1003 1974 208563 946.48 ## 3 NA Barbour County, AL 1005 256 25965 985.94 ## 4 NA Bibb County, AL 1007 239 22643 1055.51 ## 5 NA Blount County, AL 1009 697 57704 1207.89 ## 6 NA Bullock County, AL 1011 133 10362 1283.54 ## Age.Adjusted.Rate Age.Adjusted.Rate.Standard.Error ## 1 884.4 39.46 ## 2 716.9 16.58 ## 3 800.7 51.09 ## 4 927.7 61.03 ## 5 989.4 38.35 ## 6 1063.0 93.69 We can create a visualization of values throughout the U.S. to further inform what these data look like: # Can merge them by the FIPS county code which we need to create for the counties_shapefile counties_shapefile$State_County_Code &lt;- as.character(as.numeric(paste0(counties_shapefile$STATEFP, counties_shapefile$COUNTYFP))) # Let&#39;s merge in the air pollution and mortality data and plot it counties_shapefile &lt;- merge(counties_shapefile, epa_ap_county, by.x=&quot;State_County_Code&quot;, by.y=&quot;State_County_Code&quot;, all.x=TRUE) counties_shapefile &lt;- merge(counties_shapefile, cdc_mortality, by.x=&quot;State_County_Code&quot;, by.y=&quot;County.Code&quot;) # Will remove alaska and hawaii just so we can look at the continental USA county_plot &lt;- subset(counties_shapefile, !STATEFP %in% c(&quot;02&quot;,&quot;15&quot;)) # We can start with a simple plot of age-adjusted mortality rate, PM2.5, NO2, and SO2 levels across the U.S. plot(county_plot[,c(&quot;Age.Adjusted.Rate&quot;,&quot;PM25&quot;,&quot;NO2&quot;,&quot;SO2&quot;)]) You can see that these result in the generation of four different nation-wide plots, showing the distributions of age adjusted mortality rates, PM2.5 concentrations, NO2 concentrations, and SO2 concentrations, averaged per-county. Let’s make a nicer looking plot with ggplot, looking just at PM2.5 levels: # Using ggplot we can have more control p &lt;- ggplot(data=county_plot) + geom_sf(aes(fill=PM25)) + scale_fill_viridis_c(option=&quot;plasma&quot;, name=&quot;PM2.5&quot;, guide = guide_colorbar( direction = &quot;horizontal&quot;, barheight = unit(2, units = &quot;mm&quot;), barwidth = unit(50, units = &quot;mm&quot;), draw.ulim = F, title.position = &#39;top&#39;, # some shifting around title.hjust = 0.5, label.hjust = 0.5)) + ggtitle(&quot;2016 Annual PM25 (EPA Monitors)&quot;) + theme_map() + theme(plot.title = element_text(hjust = 0.5, size=22)) p With this, we can answer Environmental Health Question 1: What areas of the U.S. are most heavily monitored for air quality? Answer: We can tell from the PM2.5 specific plot that air monitors are densely located in California, and other areas with high populations (including the East Coast), while large sections of central U.S. lack air monitoring data. Analyzing Relationships between PM2.5 and Mortality Now the primary question is whether counties with higher PM2.5 also have higher mortality rates. To answer this question, first we need to run some data merging in preparation for this analysis: model_data &lt;- merge(epa_ap_county, cdc_mortality, by.x=&quot;State_County_Code&quot;, by.y=&quot;County.Code&quot;) As we saw in the above plot, only a portion of the USA is covered by PM2.5 monitors. Let’s see what our population coverage is: sum(model_data$Population, na.rm=TRUE) ## [1] 232169063 sum(cdc_mortality$Population, na.rm=TRUE) ## [1] 323127513 sum(model_data$Population, na.rm=TRUE)/sum(cdc_mortality$Population, na.rm=TRUE)*100 ## [1] 71.85 We can do a quick visual inspection of this using a scatter plot which will also let us check for unexpected distributions of the data (always a good idea): plot(model_data$Age.Adjusted.Rate, model_data$PM25, main=&quot;PM2.5 by Mortality Rate&quot;, xlab=&quot;Age Adjusted Mortality Rate&quot;, ylab=&quot;PM25&quot;) The univariate correlation is a simple way of quantifying this potential relationships, though it does not nearly tell the complete story. Just as a starting point, let’s run this simple univariate correlation calculation using the ‘cor’ function: cor(model_data$Age.Adjusted.Rate, model_data$PM25, use=&quot;complete.obs&quot;) ## [1] 0.1785 Regression Modeling Now, let’s obtain a more complete estimate of the data through regression modeling. As an initial starting point, let’s run this model without any confounders (also known as a ‘crude’ model). A simple linear regression model in R can be carried out using the ‘lm’ (linear model) function. Here, we are evaluating age adjusted mortality rate (age.adjusted.rate) as the dependent variable, and PM2.5 as the independent variable. Values used in evaluating this model were weighted to adjust for the fact that some counties have higher precision in their age adjusted mortality rate (represented by a smaller age adjusted rate standard error). # Running the linear regression model m &lt;- lm(Age.Adjusted.Rate ~ PM25, data = model_data, weights=1/model_data$Age.Adjusted.Rate.Standard.Error) # Viewing the model results through the summary function summary(m) ## ## Call: ## lm(formula = Age.Adjusted.Rate ~ PM25, data = model_data, weights = 1/model_data$Age.Adjusted.Rate.Standard.Error) ## ## Weighted Residuals: ## Min 1Q Median 3Q Max ## -110.10 -9.79 8.36 25.09 84.32 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 661.04 22.53 29.34 &lt;2e-16 *** ## PM25 8.96 2.88 3.11 0.0019 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 29.9 on 608 degrees of freedom ## (88 observations deleted due to missingness) ## Multiple R-squared: 0.0157, Adjusted R-squared: 0.0141 ## F-statistic: 9.68 on 1 and 608 DF, p-value: 0.00195 Shown here are summary level statistics summarizing the results of the linear regression model. In the model summary we see several features. The “Estimate” column is the regression coefficient which tells us the relationship between a 1 ug/m3 change (elevation) in PM2.5 and the age-adjusted all-cause mortality rate. “Std. Error” is the standard error of the estimate. The column “t value” represents the T-statistic which is the test statistic for linear regression models and is simply the “Estimate” divied by “Std. Error”. This “t value” is compared with the Student’s T distribution in order to determine the p-value (“Pr(&gt;|t|)”). The residuals are the difference between the predicted outcome (age-adjusted mortality rate) and known outcome from the data. For linear regression to be valid this should be normally distributed. The residuals from a linear model can be extracted using the residuals() command and plotted to see their distribution. With this, we can answer Environmental Health Question 2: Is there an association between long-term, ambient PM2.5 concentrations and mortality at the county level? Stated another way we are asking: Do counties with higher annual average PM2.5 concentrations also have higher all-cause mortality rates? Answer: Based on these model results, there may indeed be an association between PM2.5 concentrations and mortality (p=0.0019). To more thoroughly examine the potential relationship between PM2.5 concentrations and mortality it is absolutely essential to adjust for confounders. # First we merge the covariate data in with the AQS data model_data &lt;- merge(model_data, county_health, by.x=&quot;State_County_Code&quot;, by.y=&quot;County.5.digit.FIPS.Code&quot;, all.x=TRUE) # Now we add some relevant confounders to the linear regression model m &lt;- lm(Age.Adjusted.Rate ~ PM25 + Adult.smoking + Adult.obesity + Food.environment.index + Physical.inactivity + High.school.graduation + Some.college + Unemployment + Violent.crime + Percent.Rural + Percent.Females + Percent.Asian + Percent.Non.Hispanic.African.American + Percent.American.Indian.and.Alaskan.Native + Percent.NonHispanic.white, data = model_data, weights=1/model_data$Age.Adjusted.Rate.Standard.Error) # And finally we check to see if the statistical association persists summary(m) ## ## Call: ## lm(formula = Age.Adjusted.Rate ~ PM25 + Adult.smoking + Adult.obesity + ## Food.environment.index + Physical.inactivity + High.school.graduation + ## Some.college + Unemployment + Violent.crime + Percent.Rural + ## Percent.Females + Percent.Asian + Percent.Non.Hispanic.African.American + ## Percent.American.Indian.and.Alaskan.Native + Percent.NonHispanic.white, ## data = model_data, weights = 1/model_data$Age.Adjusted.Rate.Standard.Error) ## ## Weighted Residuals: ## Min 1Q Median 3Q Max ## -70.06 -7.05 0.96 8.12 64.05 ## ## Coefficients: ## Estimate Std. Error t value ## (Intercept) 616.4798 157.2084 3.92 ## PM25 3.8485 1.6932 2.27 ## Adult.smoking 859.2734 137.8131 6.24 ## Adult.obesity 605.8431 97.7195 6.20 ## Food.environment.index -28.6554 3.9336 -7.28 ## Physical.inactivity 117.6092 91.4152 1.29 ## High.school.graduation 55.1445 40.7849 1.35 ## Some.college -244.4126 49.7876 -4.91 ## Unemployment 97.9816 161.8902 0.61 ## Violent.crime 0.0755 0.0152 4.98 ## Percent.Rural -12.5531 20.8483 -0.60 ## Percent.Females -271.4550 299.1693 -0.91 ## Percent.Asian 74.5433 55.8926 1.33 ## Percent.Non.Hispanic.African.American 153.3991 39.4962 3.88 ## Percent.American.Indian.and.Alaskan.Native 200.7001 102.7503 1.95 ## Percent.NonHispanic.white 240.1798 26.3054 9.13 ## Pr(&gt;|t|) ## (Intercept) 1.0e-04 *** ## PM25 0.02344 * ## Adult.smoking 9.4e-10 *** ## Adult.obesity 1.2e-09 *** ## Food.environment.index 1.2e-12 *** ## Physical.inactivity 0.19883 ## High.school.graduation 0.17694 ## Some.college 1.2e-06 *** ## Unemployment 0.54529 ## Violent.crime 8.6e-07 *** ## Percent.Rural 0.54736 ## Percent.Females 0.36464 ## Percent.Asian 0.18290 ## Percent.Non.Hispanic.African.American 0.00012 *** ## Percent.American.Indian.and.Alaskan.Native 0.05133 . ## Percent.NonHispanic.white &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 14.2 on 514 degrees of freedom ## (168 observations deleted due to missingness) ## Multiple R-squared: 0.784, Adjusted R-squared: 0.778 ## F-statistic: 124 on 15 and 514 DF, p-value: &lt;2e-16 With this, we can answer Environmental Health Question 3: What is the difference when running crude statistical models vs statistical models that adjust for potential confounding, when evaluating the relationship between PM2.5 and mortality? Answer: The relationship between PM2.5 and mortality remains statistically significant when confounders are considered (p=0.023), though is not as significant as when running the crude model (p=0.0019). Environmental Justice Considerations Environmental Justice is the study of how societal inequities manifest in differences in environmental health risks either due to greater exposures or a worse health response to exposures. Racism and racial discrimination are major factors in both how much pollution people are exposed to as well what their responses might be due to other co-existing inequities (e.g. poverty, access to healthcare, food deserts). Race is a commonly used proxy for experiences of racism and racial discrimination. Here we will consider the race category of ‘Non-Hispanic African-American’ to investigate if pollution levels differ by percent African-Americans in a county and if associations between PM2.5 and mortality also differ by this variable, which could indicate Environmental Justice-relevant issues revealed by this data. We will specifically evaluate data distributions across counties with the highest percentage of African-Americans (top 25%) vs lowest percentage of African-Americans (bottom 25%). First let’s visualize the distribution of % African-Americans in these data: hist(model_data$Percent.Non.Hispanic.African.American*100, main=&quot;Percent African-American by County&quot;,xlab=&quot;Percent&quot;) And let’s look at a summary of the data: summary(model_data$Percent.Non.Hispanic.African.American) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 0.00 0.01 0.04 0.09 0.12 0.70 67 We can compute quartiles of the data: model_data$AA_quartile &lt;- with(model_data, cut(Percent.Non.Hispanic.African.American, breaks=quantile(Percent.Non.Hispanic.African.American, probs=seq(0,1, by=0.25), na.rm=TRUE), include.lowest=TRUE, ordered_result=TRUE, labels=FALSE)) Then we can use these quartiles to see that as the Percent African-American increases so does the PM2.5 exposure by county: AA_summary &lt;- model_data %&gt;% filter(!is.na(Percent.Non.Hispanic.African.American)) %&gt;% group_by(AA_quartile) %&gt;% summarise(Percent_AA = mean(Percent.Non.Hispanic.African.American, na.rm=TRUE), Mean_PM25 = mean(PM25, na.rm=TRUE)) AA_summary ## # A tibble: 4 × 3 ## AA_quartile Percent_AA Mean_PM25 ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0.00671 5.97 ## 2 2 0.0238 7.03 ## 3 3 0.0766 7.93 ## 4 4 0.269 8.05 Now that we can see this trend, let’s add some statistics. Let’s specifically compare the relationships between PM2.5 and mortality within the bottom 25% AA counties (quartile 1); and also the highest 25% AA counties (quartile 4): # First need to subset the data by these quartiles of interest low_AA &lt;- subset(model_data, AA_quartile==1) high_AA &lt;- subset(model_data, AA_quartile==4) # Then we can run the relevant statistical models m.low &lt;- lm(Age.Adjusted.Rate ~ PM25 + Adult.smoking + Adult.obesity + Food.environment.index + Physical.inactivity + High.school.graduation + Some.college + Unemployment + Violent.crime + Percent.Rural + Percent.Females + Percent.Asian + Percent.American.Indian.and.Alaskan.Native + Percent.NonHispanic.white, data = low_AA, weights=1/low_AA$Age.Adjusted.Rate.Standard.Error) m.high &lt;- lm(Age.Adjusted.Rate ~ PM25 + Adult.smoking + Adult.obesity + Food.environment.index + Physical.inactivity + High.school.graduation + Some.college + Unemployment + Violent.crime + Percent.Rural + Percent.Females + Percent.Asian + Percent.American.Indian.and.Alaskan.Native + Percent.NonHispanic.white, data = high_AA, weights=1/high_AA$Age.Adjusted.Rate.Standard.Error) # We see a striking difference in the associations rbind(c(&quot;Bottom 25% AA Counties&quot;,round(summary(m.low)$coefficients[&quot;PM25&quot;,c(1,2,4)],3)), c(&quot;Top 25% AA Counties&quot;,round(summary(m.high)$coefficients[&quot;PM25&quot;,c(1,2,4)],3))) ## Estimate Std. Error Pr(&gt;|t|) ## [1,] &quot;Bottom 25% AA Counties&quot; &quot;4.782&quot; &quot;3.895&quot; &quot;0.222&quot; ## [2,] &quot;Top 25% AA Counties&quot; &quot;14.552&quot; &quot;4.13&quot; &quot;0.001&quot; With this, we can answer Environmental Health Question 4: Do observed associations differ when comparing between counties with a higher vs. lower percentage of African-Americans which can indicate environmental justice concerns? Answer: Yes. Counties with the highest percentage of African-Americans (top 25%) demonstrated a highly significant association between PM2.5 and age adjusted mortality, even when adjusting for confounders (p=0.001), meaning that the association between PM2.5 and mortality within these counties may be exacerbated by factors relevant to race. Conversely, counties with the lowest percentages of African-Americans (bottom 25%) did not demonstrate a significant association between PM2.5 and age adjusted mortality, indicating that these counties may have lower environmental health risks due to factors correlated with race. Concluding Remarks In conclusion, this training module serves as a novel example data integration effort of high relevance to environmental health issues. Databases that were evaluated here span exposure data (i.e., Air Quality System data), health outcome data (i.e., mortality data), and county-level characteristics on healthcare, food environment, and other potentially relevant confounders (i.e., county-level variables that may impact observed relationships), and environmental justice data (e.g., race). Many different visualization and statistical approaches were used, largely based on linear regression modeling and county-level characteristic stratifications. These example statistics clearly support the now-established relationship between PM2.5 concentrations in the air and mortality. Importantly, these related methods can be tailored to address new questions to increase our understanding between exposures to chemicals in the environment and adverse health outcomes, as well as the impact of different individual or area charateristics on these relationships - particularly those that might relate to environmental justice concerns. "],["resources.html", "Resources", " Resources Here we provide additional resources that will benefit trainees who want to learn more about certain aspects of data science and computational methods discussed throughout these training modules. There are many additional resources available for learning more about data management and analysis methods. Select resources that we have found helpful are detailed below. R Programming Resources: Coursera provides online courses for many technical topics, including several on R programming. Datacamp provides online courses for learning R, Python, statistics, and more. R for Data Science, developed by Hadley Wickham and Garrett Grolemund, is an online resource that also comes in the format of a book, that teaches participants how to do data science using R. R Graphics Cookbook, developed by Winston Chang, is a practical guide that provides more than 150 “recipes” to demonstrate generating graphics in R quickly. R for Graduate Students, developed by a PhD student, Wendy Huynh, who taught herself R as a needed component of her research while obtaining a PhD and wanted to share her experience and lessons learned. Reproducible Medical Research with R, developed by Dr. Higgins as an online resource for those in the medical field interested in analyzing the data to better understand health, disease, or the delivery of care. R Packages Resources: R Comprehensive R Archive Network (CRAN) website Bioconductor: Open Source Software for Bioinformatics website Community Discussions on R and R Packages: Stack Overflow online discussion forums. Discussion forums within Bioconductor, where you can communicate directly with package developers. R Interfaces RStudio is an integrated development environment for R that is available in two formats: RStudio Desktop, a regular desktop application, and RStudio Server, a remote server that allows accessing RStudio using a web browser. R Markdown is a LaTeX-like documentation format that allows users to draft comprehensive documentation throughout R-based scripts. R Markdown is advantageous in that users can run their code on their computer and save a ‘knitted’ version of the code that also displays messages, results, and graphics that result from execution of the developed script. R Notebook is an R Markdown document that allows users to run independent and interactive execution of chunks of code. This interface allows users to visually assess the output as they develop a R Markdown document without having to knit the entire document in order to visualize the output. Data Science and Statistical Analysis Resources: The Elements of Statistical Learning, written by Trevor Hastie, Robert Tibshirani, and Jerome Friedman. Towards Data Science, developed by Mason Adair, Jobelle Braun, Nick Cohn and Thibaut Dubernet. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
